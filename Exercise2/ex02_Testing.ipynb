{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e86f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce998a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_reviews = pd.read_csv(r\"C:\\Users\\Damja\\OneDrive\\Damjan\\HS22\\NLP UZH\\Exercise2\\tripadvisor_hotel_reviews.csv\")\n",
    "#scifi = pd.read_csv(r\"C:\\Users\\Damja\\OneDrive\\Damjan\\HS22\\NLP UZH\\Exercise2\\scifi.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73cfe09",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
    "\n",
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typcially, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word wi and an N context window on each side, wi−1,…,wi−N and wi+1,…,wi+N, referring to all context words collectively as C, CBOW tries to minimize\n",
    "\n",
    "−logp(wi|C)=−logSoftmax(A(∑w∈Cqw)+b)\n",
    "\n",
    "where qw is the embedding for word w.\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some tips:\n",
    "\n",
    "    Think about which parameters you need to define.\n",
    "    Make sure you know what shape each operation expects. Use .view() if you need to reshape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927e351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 20, 30, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = dict((v, k) for k, v in word_to_ix.items())\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc2ef38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'In', 1: 'idea', 2: 'programs', 3: 'People', 4: 'the', 5: 'a', 6: 'Computational', 7: 'of', 8: 'beings', 9: 'they', 10: 'process.', 11: 'program.', 12: 'As', 13: 'evolve,', 14: 'effect,', 15: 'study', 16: 'directed', 17: 'pattern', 18: 'We', 19: 'computer', 20: 'are', 21: 'processes.', 22: 'data.', 23: 'we', 24: 'The', 25: 'spells.', 26: 'manipulate', 27: 'computers.', 28: 'our', 29: 'by', 30: 'to', 31: 'processes', 32: 'is', 33: 'abstract', 34: 'about', 35: 'evolution', 36: 'process', 37: 'direct', 38: 'computational', 39: 'that', 40: 'called', 41: 'conjure', 42: 'spirits', 43: 'other', 44: 'inhabit', 45: 'rules', 46: 'create', 47: 'things', 48: 'with'}\n",
      "{'In': 0, 'idea': 1, 'programs': 2, 'People': 3, 'the': 4, 'a': 5, 'Computational': 6, 'of': 7, 'beings': 8, 'they': 9, 'process.': 10, 'program.': 11, 'As': 12, 'evolve,': 13, 'effect,': 14, 'study': 15, 'directed': 16, 'pattern': 17, 'We': 18, 'computer': 19, 'are': 20, 'processes.': 21, 'data.': 22, 'we': 23, 'The': 24, 'spells.': 25, 'manipulate': 26, 'computers.': 27, 'our': 28, 'by': 29, 'to': 30, 'processes': 31, 'is': 32, 'abstract': 33, 'about': 34, 'evolution': 35, 'process': 36, 'direct': 37, 'computational': 38, 'that': 39, 'called': 40, 'conjure': 41, 'spirits': 42, 'other': 43, 'inhabit': 44, 'rules': 45, 'create': 46, 'things': 47, 'with': 48}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_word)\n",
    "\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d0312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset class for transforming out dataset\n",
    "# takes the loaded data and transforms it into a Dataset object to pass to the dataloader\n",
    "# we could also include the import here but we will not\n",
    "\n",
    "class CBOW_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = make_context_vector(data[idx][0], word_to_ix)\n",
    "        y = torch.tensor(word_to_ix[data[idx][1]], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a673d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS ONE DOESNT WORK GOD KNOWS WHY SUCH MINOR DIFFERENCES ONLY F THIS\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.l1 = nn.Linear(in_features=embedding_size,\n",
    "                            out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = torch.sum(self.embedding(inputs), dim=1) # [200, 4, 50] => [200, 50]\n",
    "        # embeds = self.embeddings(inputs).view((batch_size, -1))\n",
    "        y_out = self.l1(embeddings) \n",
    "        log_probs = F.log_softmax(y_out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36400e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                            out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_embedded = sum(self.embedding(inputs))\n",
    "        y_out = self.fc1(x_embedded)\n",
    "        m = nn.LogSoftmax()\n",
    "        y_out = m(y_out).reshape(-1)\n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22930d99",
   "metadata": {},
   "source": [
    "### Let's try and train this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecd5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CBOW_Dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b7e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = CBOW(vocab_size, embedding_size=30)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1d12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damja\\AppData\\Local\\Temp\\ipykernel_22776\\1816555312.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_out = m(y_out).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], batch: [0/58, loss: 3.2026]\n",
      "Epoch [1/10], batch: [1/58, loss: 6.7962]\n",
      "Epoch [1/10], batch: [2/58, loss: 3.0113]\n",
      "Epoch [1/10], batch: [3/58, loss: 4.4938]\n",
      "Epoch [1/10], batch: [4/58, loss: 2.2399]\n",
      "Epoch [1/10], batch: [5/58, loss: 6.2244]\n",
      "Epoch [1/10], batch: [6/58, loss: 4.1509]\n",
      "Epoch [1/10], batch: [7/58, loss: 5.1393]\n",
      "Epoch [1/10], batch: [8/58, loss: 4.0848]\n",
      "Epoch [1/10], batch: [9/58, loss: 7.1552]\n",
      "Epoch [1/10], batch: [10/58, loss: 5.8895]\n",
      "Epoch [1/10], batch: [11/58, loss: 4.2735]\n",
      "Epoch [1/10], batch: [12/58, loss: 3.9668]\n",
      "Epoch [1/10], batch: [13/58, loss: 6.1331]\n",
      "Epoch [1/10], batch: [14/58, loss: 7.8882]\n",
      "Epoch [1/10], batch: [15/58, loss: 2.9878]\n",
      "Epoch [1/10], batch: [16/58, loss: 3.4766]\n",
      "Epoch [1/10], batch: [17/58, loss: 6.0062]\n",
      "Epoch [1/10], batch: [18/58, loss: 3.1658]\n",
      "Epoch [1/10], batch: [19/58, loss: 4.3190]\n",
      "Epoch [1/10], batch: [20/58, loss: 4.7163]\n",
      "Epoch [1/10], batch: [21/58, loss: 2.8541]\n",
      "Epoch [1/10], batch: [22/58, loss: 2.6672]\n",
      "Epoch [1/10], batch: [23/58, loss: 3.8095]\n",
      "Epoch [1/10], batch: [24/58, loss: 4.8753]\n",
      "Epoch [1/10], batch: [25/58, loss: 5.6191]\n",
      "Epoch [1/10], batch: [26/58, loss: 4.2189]\n",
      "Epoch [1/10], batch: [27/58, loss: 3.6198]\n",
      "Epoch [1/10], batch: [28/58, loss: 4.2225]\n",
      "Epoch [1/10], batch: [29/58, loss: 1.9482]\n",
      "Epoch [1/10], batch: [30/58, loss: 2.5261]\n",
      "Epoch [1/10], batch: [31/58, loss: 4.4445]\n",
      "Epoch [1/10], batch: [32/58, loss: 6.3164]\n",
      "Epoch [1/10], batch: [33/58, loss: 5.3138]\n",
      "Epoch [1/10], batch: [34/58, loss: 5.4433]\n",
      "Epoch [1/10], batch: [35/58, loss: 4.0815]\n",
      "Epoch [1/10], batch: [36/58, loss: 4.7858]\n",
      "Epoch [1/10], batch: [37/58, loss: 5.9727]\n",
      "Epoch [1/10], batch: [38/58, loss: 5.4717]\n",
      "Epoch [1/10], batch: [39/58, loss: 5.2008]\n",
      "Epoch [1/10], batch: [40/58, loss: 4.5924]\n",
      "Epoch [1/10], batch: [41/58, loss: 3.9878]\n",
      "Epoch [1/10], batch: [42/58, loss: 3.3111]\n",
      "Epoch [1/10], batch: [43/58, loss: 4.8432]\n",
      "Epoch [1/10], batch: [44/58, loss: 3.8338]\n",
      "Epoch [1/10], batch: [45/58, loss: 5.3678]\n",
      "Epoch [1/10], batch: [46/58, loss: 5.5293]\n",
      "Epoch [1/10], batch: [47/58, loss: 5.8426]\n",
      "Epoch [1/10], batch: [48/58, loss: 5.1622]\n",
      "Epoch [1/10], batch: [49/58, loss: 3.0309]\n",
      "Epoch [1/10], batch: [50/58, loss: 4.3479]\n",
      "Epoch [1/10], batch: [51/58, loss: 6.0197]\n",
      "Epoch [1/10], batch: [52/58, loss: 3.9094]\n",
      "Epoch [1/10], batch: [53/58, loss: 5.2900]\n",
      "Epoch [1/10], batch: [54/58, loss: 3.1081]\n",
      "Epoch [1/10], batch: [55/58, loss: 3.7873]\n",
      "Epoch [1/10], batch: [56/58, loss: 3.3766]\n",
      "Epoch [1/10], batch: [57/58, loss: 4.8136]\n",
      "Epoch [2/10], batch: [0/58, loss: 2.6859]\n",
      "Epoch [2/10], batch: [1/58, loss: 6.2804]\n",
      "Epoch [2/10], batch: [2/58, loss: 2.6354]\n",
      "Epoch [2/10], batch: [3/58, loss: 4.0026]\n",
      "Epoch [2/10], batch: [4/58, loss: 1.8538]\n",
      "Epoch [2/10], batch: [5/58, loss: 5.6920]\n",
      "Epoch [2/10], batch: [6/58, loss: 3.5333]\n",
      "Epoch [2/10], batch: [7/58, loss: 4.7180]\n",
      "Epoch [2/10], batch: [8/58, loss: 3.7405]\n",
      "Epoch [2/10], batch: [9/58, loss: 6.6694]\n",
      "Epoch [2/10], batch: [10/58, loss: 5.3608]\n",
      "Epoch [2/10], batch: [11/58, loss: 3.9004]\n",
      "Epoch [2/10], batch: [12/58, loss: 3.6116]\n",
      "Epoch [2/10], batch: [13/58, loss: 5.6433]\n",
      "Epoch [2/10], batch: [14/58, loss: 7.3507]\n",
      "Epoch [2/10], batch: [15/58, loss: 2.7504]\n",
      "Epoch [2/10], batch: [16/58, loss: 3.1290]\n",
      "Epoch [2/10], batch: [17/58, loss: 5.5335]\n",
      "Epoch [2/10], batch: [18/58, loss: 2.8790]\n",
      "Epoch [2/10], batch: [19/58, loss: 3.8950]\n",
      "Epoch [2/10], batch: [20/58, loss: 4.3052]\n",
      "Epoch [2/10], batch: [21/58, loss: 2.5902]\n",
      "Epoch [2/10], batch: [22/58, loss: 2.4390]\n",
      "Epoch [2/10], batch: [23/58, loss: 3.5497]\n",
      "Epoch [2/10], batch: [24/58, loss: 4.5752]\n",
      "Epoch [2/10], batch: [25/58, loss: 5.1687]\n",
      "Epoch [2/10], batch: [26/58, loss: 3.8162]\n",
      "Epoch [2/10], batch: [27/58, loss: 3.2298]\n",
      "Epoch [2/10], batch: [28/58, loss: 3.7484]\n",
      "Epoch [2/10], batch: [29/58, loss: 1.6222]\n",
      "Epoch [2/10], batch: [30/58, loss: 2.1138]\n",
      "Epoch [2/10], batch: [31/58, loss: 4.1002]\n",
      "Epoch [2/10], batch: [32/58, loss: 5.7776]\n",
      "Epoch [2/10], batch: [33/58, loss: 4.8204]\n",
      "Epoch [2/10], batch: [34/58, loss: 4.9928]\n",
      "Epoch [2/10], batch: [35/58, loss: 3.6901]\n",
      "Epoch [2/10], batch: [36/58, loss: 4.4569]\n",
      "Epoch [2/10], batch: [37/58, loss: 5.6605]\n",
      "Epoch [2/10], batch: [38/58, loss: 5.0888]\n",
      "Epoch [2/10], batch: [39/58, loss: 4.8091]\n",
      "Epoch [2/10], batch: [40/58, loss: 4.3308]\n",
      "Epoch [2/10], batch: [41/58, loss: 3.5936]\n",
      "Epoch [2/10], batch: [42/58, loss: 2.9326]\n",
      "Epoch [2/10], batch: [43/58, loss: 4.3906]\n",
      "Epoch [2/10], batch: [44/58, loss: 3.3870]\n",
      "Epoch [2/10], batch: [45/58, loss: 5.0655]\n",
      "Epoch [2/10], batch: [46/58, loss: 5.0796]\n",
      "Epoch [2/10], batch: [47/58, loss: 5.3469]\n",
      "Epoch [2/10], batch: [48/58, loss: 4.7521]\n",
      "Epoch [2/10], batch: [49/58, loss: 2.7061]\n",
      "Epoch [2/10], batch: [50/58, loss: 4.0148]\n",
      "Epoch [2/10], batch: [51/58, loss: 5.6691]\n",
      "Epoch [2/10], batch: [52/58, loss: 3.5091]\n",
      "Epoch [2/10], batch: [53/58, loss: 4.7669]\n",
      "Epoch [2/10], batch: [54/58, loss: 2.7468]\n",
      "Epoch [2/10], batch: [55/58, loss: 3.4667]\n",
      "Epoch [2/10], batch: [56/58, loss: 3.0124]\n",
      "Epoch [2/10], batch: [57/58, loss: 4.3702]\n",
      "Epoch [3/10], batch: [0/58, loss: 2.4384]\n",
      "Epoch [3/10], batch: [1/58, loss: 6.0015]\n",
      "Epoch [3/10], batch: [2/58, loss: 2.4213]\n",
      "Epoch [3/10], batch: [3/58, loss: 3.6861]\n",
      "Epoch [3/10], batch: [4/58, loss: 1.6714]\n",
      "Epoch [3/10], batch: [5/58, loss: 5.2811]\n",
      "Epoch [3/10], batch: [6/58, loss: 3.0432]\n",
      "Epoch [3/10], batch: [7/58, loss: 4.3724]\n",
      "Epoch [3/10], batch: [8/58, loss: 3.4430]\n",
      "Epoch [3/10], batch: [9/58, loss: 6.1836]\n",
      "Epoch [3/10], batch: [10/58, loss: 4.8946]\n",
      "Epoch [3/10], batch: [11/58, loss: 3.5869]\n",
      "Epoch [3/10], batch: [12/58, loss: 3.3144]\n",
      "Epoch [3/10], batch: [13/58, loss: 5.1872]\n",
      "Epoch [3/10], batch: [14/58, loss: 6.8422]\n",
      "Epoch [3/10], batch: [15/58, loss: 2.5475]\n",
      "Epoch [3/10], batch: [16/58, loss: 2.8200]\n",
      "Epoch [3/10], batch: [17/58, loss: 5.1192]\n",
      "Epoch [3/10], batch: [18/58, loss: 2.6295]\n",
      "Epoch [3/10], batch: [19/58, loss: 3.5142]\n",
      "Epoch [3/10], batch: [20/58, loss: 3.9286]\n",
      "Epoch [3/10], batch: [21/58, loss: 2.3553]\n",
      "Epoch [3/10], batch: [22/58, loss: 2.2462]\n",
      "Epoch [3/10], batch: [23/58, loss: 3.3176]\n",
      "Epoch [3/10], batch: [24/58, loss: 4.2896]\n",
      "Epoch [3/10], batch: [25/58, loss: 4.7845]\n",
      "Epoch [3/10], batch: [26/58, loss: 3.4540]\n",
      "Epoch [3/10], batch: [27/58, loss: 2.8845]\n",
      "Epoch [3/10], batch: [28/58, loss: 3.3389]\n",
      "Epoch [3/10], batch: [29/58, loss: 1.3738]\n",
      "Epoch [3/10], batch: [30/58, loss: 1.7583]\n",
      "Epoch [3/10], batch: [31/58, loss: 3.7943]\n",
      "Epoch [3/10], batch: [32/58, loss: 5.2806]\n",
      "Epoch [3/10], batch: [33/58, loss: 4.3579]\n",
      "Epoch [3/10], batch: [34/58, loss: 4.6015]\n",
      "Epoch [3/10], batch: [35/58, loss: 3.3518]\n",
      "Epoch [3/10], batch: [36/58, loss: 4.1585]\n",
      "Epoch [3/10], batch: [37/58, loss: 5.3750]\n",
      "Epoch [3/10], batch: [38/58, loss: 4.7336]\n",
      "Epoch [3/10], batch: [39/58, loss: 4.4491]\n",
      "Epoch [3/10], batch: [40/58, loss: 4.0828]\n",
      "Epoch [3/10], batch: [41/58, loss: 3.2272]\n",
      "Epoch [3/10], batch: [42/58, loss: 2.5956]\n",
      "Epoch [3/10], batch: [43/58, loss: 3.9633]\n",
      "Epoch [3/10], batch: [44/58, loss: 2.9741]\n",
      "Epoch [3/10], batch: [45/58, loss: 4.7837]\n",
      "Epoch [3/10], batch: [46/58, loss: 4.6568]\n",
      "Epoch [3/10], batch: [47/58, loss: 4.8864]\n",
      "Epoch [3/10], batch: [48/58, loss: 4.3709]\n",
      "Epoch [3/10], batch: [49/58, loss: 2.4077]\n",
      "Epoch [3/10], batch: [50/58, loss: 3.7142]\n",
      "Epoch [3/10], batch: [51/58, loss: 5.3415]\n",
      "Epoch [3/10], batch: [52/58, loss: 3.1466]\n",
      "Epoch [3/10], batch: [53/58, loss: 4.2952]\n",
      "Epoch [3/10], batch: [54/58, loss: 2.4299]\n",
      "Epoch [3/10], batch: [55/58, loss: 3.1724]\n",
      "Epoch [3/10], batch: [56/58, loss: 2.7071]\n",
      "Epoch [3/10], batch: [57/58, loss: 3.9666]\n",
      "Epoch [4/10], batch: [0/58, loss: 2.1976]\n",
      "Epoch [4/10], batch: [1/58, loss: 5.7253]\n",
      "Epoch [4/10], batch: [2/58, loss: 2.2062]\n",
      "Epoch [4/10], batch: [3/58, loss: 3.3965]\n",
      "Epoch [4/10], batch: [4/58, loss: 1.5284]\n",
      "Epoch [4/10], batch: [5/58, loss: 4.8835]\n",
      "Epoch [4/10], batch: [6/58, loss: 2.6047]\n",
      "Epoch [4/10], batch: [7/58, loss: 4.0349]\n",
      "Epoch [4/10], batch: [8/58, loss: 3.1472]\n",
      "Epoch [4/10], batch: [9/58, loss: 5.6981]\n",
      "Epoch [4/10], batch: [10/58, loss: 4.4496]\n",
      "Epoch [4/10], batch: [11/58, loss: 3.2846]\n",
      "Epoch [4/10], batch: [12/58, loss: 3.0332]\n",
      "Epoch [4/10], batch: [13/58, loss: 4.7536]\n",
      "Epoch [4/10], batch: [14/58, loss: 6.3301]\n",
      "Epoch [4/10], batch: [15/58, loss: 2.3504]\n",
      "Epoch [4/10], batch: [16/58, loss: 2.5197]\n",
      "Epoch [4/10], batch: [17/58, loss: 4.7100]\n",
      "Epoch [4/10], batch: [18/58, loss: 2.3878]\n",
      "Epoch [4/10], batch: [19/58, loss: 3.1495]\n",
      "Epoch [4/10], batch: [20/58, loss: 3.5623]\n",
      "Epoch [4/10], batch: [21/58, loss: 2.1309]\n",
      "Epoch [4/10], batch: [22/58, loss: 2.0668]\n",
      "Epoch [4/10], batch: [23/58, loss: 3.0914]\n",
      "Epoch [4/10], batch: [24/58, loss: 4.0084]\n",
      "Epoch [4/10], batch: [25/58, loss: 4.4160]\n",
      "Epoch [4/10], batch: [26/58, loss: 3.1128]\n",
      "Epoch [4/10], batch: [27/58, loss: 2.5711]\n",
      "Epoch [4/10], batch: [28/58, loss: 2.9530]\n",
      "Epoch [4/10], batch: [29/58, loss: 1.1701]\n",
      "Epoch [4/10], batch: [30/58, loss: 1.4485]\n",
      "Epoch [4/10], batch: [31/58, loss: 3.4993]\n",
      "Epoch [4/10], batch: [32/58, loss: 4.8029]\n",
      "Epoch [4/10], batch: [33/58, loss: 3.9214]\n",
      "Epoch [4/10], batch: [34/58, loss: 4.2377]\n",
      "Epoch [4/10], batch: [35/58, loss: 3.0285]\n",
      "Epoch [4/10], batch: [36/58, loss: 3.8767]\n",
      "Epoch [4/10], batch: [37/58, loss: 5.1020]\n",
      "Epoch [4/10], batch: [38/58, loss: 4.3982]\n",
      "Epoch [4/10], batch: [39/58, loss: 4.1078]\n",
      "Epoch [4/10], batch: [40/58, loss: 3.8436]\n",
      "Epoch [4/10], batch: [41/58, loss: 2.8849]\n",
      "Epoch [4/10], batch: [42/58, loss: 2.2883]\n",
      "Epoch [4/10], batch: [43/58, loss: 3.5591]\n",
      "Epoch [4/10], batch: [44/58, loss: 2.5912]\n",
      "Epoch [4/10], batch: [45/58, loss: 4.5175]\n",
      "Epoch [4/10], batch: [46/58, loss: 4.2551]\n",
      "Epoch [4/10], batch: [47/58, loss: 4.4528]\n",
      "Epoch [4/10], batch: [48/58, loss: 4.0101]\n",
      "Epoch [4/10], batch: [49/58, loss: 2.1340]\n",
      "Epoch [4/10], batch: [50/58, loss: 3.4357]\n",
      "Epoch [4/10], batch: [51/58, loss: 5.0306]\n",
      "Epoch [4/10], batch: [52/58, loss: 2.8122]\n",
      "Epoch [4/10], batch: [53/58, loss: 3.8573]\n",
      "Epoch [4/10], batch: [54/58, loss: 2.1502]\n",
      "Epoch [4/10], batch: [55/58, loss: 2.8998]\n",
      "Epoch [4/10], batch: [56/58, loss: 2.4452]\n",
      "Epoch [4/10], batch: [57/58, loss: 3.5906]\n",
      "Epoch [5/10], batch: [0/58, loss: 1.9726]\n",
      "Epoch [5/10], batch: [1/58, loss: 5.4572]\n",
      "Epoch [5/10], batch: [2/58, loss: 2.0005]\n",
      "Epoch [5/10], batch: [3/58, loss: 3.1354]\n",
      "Epoch [5/10], batch: [4/58, loss: 1.4181]\n",
      "Epoch [5/10], batch: [5/58, loss: 4.5023]\n",
      "Epoch [5/10], batch: [6/58, loss: 2.2225]\n",
      "Epoch [5/10], batch: [7/58, loss: 3.7131]\n",
      "Epoch [5/10], batch: [8/58, loss: 2.8629]\n",
      "Epoch [5/10], batch: [9/58, loss: 5.2260]\n",
      "Epoch [5/10], batch: [10/58, loss: 4.0295]\n",
      "Epoch [5/10], batch: [11/58, loss: 3.0006]\n",
      "Epoch [5/10], batch: [12/58, loss: 2.7714]\n",
      "Epoch [5/10], batch: [13/58, loss: 4.3452]\n",
      "Epoch [5/10], batch: [14/58, loss: 5.8267]\n",
      "Epoch [5/10], batch: [15/58, loss: 2.1626]\n",
      "Epoch [5/10], batch: [16/58, loss: 2.2374]\n",
      "Epoch [5/10], batch: [17/58, loss: 4.3126]\n",
      "Epoch [5/10], batch: [18/58, loss: 2.1594]\n",
      "Epoch [5/10], batch: [19/58, loss: 2.8080]\n",
      "Epoch [5/10], batch: [20/58, loss: 3.2134]\n",
      "Epoch [5/10], batch: [21/58, loss: 1.9222]\n",
      "Epoch [5/10], batch: [22/58, loss: 1.9024]\n",
      "Epoch [5/10], batch: [23/58, loss: 2.8746]\n",
      "Epoch [5/10], batch: [24/58, loss: 3.7350]\n",
      "Epoch [5/10], batch: [25/58, loss: 4.0620]\n",
      "Epoch [5/10], batch: [26/58, loss: 2.7953]\n",
      "Epoch [5/10], batch: [27/58, loss: 2.2899]\n",
      "Epoch [5/10], batch: [28/58, loss: 2.5941]\n",
      "Epoch [5/10], batch: [29/58, loss: 1.0053]\n",
      "Epoch [5/10], batch: [30/58, loss: 1.1883]\n",
      "Epoch [5/10], batch: [31/58, loss: 3.2139]\n",
      "Epoch [5/10], batch: [32/58, loss: 4.3450]\n",
      "Epoch [5/10], batch: [33/58, loss: 3.5102]\n",
      "Epoch [5/10], batch: [34/58, loss: 3.8963]\n",
      "Epoch [5/10], batch: [35/58, loss: 2.7216]\n",
      "Epoch [5/10], batch: [36/58, loss: 3.6099]\n",
      "Epoch [5/10], batch: [37/58, loss: 4.8402]\n",
      "Epoch [5/10], batch: [38/58, loss: 4.0816]\n",
      "Epoch [5/10], batch: [39/58, loss: 3.7838]\n",
      "Epoch [5/10], batch: [40/58, loss: 3.6129]\n",
      "Epoch [5/10], batch: [41/58, loss: 2.5675]\n",
      "Epoch [5/10], batch: [42/58, loss: 2.0108]\n",
      "Epoch [5/10], batch: [43/58, loss: 3.1780]\n",
      "Epoch [5/10], batch: [44/58, loss: 2.2395]\n",
      "Epoch [5/10], batch: [45/58, loss: 4.2650]\n",
      "Epoch [5/10], batch: [46/58, loss: 3.8720]\n",
      "Epoch [5/10], batch: [47/58, loss: 4.0425]\n",
      "Epoch [5/10], batch: [48/58, loss: 3.6662]\n",
      "Epoch [5/10], batch: [49/58, loss: 1.8849]\n",
      "Epoch [5/10], batch: [50/58, loss: 3.1760]\n",
      "Epoch [5/10], batch: [51/58, loss: 4.7332]\n",
      "Epoch [5/10], batch: [52/58, loss: 2.5028]\n",
      "Epoch [5/10], batch: [53/58, loss: 3.4465]\n",
      "Epoch [5/10], batch: [54/58, loss: 1.9039]\n",
      "Epoch [5/10], batch: [55/58, loss: 2.6462]\n",
      "Epoch [5/10], batch: [56/58, loss: 2.2158]\n",
      "Epoch [5/10], batch: [57/58, loss: 3.2368]\n",
      "Epoch [6/10], batch: [0/58, loss: 1.7650]\n",
      "Epoch [6/10], batch: [1/58, loss: 5.1966]\n",
      "Epoch [6/10], batch: [2/58, loss: 1.8081]\n",
      "Epoch [6/10], batch: [3/58, loss: 2.8991]\n",
      "Epoch [6/10], batch: [4/58, loss: 1.3321]\n",
      "Epoch [6/10], batch: [5/58, loss: 4.1373]\n",
      "Epoch [6/10], batch: [6/58, loss: 1.8928]\n",
      "Epoch [6/10], batch: [7/58, loss: 3.4076]\n",
      "Epoch [6/10], batch: [8/58, loss: 2.5933]\n",
      "Epoch [6/10], batch: [9/58, loss: 4.7695]\n",
      "Epoch [6/10], batch: [10/58, loss: 3.6329]\n",
      "Epoch [6/10], batch: [11/58, loss: 2.7368]\n",
      "Epoch [6/10], batch: [12/58, loss: 2.5288]\n",
      "Epoch [6/10], batch: [13/58, loss: 3.9589]\n",
      "Epoch [6/10], batch: [14/58, loss: 5.3352]\n",
      "Epoch [6/10], batch: [15/58, loss: 1.9847]\n",
      "Epoch [6/10], batch: [16/58, loss: 1.9774]\n",
      "Epoch [6/10], batch: [17/58, loss: 3.9284]\n",
      "Epoch [6/10], batch: [18/58, loss: 1.9460]\n",
      "Epoch [6/10], batch: [19/58, loss: 2.4925]\n",
      "Epoch [6/10], batch: [20/58, loss: 2.8844]\n",
      "Epoch [6/10], batch: [21/58, loss: 1.7315]\n",
      "Epoch [6/10], batch: [22/58, loss: 1.7527]\n",
      "Epoch [6/10], batch: [23/58, loss: 2.6676]\n",
      "Epoch [6/10], batch: [24/58, loss: 3.4700]\n",
      "Epoch [6/10], batch: [25/58, loss: 3.7217]\n",
      "Epoch [6/10], batch: [26/58, loss: 2.5016]\n",
      "Epoch [6/10], batch: [27/58, loss: 2.0391]\n",
      "Epoch [6/10], batch: [28/58, loss: 2.2644]\n",
      "Epoch [6/10], batch: [29/58, loss: 0.8729]\n",
      "Epoch [6/10], batch: [30/58, loss: 0.9770]\n",
      "Epoch [6/10], batch: [31/58, loss: 2.9373]\n",
      "Epoch [6/10], batch: [32/58, loss: 3.9063]\n",
      "Epoch [6/10], batch: [33/58, loss: 3.1229]\n",
      "Epoch [6/10], batch: [34/58, loss: 3.5730]\n",
      "Epoch [6/10], batch: [35/58, loss: 2.4324]\n",
      "Epoch [6/10], batch: [36/58, loss: 3.3565]\n",
      "Epoch [6/10], batch: [37/58, loss: 4.5883]\n",
      "Epoch [6/10], batch: [38/58, loss: 3.7824]\n",
      "Epoch [6/10], batch: [39/58, loss: 3.4754]\n",
      "Epoch [6/10], batch: [40/58, loss: 3.3899]\n",
      "Epoch [6/10], batch: [41/58, loss: 2.2754]\n",
      "Epoch [6/10], batch: [42/58, loss: 1.7630]\n",
      "Epoch [6/10], batch: [43/58, loss: 2.8199]\n",
      "Epoch [6/10], batch: [44/58, loss: 1.9208]\n",
      "Epoch [6/10], batch: [45/58, loss: 4.0245]\n",
      "Epoch [6/10], batch: [46/58, loss: 3.5058]\n",
      "Epoch [6/10], batch: [47/58, loss: 3.6529]\n",
      "Epoch [6/10], batch: [48/58, loss: 3.3369]\n",
      "Epoch [6/10], batch: [49/58, loss: 1.6602]\n",
      "Epoch [6/10], batch: [50/58, loss: 2.9327]\n",
      "Epoch [6/10], batch: [51/58, loss: 4.4467]\n",
      "Epoch [6/10], batch: [52/58, loss: 2.2167]\n",
      "Epoch [6/10], batch: [53/58, loss: 3.0600]\n",
      "Epoch [6/10], batch: [54/58, loss: 1.6883]\n",
      "Epoch [6/10], batch: [55/58, loss: 2.4096]\n",
      "Epoch [6/10], batch: [56/58, loss: 2.0117]\n",
      "Epoch [6/10], batch: [57/58, loss: 2.9019]\n",
      "Epoch [7/10], batch: [0/58, loss: 1.5753]\n",
      "Epoch [7/10], batch: [1/58, loss: 4.9424]\n",
      "Epoch [7/10], batch: [2/58, loss: 1.6310]\n",
      "Epoch [7/10], batch: [3/58, loss: 2.6835]\n",
      "Epoch [7/10], batch: [4/58, loss: 1.2636]\n",
      "Epoch [7/10], batch: [5/58, loss: 3.7879]\n",
      "Epoch [7/10], batch: [6/58, loss: 1.6112]\n",
      "Epoch [7/10], batch: [7/58, loss: 3.1179]\n",
      "Epoch [7/10], batch: [8/58, loss: 2.3393]\n",
      "Epoch [7/10], batch: [9/58, loss: 4.3285]\n",
      "Epoch [7/10], batch: [10/58, loss: 3.2583]\n",
      "Epoch [7/10], batch: [11/58, loss: 2.4939]\n",
      "Epoch [7/10], batch: [12/58, loss: 2.3044]\n",
      "Epoch [7/10], batch: [13/58, loss: 3.5913]\n",
      "Epoch [7/10], batch: [14/58, loss: 4.8558]\n",
      "Epoch [7/10], batch: [15/58, loss: 1.8170]\n",
      "Epoch [7/10], batch: [16/58, loss: 1.7422]\n",
      "Epoch [7/10], batch: [17/58, loss: 3.5577]\n",
      "Epoch [7/10], batch: [18/58, loss: 1.7483]\n",
      "Epoch [7/10], batch: [19/58, loss: 2.2040]\n",
      "Epoch [7/10], batch: [20/58, loss: 2.5766]\n",
      "Epoch [7/10], batch: [21/58, loss: 1.5598]\n",
      "Epoch [7/10], batch: [22/58, loss: 1.6167]\n",
      "Epoch [7/10], batch: [23/58, loss: 2.4704]\n",
      "Epoch [7/10], batch: [24/58, loss: 3.2131]\n",
      "Epoch [7/10], batch: [25/58, loss: 3.3947]\n",
      "Epoch [7/10], batch: [26/58, loss: 2.2311]\n",
      "Epoch [7/10], batch: [27/58, loss: 1.8158]\n",
      "Epoch [7/10], batch: [28/58, loss: 1.9665]\n",
      "Epoch [7/10], batch: [29/58, loss: 0.7670]\n",
      "Epoch [7/10], batch: [30/58, loss: 0.8099]\n",
      "Epoch [7/10], batch: [31/58, loss: 2.6695]\n",
      "Epoch [7/10], batch: [32/58, loss: 3.4868]\n",
      "Epoch [7/10], batch: [33/58, loss: 2.7588]\n",
      "Epoch [7/10], batch: [34/58, loss: 3.2650]\n",
      "Epoch [7/10], batch: [35/58, loss: 2.1623]\n",
      "Epoch [7/10], batch: [36/58, loss: 3.1151]\n",
      "Epoch [7/10], batch: [37/58, loss: 4.3448]\n",
      "Epoch [7/10], batch: [38/58, loss: 3.4992]\n",
      "Epoch [7/10], batch: [39/58, loss: 3.1816]\n",
      "Epoch [7/10], batch: [40/58, loss: 3.1740]\n",
      "Epoch [7/10], batch: [41/58, loss: 2.0087]\n",
      "Epoch [7/10], batch: [42/58, loss: 1.5443]\n",
      "Epoch [7/10], batch: [43/58, loss: 2.4856]\n",
      "Epoch [7/10], batch: [44/58, loss: 1.6373]\n",
      "Epoch [7/10], batch: [45/58, loss: 3.7944]\n",
      "Epoch [7/10], batch: [46/58, loss: 3.1557]\n",
      "Epoch [7/10], batch: [47/58, loss: 3.2823]\n",
      "Epoch [7/10], batch: [48/58, loss: 3.0208]\n",
      "Epoch [7/10], batch: [49/58, loss: 1.4594]\n",
      "Epoch [7/10], batch: [50/58, loss: 2.7040]\n",
      "Epoch [7/10], batch: [51/58, loss: 4.1692]\n",
      "Epoch [7/10], batch: [52/58, loss: 1.9532]\n",
      "Epoch [7/10], batch: [53/58, loss: 2.6976]\n",
      "Epoch [7/10], batch: [54/58, loss: 1.5008]\n",
      "Epoch [7/10], batch: [55/58, loss: 2.1885]\n",
      "Epoch [7/10], batch: [56/58, loss: 1.8276]\n",
      "Epoch [7/10], batch: [57/58, loss: 2.5848]\n",
      "Epoch [8/10], batch: [0/58, loss: 1.4031]\n",
      "Epoch [8/10], batch: [1/58, loss: 4.6934]\n",
      "Epoch [8/10], batch: [2/58, loss: 1.4701]\n",
      "Epoch [8/10], batch: [3/58, loss: 2.4847]\n",
      "Epoch [8/10], batch: [4/58, loss: 1.2074]\n",
      "Epoch [8/10], batch: [5/58, loss: 3.4533]\n",
      "Epoch [8/10], batch: [6/58, loss: 1.3739]\n",
      "Epoch [8/10], batch: [7/58, loss: 2.8434]\n",
      "Epoch [8/10], batch: [8/58, loss: 2.1017]\n",
      "Epoch [8/10], batch: [9/58, loss: 3.9028]\n",
      "Epoch [8/10], batch: [10/58, loss: 2.9050]\n",
      "Epoch [8/10], batch: [11/58, loss: 2.2716]\n",
      "Epoch [8/10], batch: [12/58, loss: 2.0968]\n",
      "Epoch [8/10], batch: [13/58, loss: 3.2402]\n",
      "Epoch [8/10], batch: [14/58, loss: 4.3880]\n",
      "Epoch [8/10], batch: [15/58, loss: 1.6594]\n",
      "Epoch [8/10], batch: [16/58, loss: 1.5329]\n",
      "Epoch [8/10], batch: [17/58, loss: 3.2005]\n",
      "Epoch [8/10], batch: [18/58, loss: 1.5662]\n",
      "Epoch [8/10], batch: [19/58, loss: 1.9432]\n",
      "Epoch [8/10], batch: [20/58, loss: 2.2910]\n",
      "Epoch [8/10], batch: [21/58, loss: 1.4071]\n",
      "Epoch [8/10], batch: [22/58, loss: 1.4933]\n",
      "Epoch [8/10], batch: [23/58, loss: 2.2826]\n",
      "Epoch [8/10], batch: [24/58, loss: 2.9641]\n",
      "Epoch [8/10], batch: [25/58, loss: 3.0813]\n",
      "Epoch [8/10], batch: [26/58, loss: 1.9835]\n",
      "Epoch [8/10], batch: [27/58, loss: 1.6171]\n",
      "Epoch [8/10], batch: [28/58, loss: 1.7025]\n",
      "Epoch [8/10], batch: [29/58, loss: 0.6820]\n",
      "Epoch [8/10], batch: [30/58, loss: 0.6801]\n",
      "Epoch [8/10], batch: [31/58, loss: 2.4111]\n",
      "Epoch [8/10], batch: [32/58, loss: 3.0872]\n",
      "Epoch [8/10], batch: [33/58, loss: 2.4189]\n",
      "Epoch [8/10], batch: [34/58, loss: 2.9704]\n",
      "Epoch [8/10], batch: [35/58, loss: 1.9130]\n",
      "Epoch [8/10], batch: [36/58, loss: 2.8847]\n",
      "Epoch [8/10], batch: [37/58, loss: 4.1083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], batch: [38/58, loss: 3.2309]\n",
      "Epoch [8/10], batch: [39/58, loss: 2.9018]\n",
      "Epoch [8/10], batch: [40/58, loss: 2.9649]\n",
      "Epoch [8/10], batch: [41/58, loss: 1.7677]\n",
      "Epoch [8/10], batch: [42/58, loss: 1.3538]\n",
      "Epoch [8/10], batch: [43/58, loss: 2.1766]\n",
      "Epoch [8/10], batch: [44/58, loss: 1.3906]\n",
      "Epoch [8/10], batch: [45/58, loss: 3.5733]\n",
      "Epoch [8/10], batch: [46/58, loss: 2.8215]\n",
      "Epoch [8/10], batch: [47/58, loss: 2.9303]\n",
      "Epoch [8/10], batch: [48/58, loss: 2.7174]\n",
      "Epoch [8/10], batch: [49/58, loss: 1.2820]\n",
      "Epoch [8/10], batch: [50/58, loss: 2.4885]\n",
      "Epoch [8/10], batch: [51/58, loss: 3.8988]\n",
      "Epoch [8/10], batch: [52/58, loss: 1.7122]\n",
      "Epoch [8/10], batch: [53/58, loss: 2.3607]\n",
      "Epoch [8/10], batch: [54/58, loss: 1.3390]\n",
      "Epoch [8/10], batch: [55/58, loss: 1.9818]\n",
      "Epoch [8/10], batch: [56/58, loss: 1.6602]\n",
      "Epoch [8/10], batch: [57/58, loss: 2.2851]\n",
      "Epoch [9/10], batch: [0/58, loss: 1.2482]\n",
      "Epoch [9/10], batch: [1/58, loss: 4.4485]\n",
      "Epoch [9/10], batch: [2/58, loss: 1.3257]\n",
      "Epoch [9/10], batch: [3/58, loss: 2.2993]\n",
      "Epoch [9/10], batch: [4/58, loss: 1.1596]\n",
      "Epoch [9/10], batch: [5/58, loss: 3.1334]\n",
      "Epoch [9/10], batch: [6/58, loss: 1.1765]\n",
      "Epoch [9/10], batch: [7/58, loss: 2.5838]\n",
      "Epoch [9/10], batch: [8/58, loss: 1.8811]\n",
      "Epoch [9/10], batch: [9/58, loss: 3.4922]\n",
      "Epoch [9/10], batch: [10/58, loss: 2.5737]\n",
      "Epoch [9/10], batch: [11/58, loss: 2.0695]\n",
      "Epoch [9/10], batch: [12/58, loss: 1.9052]\n",
      "Epoch [9/10], batch: [13/58, loss: 2.9044]\n",
      "Epoch [9/10], batch: [14/58, loss: 3.9314]\n",
      "Epoch [9/10], batch: [15/58, loss: 1.5120]\n",
      "Epoch [9/10], batch: [16/58, loss: 1.3496]\n",
      "Epoch [9/10], batch: [17/58, loss: 2.8574]\n",
      "Epoch [9/10], batch: [18/58, loss: 1.3996]\n",
      "Epoch [9/10], batch: [19/58, loss: 1.7099]\n",
      "Epoch [9/10], batch: [20/58, loss: 2.0285]\n",
      "Epoch [9/10], batch: [21/58, loss: 1.2727]\n",
      "Epoch [9/10], batch: [22/58, loss: 1.3811]\n",
      "Epoch [9/10], batch: [23/58, loss: 2.1037]\n",
      "Epoch [9/10], batch: [24/58, loss: 2.7228]\n",
      "Epoch [9/10], batch: [25/58, loss: 2.7818]\n",
      "Epoch [9/10], batch: [26/58, loss: 1.7582]\n",
      "Epoch [9/10], batch: [27/58, loss: 1.4402]\n",
      "Epoch [9/10], batch: [28/58, loss: 1.4736]\n",
      "Epoch [9/10], batch: [29/58, loss: 0.6133]\n",
      "Epoch [9/10], batch: [30/58, loss: 0.5798]\n",
      "Epoch [9/10], batch: [31/58, loss: 2.1635]\n",
      "Epoch [9/10], batch: [32/58, loss: 2.7094]\n",
      "Epoch [9/10], batch: [33/58, loss: 2.1051]\n",
      "Epoch [9/10], batch: [34/58, loss: 2.6885]\n",
      "Epoch [9/10], batch: [35/58, loss: 1.6856]\n",
      "Epoch [9/10], batch: [36/58, loss: 2.6644]\n",
      "Epoch [9/10], batch: [37/58, loss: 3.8777]\n",
      "Epoch [9/10], batch: [38/58, loss: 2.9767]\n",
      "Epoch [9/10], batch: [39/58, loss: 2.6361]\n",
      "Epoch [9/10], batch: [40/58, loss: 2.7620]\n",
      "Epoch [9/10], batch: [41/58, loss: 1.5523]\n",
      "Epoch [9/10], batch: [42/58, loss: 1.1897]\n",
      "Epoch [9/10], batch: [43/58, loss: 1.8945]\n",
      "Epoch [9/10], batch: [44/58, loss: 1.1808]\n",
      "Epoch [9/10], batch: [45/58, loss: 3.3598]\n",
      "Epoch [9/10], batch: [46/58, loss: 2.5042]\n",
      "Epoch [9/10], batch: [47/58, loss: 2.5973]\n",
      "Epoch [9/10], batch: [48/58, loss: 2.4270]\n",
      "Epoch [9/10], batch: [49/58, loss: 1.1268]\n",
      "Epoch [9/10], batch: [50/58, loss: 2.2853]\n",
      "Epoch [9/10], batch: [51/58, loss: 3.6342]\n",
      "Epoch [9/10], batch: [52/58, loss: 1.4939]\n",
      "Epoch [9/10], batch: [53/58, loss: 2.0514]\n",
      "Epoch [9/10], batch: [54/58, loss: 1.2000]\n",
      "Epoch [9/10], batch: [55/58, loss: 1.7887]\n",
      "Epoch [9/10], batch: [56/58, loss: 1.5072]\n",
      "Epoch [9/10], batch: [57/58, loss: 2.0038]\n",
      "Epoch [10/10], batch: [0/58, loss: 1.1099]\n",
      "Epoch [10/10], batch: [1/58, loss: 4.2067]\n",
      "Epoch [10/10], batch: [2/58, loss: 1.1974]\n",
      "Epoch [10/10], batch: [3/58, loss: 2.1245]\n",
      "Epoch [10/10], batch: [4/58, loss: 1.1171]\n",
      "Epoch [10/10], batch: [5/58, loss: 2.8281]\n",
      "Epoch [10/10], batch: [6/58, loss: 1.0143]\n",
      "Epoch [10/10], batch: [7/58, loss: 2.3389]\n",
      "Epoch [10/10], batch: [8/58, loss: 1.6781]\n",
      "Epoch [10/10], batch: [9/58, loss: 3.0977]\n",
      "Epoch [10/10], batch: [10/58, loss: 2.2658]\n",
      "Epoch [10/10], batch: [11/58, loss: 1.8865]\n",
      "Epoch [10/10], batch: [12/58, loss: 1.7283]\n",
      "Epoch [10/10], batch: [13/58, loss: 2.5844]\n",
      "Epoch [10/10], batch: [14/58, loss: 3.4863]\n",
      "Epoch [10/10], batch: [15/58, loss: 1.3748]\n",
      "Epoch [10/10], batch: [16/58, loss: 1.1911]\n",
      "Epoch [10/10], batch: [17/58, loss: 2.5298]\n",
      "Epoch [10/10], batch: [18/58, loss: 1.2479]\n",
      "Epoch [10/10], batch: [19/58, loss: 1.5034]\n",
      "Epoch [10/10], batch: [20/58, loss: 1.7901]\n",
      "Epoch [10/10], batch: [21/58, loss: 1.1552]\n",
      "Epoch [10/10], batch: [22/58, loss: 1.2789]\n",
      "Epoch [10/10], batch: [23/58, loss: 1.9336]\n",
      "Epoch [10/10], batch: [24/58, loss: 2.4894]\n",
      "Epoch [10/10], batch: [25/58, loss: 2.4971]\n",
      "Epoch [10/10], batch: [26/58, loss: 1.5549]\n",
      "Epoch [10/10], batch: [27/58, loss: 1.2826]\n",
      "Epoch [10/10], batch: [28/58, loss: 1.2796]\n",
      "Epoch [10/10], batch: [29/58, loss: 0.5571]\n",
      "Epoch [10/10], batch: [30/58, loss: 0.5023]\n",
      "Epoch [10/10], batch: [31/58, loss: 1.9284]\n",
      "Epoch [10/10], batch: [32/58, loss: 2.3561]\n",
      "Epoch [10/10], batch: [33/58, loss: 1.8200]\n",
      "Epoch [10/10], batch: [34/58, loss: 2.4192]\n",
      "Epoch [10/10], batch: [35/58, loss: 1.4809]\n",
      "Epoch [10/10], batch: [36/58, loss: 2.4536]\n",
      "Epoch [10/10], batch: [37/58, loss: 3.6519]\n",
      "Epoch [10/10], batch: [38/58, loss: 2.7360]\n",
      "Epoch [10/10], batch: [39/58, loss: 2.3849]\n",
      "Epoch [10/10], batch: [40/58, loss: 2.5651]\n",
      "Epoch [10/10], batch: [41/58, loss: 1.3616]\n",
      "Epoch [10/10], batch: [42/58, loss: 1.0493]\n",
      "Epoch [10/10], batch: [43/58, loss: 1.6413]\n",
      "Epoch [10/10], batch: [44/58, loss: 1.0059]\n",
      "Epoch [10/10], batch: [45/58, loss: 3.1525]\n",
      "Epoch [10/10], batch: [46/58, loss: 2.2053]\n",
      "Epoch [10/10], batch: [47/58, loss: 2.2847]\n",
      "Epoch [10/10], batch: [48/58, loss: 2.1507]\n",
      "Epoch [10/10], batch: [49/58, loss: 0.9919]\n",
      "Epoch [10/10], batch: [50/58, loss: 2.0939]\n",
      "Epoch [10/10], batch: [51/58, loss: 3.3743]\n",
      "Epoch [10/10], batch: [52/58, loss: 1.2983]\n",
      "Epoch [10/10], batch: [53/58, loss: 1.7724]\n",
      "Epoch [10/10], batch: [54/58, loss: 1.0808]\n",
      "Epoch [10/10], batch: [55/58, loss: 1.6092]\n",
      "Epoch [10/10], batch: [56/58, loss: 1.3668]\n",
      "Epoch [10/10], batch: [57/58, loss: 1.7427]\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "num_batches = len(data_loader)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(d[0])        \n",
    "        loss = loss_func(y_pred, d[1])\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8912e6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.3167, -2.3323, -4.8522, -3.3945, -4.0869, -5.6906, -5.8298, -2.4287,\n",
       "        -4.3467, -6.1285, -5.7904, -5.9563, -5.7445, -4.5328, -6.3826, -4.0178,\n",
       "        -4.9200, -3.7772, -6.0492, -2.1705, -3.9264, -4.7247, -4.2075, -3.5732,\n",
       "        -5.9498, -5.4486, -4.6216, -4.4562, -4.8541, -5.8288, -4.1564, -6.3319,\n",
       "        -5.1742, -5.9197, -3.7224, -4.8586, -5.2240, -5.6489, -4.8130, -4.9374,\n",
       "        -4.6739, -2.9141, -2.7901, -4.4224, -4.9281, -3.8123, -4.7366, -6.4402,\n",
       "        -1.7427], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9282c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "\n",
      "Context: ['We', 'are', 'to', 'study']\n",
      "\n",
      "Prediction: about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damja\\AppData\\Local\\Temp\\ipykernel_22776\\1816555312.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_out = m(y_out).reshape(-1)\n"
     ]
    }
   ],
   "source": [
    "context = ['We', 'are', 'to', 'study']\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "pred = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f'Context: {context}\\n')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(pred).item()]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
