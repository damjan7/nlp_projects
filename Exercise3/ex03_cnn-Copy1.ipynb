{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facd63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score   \n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b9b02",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe5c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['mapping', 'test_labels', 'test_text', 'train_labels', 'train_text', 'val_labels', 'val_text']\n",
    "f1 = lambda file_name: f'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/{file_name}.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec56bc1",
   "metadata": {},
   "source": [
    "Below, we create all the variables containing the data. Also a variable called 'all_inputs' is created (after the preprocessing), which stores all the input data together. We will use this for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da24bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv(f1('mapping'), sep='\\t', names=['mapping', 'sentiment'])\n",
    "test_labels = pd.read_csv(f1('test_labels'), sep='\\t', names=['sentiment'])\n",
    "train_labels = pd.read_csv(f1('train_labels'), sep='\\t', names=['sentiment'])\n",
    "val_labels = pd.read_csv(f1('val_labels'), sep='\\t', names=['sentiment'])\n",
    "test_text = pd.read_csv(f1('test_text'), sep='\\t', names = ['input'])\n",
    "train_text = pd.read_csv(f1('train_text'), sep='\\t', names = ['input'])\n",
    "val_text = pd.read_csv(f1('val_text'), sep='\\t', names = ['input'])\n",
    "\n",
    "all_input = pd.concat([train_text, val_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253ff72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>â€œWorry is a down payment on a problem you may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>I get discouraged because I try for 5 fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>The @user are in contention and hosting @user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>@user @user @user @user @user as a fellow UP g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>You have a #problem? Yes! Can you do #somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>@user @user i will fight this guy! Don't insul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3257 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     â€œWorry is a down payment on a problem you may ...\n",
       "1     My roommate: it's okay that we can't spell bec...\n",
       "2     No but that's so cute. Atsu was probably shy a...\n",
       "3     Rooneys fucking untouchable isn't he? Been fuc...\n",
       "4     it's pretty depressing when u hit pan on ur fa...\n",
       "...                                                 ...\n",
       "3252  I get discouraged because I try for 5 fucking ...\n",
       "3253  The @user are in contention and hosting @user ...\n",
       "3254  @user @user @user @user @user as a fellow UP g...\n",
       "3255  You have a #problem? Yes! Can you do #somethin...\n",
       "3256  @user @user i will fight this guy! Don't insul...\n",
       "\n",
       "[3257 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de413a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapping</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mapping sentiment\n",
       "0        0     anger\n",
       "1        1       joy\n",
       "2        2  optimism\n",
       "3        3   sadness"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adc39f",
   "metadata": {},
   "source": [
    "As a first step, we will use only the 2 sentiments anger and joy (0 and 1). And then, as asked in the task description, we will exchange one of the sentiments. Hence, we will use joy and sadness in a second step (1 and 3). We will need this in later tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf6faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_joy_test_idx = test_labels['sentiment'].isin([0, 1])\n",
    "anger_joy_train_idx = train_labels['sentiment'].isin([0, 1])\n",
    "anger_joy_val_idx = val_labels['sentiment'].isin([0, 1])\n",
    "\n",
    "sadness_joy_test_idx = test_labels['sentiment'].isin([3, 1])\n",
    "sadness_joy_train_idx = train_labels['sentiment'].isin([3, 1])\n",
    "sadness_joy_val_idx = val_labels['sentiment'].isin([3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59455bcc",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Before we actually split the datasets, we will apply our preprocessing pipeline as it equally affects all the inputs, independently of the sentiment.\n",
    "\n",
    "Here, we apply following preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c968945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Deppression is real. Partners w/ #depressed p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user Interesting choice of words... Are you c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My visit to hospital for care triggered #traum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user Welcome to #MPSVT! We are delighted to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What makes you feel #joyful?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>I need a sparkling bodysuit . No occasion. Jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>@user I've finished reading it; simply mind-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>shaft abrasions from panties merely shifted to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>All this fake outrage. Y'all need to stop ðŸ¤£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>Would be ever so grateful if you could record ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1421 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     #Deppression is real. Partners w/ #depressed p...\n",
       "1     @user Interesting choice of words... Are you c...\n",
       "2     My visit to hospital for care triggered #traum...\n",
       "3     @user Welcome to #MPSVT! We are delighted to h...\n",
       "4                         What makes you feel #joyful? \n",
       "...                                                 ...\n",
       "1416  I need a sparkling bodysuit . No occasion. Jus...\n",
       "1417  @user I've finished reading it; simply mind-bl...\n",
       "1418  shaft abrasions from panties merely shifted to...\n",
       "1419       All this fake outrage. Y'all need to stop ðŸ¤£ \n",
       "1420  Would be ever so grateful if you could record ...\n",
       "\n",
       "[1421 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865128dc",
   "metadata": {},
   "source": [
    "The preprocessing steps are the same that we did in previous tasks. Hence, I will not go into detail about the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.word_to_ix = {}\n",
    "        self.ix_to_word = {}\n",
    "        self.context_dataset = []\n",
    "        self.vocab_size = None\n",
    "        \n",
    "    def convert_lowercase(self, x):\n",
    "        x = x.lower()\n",
    "        return x\n",
    "        \n",
    "    def remove_emoji(self, x):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', x)\n",
    "        \n",
    "    exclude = string.punctuation\n",
    "    \n",
    "    def remove_punc(self, x):\n",
    "        exclude = string.punctuation\n",
    "        return x.translate(str.maketrans('', '', exclude))\n",
    "    \n",
    "    def remove_special_chars(self, x):\n",
    "        x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "        return x\n",
    "\n",
    "    def remove_one_letter_words(self, x):\n",
    "        x = re.sub(r'(?:^| )\\w(?:$| )', ' ', x).strip()\n",
    "        return x\n",
    "    \n",
    "    # default is to apply all these preprocessing steps\n",
    "    def apply_preprocessing(self,\n",
    "                            lowercase=True,\n",
    "                            remove_emoji=True,\n",
    "                            remove_punc=True,\n",
    "                            remove_special_chars=True,\n",
    "                            remove_one_letter_words=True):\n",
    "        if lowercase:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.convert_lowercase)\n",
    "        if remove_emoji:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_emoji)\n",
    "        if remove_punc:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_punc)\n",
    "        if remove_special_chars:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_special_chars)\n",
    "        if remove_one_letter_words:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_one_letter_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbf947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = Preprocessing(test_text)\n",
    "cl1.apply_preprocessing()\n",
    "test_text_preprocessed = cl1.full_dataset\n",
    "\n",
    "cl2 = Preprocessing(val_text)\n",
    "cl2.apply_preprocessing()\n",
    "val_text_preprocessed = cl2.full_dataset\n",
    "\n",
    "cl3 = Preprocessing(train_text)\n",
    "cl3.apply_preprocessing()\n",
    "train_text_preprocessed = cl3.full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b56a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_preprocessed = pd.DataFrame(test_text['input'].apply(custom_tokenize))\n",
    "train_text_preprocessed = pd.DataFrame(train_text['input'].apply(custom_tokenize))\n",
    "val_text_preprocessed = pd.DataFrame(val_text['input'].apply(custom_tokenize))\n",
    "all_input_preprocessed = pd.concat([train_text_preprocessed, val_text_preprocessed, test_text_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d292665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[worry, is, down, payment, on, problem, you, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, roommate, its, okay, that, we, cant, spel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[no, but, thats, so, cute, atsu, was, probably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rooneys, fucking, untouchable, isnt, he, been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[its, pretty, depressing, when, hit, pan, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>[need, sparkling, bodysuit, no, occasion, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>[user, ive, finished, reading, it, simply, min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>[shaft, abrasions, from, panties, merely, shif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>[all, this, fake, outrage, yall, need, to, stop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>[would, be, ever, so, grateful, if, you, could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5052 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     [worry, is, down, payment, on, problem, you, m...\n",
       "1     [my, roommate, its, okay, that, we, cant, spel...\n",
       "2     [no, but, thats, so, cute, atsu, was, probably...\n",
       "3     [rooneys, fucking, untouchable, isnt, he, been...\n",
       "4     [its, pretty, depressing, when, hit, pan, on, ...\n",
       "...                                                 ...\n",
       "1416  [need, sparkling, bodysuit, no, occasion, just...\n",
       "1417  [user, ive, finished, reading, it, simply, min...\n",
       "1418  [shaft, abrasions, from, panties, merely, shif...\n",
       "1419   [all, this, fake, outrage, yall, need, to, stop]\n",
       "1420  [would, be, ever, so, grateful, if, you, could...\n",
       "\n",
       "[5052 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fcd19",
   "metadata": {},
   "source": [
    "Get the the length of the largest tweet, so we can create a padding for all tweets that contain fewer words. Also, let us define the needed funtion to create the padding. This is done as pytorch needs the inputs to be of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8eed954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_tweet = max(all_input_preprocessed['input'].str.len())\n",
    "max_len_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9427bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding(dataset, max_len):\n",
    "    \"\"\"\n",
    "    Creates a padding on the whole dataset such that each datapoint is of same length.\n",
    "    The length is given as input by max_len.\n",
    "    \"\"\"\n",
    "    for idx, row in enumerate(dataset['input']):\n",
    "        if len(row) < max_len:\n",
    "            tmp = len(row)\n",
    "            pad1 = (max_len - tmp) // 2\n",
    "            row = ['PADDING'] * pad1 + row\n",
    "            row = row + ['PADDING'] * (30 - tmp - pad1)\n",
    "            dataset['input'].iloc[idx] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "010f3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_padding(test_text_preprocessed, max_len_tweet)\n",
    "create_padding(train_text_preprocessed, max_len_tweet)\n",
    "create_padding(val_text_preprocessed, max_len_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de3346",
   "metadata": {},
   "source": [
    "Let's see an example of how a padded input datapoint looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4160014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'am',\n",
       " 'revolting',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_preprocessed.loc[5, 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bada739",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_preprocessed = pd.concat([train_text_preprocessed, val_text_preprocessed, test_text_preprocessed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea16dc5",
   "metadata": {},
   "source": [
    "Now, our data is in a somewhat nice format to work with. Every word is separated by a comma. As a next step, we want to create the word_to_ix and ix_to_word dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d666065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "cur_idx = 0\n",
    "for l in all_input_preprocessed['input'].tolist():\n",
    "    for el in l:\n",
    "        if el not in word_to_ix:\n",
    "            word_to_ix[el] = cur_idx\n",
    "            cur_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5160b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_word = dict([(v, k) for k, v in word_to_ix.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6f89a",
   "metadata": {},
   "source": [
    "The following function creates a numeric vector out of every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eadb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_vec(x, word_to_ix):\n",
    "    \"\"\"\n",
    "    Takes a tokenized tweet as input and returns a numeric vector.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for el in x:\n",
    "        res.append(word_to_ix[el])\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b69e2e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,   15, 1482,   73, 4488,  170, 2262, 9364, 9365,   68,\n",
       "        9366, 9367, 1262,   84, 9080,  147,   15, 2179,  778,  182,  229, 1438,\n",
       "        9368,  147,  590,    0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_num_vec(test_text_preprocessed['input'].tolist()[2], word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    The data is passed as lists\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, word_to_ix):\n",
    "        self.x = x['input'].tolist()\n",
    "        self.y = y['sentiment'].tolist()\n",
    "        self.word_to_ix = word_to_ix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = create_num_vec(self.x[idx], self.word_to_ix)\n",
    "        y = torch.tensor(self.y[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0bc7a",
   "metadata": {},
   "source": [
    "Let's see what shapes our data has, for example, so we can better design our model pipeline.\n",
    "As expected, the input data is of shape (batch_size, max_len_tweet), and the labels are of shape (batch_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c172c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=5, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(64, 16, kernel_size=3, padding=\"same\")\n",
    "\n",
    "        self.linear = nn.Linear(16, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # pool the 16 dimension to 1\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7c3d9",
   "metadata": {},
   "source": [
    "Before coming to the actual tasks, we will write some more functions which may be helpful. One of these functions is a function that calculates the loss and the accuracy (i.e. on the validation set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a77511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_acc(model, loss_fn, data_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in data_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Acccuracy  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "        print(\"F1 Score: {:.3f}\".format(f1_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b416e",
   "metadata": {},
   "source": [
    "# Task 1 -- Simple training on both datasets\n",
    "### Let's train and train the data! As a first step on the anger - joy dataset\n",
    "As a first step we need to create the datasets. Also, a small step we undertake is to change the 'sadness' sentiment, which is encoded as 3 to 0, as the cross entropy loss only accepts 0 and 1 as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25126dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_V1 = test_labels[anger_joy_test_idx]\n",
    "test_text_V1 = test_text_preprocessed[anger_joy_test_idx]\n",
    "\n",
    "val_labels_V1 = val_labels[anger_joy_val_idx]\n",
    "val_text_V1 = val_text_preprocessed[anger_joy_val_idx]\n",
    "\n",
    "train_labels_V1 = train_labels[anger_joy_train_idx]\n",
    "train_text_V1 = train_text_preprocessed[anger_joy_train_idx]\n",
    "\n",
    "\n",
    "# the same for V2\n",
    "test_labels_V2 = test_labels[sadness_joy_test_idx].replace(3, 0)\n",
    "test_text_V2 = test_text_preprocessed[sadness_joy_test_idx]\n",
    "\n",
    "val_labels_V2 = val_labels[sadness_joy_val_idx].replace(3, 0)\n",
    "val_text_V2 = val_text_preprocessed[sadness_joy_val_idx]\n",
    "\n",
    "train_labels_V2 = train_labels[sadness_joy_train_idx].replace(3, 0)\n",
    "train_text_V2 = train_text_preprocessed[sadness_joy_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f44e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111df3df",
   "metadata": {},
   "source": [
    "Let's define our datasets. We call the datasets containing the labels anger and joy 'V1' and the other datasets 'V2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61df3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_V1 = Model_Dataset(train_text_V1, train_labels_V1, word_to_ix)\n",
    "dataloader_train_V1 = DataLoader(dataloader_train_V1, batch_size=32, shuffle=True)\n",
    "\n",
    "dataloader_train_V2 = Model_Dataset(train_text_V2, train_labels_V2, word_to_ix)\n",
    "dataloader_train_V2 = DataLoader(dataloader_train_V2, batch_size=32, shuffle=True)\n",
    "\n",
    "dataloader_val_V1 = Model_Dataset(val_text_V1, val_labels_V1, word_to_ix)\n",
    "dataloader_val_V1 = DataLoader(dataloader_val_V1)\n",
    "\n",
    "dataloader_val_V2 = Model_Dataset(val_text_V2, val_labels_V2, word_to_ix)\n",
    "dataloader_val_V2 = DataLoader(dataloader_val_V2)\n",
    "\n",
    "dataloader_test_V1 = Model_Dataset(test_text_V1, test_labels_V1, word_to_ix)\n",
    "dataloader_test_V1 = DataLoader(dataloader_test_V1)\n",
    "\n",
    "dataloader_test_V2 = Model_Dataset(test_text_V2, test_labels_V2, word_to_ix)\n",
    "dataloader_test_V2 = DataLoader(dataloader_test_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2c124",
   "metadata": {},
   "source": [
    "### Training the model on the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31b4e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ed06e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_v1 = my_model(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_v1.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa18273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], batch: [0/66, loss: 0.6892]\n",
      "Epoch [0/10], batch: [1/66, loss: 0.6826]\n",
      "Epoch [0/10], batch: [2/66, loss: 0.7462]\n",
      "Epoch [0/10], batch: [3/66, loss: 0.6098]\n",
      "Epoch [0/10], batch: [4/66, loss: 0.6487]\n",
      "Epoch [0/10], batch: [5/66, loss: 0.6572]\n",
      "Epoch [0/10], batch: [6/66, loss: 0.7036]\n",
      "Epoch [0/10], batch: [7/66, loss: 0.6608]\n",
      "Epoch [0/10], batch: [8/66, loss: 0.6110]\n",
      "Epoch [0/10], batch: [9/66, loss: 0.6323]\n",
      "Epoch [0/10], batch: [10/66, loss: 0.6268]\n",
      "Epoch [0/10], batch: [11/66, loss: 0.6831]\n",
      "Epoch [0/10], batch: [12/66, loss: 0.7512]\n",
      "Epoch [0/10], batch: [13/66, loss: 0.6423]\n",
      "Epoch [0/10], batch: [14/66, loss: 0.6350]\n",
      "Epoch [0/10], batch: [15/66, loss: 0.6515]\n",
      "Epoch [0/10], batch: [16/66, loss: 0.6742]\n",
      "Epoch [0/10], batch: [17/66, loss: 0.5609]\n",
      "Epoch [0/10], batch: [18/66, loss: 0.4598]\n",
      "Epoch [0/10], batch: [19/66, loss: 0.6759]\n",
      "Epoch [0/10], batch: [20/66, loss: 0.7048]\n",
      "Epoch [0/10], batch: [21/66, loss: 0.7785]\n",
      "Epoch [0/10], batch: [22/66, loss: 0.7087]\n",
      "Epoch [0/10], batch: [23/66, loss: 0.5890]\n",
      "Epoch [0/10], batch: [24/66, loss: 0.6670]\n",
      "Epoch [0/10], batch: [25/66, loss: 0.6743]\n",
      "Epoch [0/10], batch: [26/66, loss: 0.5587]\n",
      "Epoch [0/10], batch: [27/66, loss: 0.6096]\n",
      "Epoch [0/10], batch: [28/66, loss: 0.6061]\n",
      "Epoch [0/10], batch: [29/66, loss: 0.6443]\n",
      "Epoch [0/10], batch: [30/66, loss: 0.6640]\n",
      "Epoch [0/10], batch: [31/66, loss: 0.6331]\n",
      "Epoch [0/10], batch: [32/66, loss: 0.6469]\n",
      "Epoch [0/10], batch: [33/66, loss: 0.6274]\n",
      "Epoch [0/10], batch: [34/66, loss: 0.6600]\n",
      "Epoch [0/10], batch: [35/66, loss: 0.6405]\n",
      "Epoch [0/10], batch: [36/66, loss: 0.5973]\n",
      "Epoch [0/10], batch: [37/66, loss: 0.6290]\n",
      "Epoch [0/10], batch: [38/66, loss: 0.5527]\n",
      "Epoch [0/10], batch: [39/66, loss: 0.6698]\n",
      "Epoch [0/10], batch: [40/66, loss: 0.6586]\n",
      "Epoch [0/10], batch: [41/66, loss: 0.5387]\n",
      "Epoch [0/10], batch: [42/66, loss: 0.6565]\n",
      "Epoch [0/10], batch: [43/66, loss: 0.5468]\n",
      "Epoch [0/10], batch: [44/66, loss: 0.6052]\n",
      "Epoch [0/10], batch: [45/66, loss: 0.7014]\n",
      "Epoch [0/10], batch: [46/66, loss: 0.5718]\n",
      "Epoch [0/10], batch: [47/66, loss: 0.5238]\n",
      "Epoch [0/10], batch: [48/66, loss: 0.7157]\n",
      "Epoch [0/10], batch: [49/66, loss: 0.5759]\n",
      "Epoch [0/10], batch: [50/66, loss: 0.5897]\n",
      "Epoch [0/10], batch: [51/66, loss: 0.6099]\n",
      "Epoch [0/10], batch: [52/66, loss: 0.6460]\n",
      "Epoch [0/10], batch: [53/66, loss: 0.6074]\n",
      "Epoch [0/10], batch: [54/66, loss: 0.6924]\n",
      "Epoch [0/10], batch: [55/66, loss: 0.6045]\n",
      "Epoch [0/10], batch: [56/66, loss: 0.6443]\n",
      "Epoch [0/10], batch: [57/66, loss: 0.6208]\n",
      "Epoch [0/10], batch: [58/66, loss: 0.6729]\n",
      "Epoch [0/10], batch: [59/66, loss: 0.6328]\n",
      "Epoch [0/10], batch: [60/66, loss: 0.6263]\n",
      "Epoch [0/10], batch: [61/66, loss: 0.6662]\n",
      "Epoch [0/10], batch: [62/66, loss: 0.5634]\n",
      "Epoch [0/10], batch: [63/66, loss: 0.5802]\n",
      "Epoch [0/10], batch: [64/66, loss: 0.6151]\n",
      "Epoch [0/10], batch: [65/66, loss: 0.7719]\n",
      "Epoch [1/10], batch: [0/66, loss: 0.6284]\n",
      "Epoch [1/10], batch: [1/66, loss: 0.6095]\n",
      "Epoch [1/10], batch: [2/66, loss: 0.6689]\n",
      "Epoch [1/10], batch: [3/66, loss: 0.6194]\n",
      "Epoch [1/10], batch: [4/66, loss: 0.5981]\n",
      "Epoch [1/10], batch: [5/66, loss: 0.6614]\n",
      "Epoch [1/10], batch: [6/66, loss: 0.5551]\n",
      "Epoch [1/10], batch: [7/66, loss: 0.6964]\n",
      "Epoch [1/10], batch: [8/66, loss: 0.6478]\n",
      "Epoch [1/10], batch: [9/66, loss: 0.6046]\n",
      "Epoch [1/10], batch: [10/66, loss: 0.5901]\n",
      "Epoch [1/10], batch: [11/66, loss: 0.5442]\n",
      "Epoch [1/10], batch: [12/66, loss: 0.5049]\n",
      "Epoch [1/10], batch: [13/66, loss: 0.5715]\n",
      "Epoch [1/10], batch: [14/66, loss: 0.4730]\n",
      "Epoch [1/10], batch: [15/66, loss: 0.8904]\n",
      "Epoch [1/10], batch: [16/66, loss: 0.7339]\n",
      "Epoch [1/10], batch: [17/66, loss: 0.5882]\n",
      "Epoch [1/10], batch: [18/66, loss: 0.6321]\n",
      "Epoch [1/10], batch: [19/66, loss: 0.6023]\n",
      "Epoch [1/10], batch: [20/66, loss: 0.6014]\n",
      "Epoch [1/10], batch: [21/66, loss: 0.5785]\n",
      "Epoch [1/10], batch: [22/66, loss: 0.6543]\n",
      "Epoch [1/10], batch: [23/66, loss: 0.6115]\n",
      "Epoch [1/10], batch: [24/66, loss: 0.6088]\n",
      "Epoch [1/10], batch: [25/66, loss: 0.6724]\n",
      "Epoch [1/10], batch: [26/66, loss: 0.6184]\n",
      "Epoch [1/10], batch: [27/66, loss: 0.6071]\n",
      "Epoch [1/10], batch: [28/66, loss: 0.5511]\n",
      "Epoch [1/10], batch: [29/66, loss: 0.6407]\n",
      "Epoch [1/10], batch: [30/66, loss: 0.5133]\n",
      "Epoch [1/10], batch: [31/66, loss: 0.4991]\n",
      "Epoch [1/10], batch: [32/66, loss: 0.6441]\n",
      "Epoch [1/10], batch: [33/66, loss: 0.7177]\n",
      "Epoch [1/10], batch: [34/66, loss: 0.5391]\n",
      "Epoch [1/10], batch: [35/66, loss: 0.4721]\n",
      "Epoch [1/10], batch: [36/66, loss: 0.5359]\n",
      "Epoch [1/10], batch: [37/66, loss: 0.7049]\n",
      "Epoch [1/10], batch: [38/66, loss: 0.5229]\n",
      "Epoch [1/10], batch: [39/66, loss: 0.5817]\n",
      "Epoch [1/10], batch: [40/66, loss: 0.7050]\n",
      "Epoch [1/10], batch: [41/66, loss: 0.6052]\n",
      "Epoch [1/10], batch: [42/66, loss: 0.6077]\n",
      "Epoch [1/10], batch: [43/66, loss: 0.5221]\n",
      "Epoch [1/10], batch: [44/66, loss: 0.6002]\n",
      "Epoch [1/10], batch: [45/66, loss: 0.4939]\n",
      "Epoch [1/10], batch: [46/66, loss: 0.6313]\n",
      "Epoch [1/10], batch: [47/66, loss: 0.6133]\n",
      "Epoch [1/10], batch: [48/66, loss: 0.6871]\n",
      "Epoch [1/10], batch: [49/66, loss: 0.6184]\n",
      "Epoch [1/10], batch: [50/66, loss: 0.6079]\n",
      "Epoch [1/10], batch: [51/66, loss: 0.5651]\n",
      "Epoch [1/10], batch: [52/66, loss: 0.5696]\n",
      "Epoch [1/10], batch: [53/66, loss: 0.6248]\n",
      "Epoch [1/10], batch: [54/66, loss: 0.5828]\n",
      "Epoch [1/10], batch: [55/66, loss: 0.5837]\n",
      "Epoch [1/10], batch: [56/66, loss: 0.6329]\n",
      "Epoch [1/10], batch: [57/66, loss: 0.6614]\n",
      "Epoch [1/10], batch: [58/66, loss: 0.6272]\n",
      "Epoch [1/10], batch: [59/66, loss: 0.5650]\n",
      "Epoch [1/10], batch: [60/66, loss: 0.5348]\n",
      "Epoch [1/10], batch: [61/66, loss: 0.5538]\n",
      "Epoch [1/10], batch: [62/66, loss: 0.4428]\n",
      "Epoch [1/10], batch: [63/66, loss: 0.5299]\n",
      "Epoch [1/10], batch: [64/66, loss: 0.6542]\n",
      "Epoch [1/10], batch: [65/66, loss: 0.5333]\n",
      "Epoch [2/10], batch: [0/66, loss: 0.4594]\n",
      "Epoch [2/10], batch: [1/66, loss: 0.4705]\n",
      "Epoch [2/10], batch: [2/66, loss: 0.5315]\n",
      "Epoch [2/10], batch: [3/66, loss: 0.5259]\n",
      "Epoch [2/10], batch: [4/66, loss: 0.6387]\n",
      "Epoch [2/10], batch: [5/66, loss: 0.6185]\n",
      "Epoch [2/10], batch: [6/66, loss: 0.5859]\n",
      "Epoch [2/10], batch: [7/66, loss: 0.4984]\n",
      "Epoch [2/10], batch: [8/66, loss: 0.5459]\n",
      "Epoch [2/10], batch: [9/66, loss: 0.4544]\n",
      "Epoch [2/10], batch: [10/66, loss: 0.6147]\n",
      "Epoch [2/10], batch: [11/66, loss: 0.6075]\n",
      "Epoch [2/10], batch: [12/66, loss: 0.4663]\n",
      "Epoch [2/10], batch: [13/66, loss: 0.5564]\n",
      "Epoch [2/10], batch: [14/66, loss: 0.4897]\n",
      "Epoch [2/10], batch: [15/66, loss: 0.5432]\n",
      "Epoch [2/10], batch: [16/66, loss: 0.4542]\n",
      "Epoch [2/10], batch: [17/66, loss: 0.4148]\n",
      "Epoch [2/10], batch: [18/66, loss: 0.6087]\n",
      "Epoch [2/10], batch: [19/66, loss: 0.3895]\n",
      "Epoch [2/10], batch: [20/66, loss: 0.5912]\n",
      "Epoch [2/10], batch: [21/66, loss: 0.4204]\n",
      "Epoch [2/10], batch: [22/66, loss: 0.5076]\n",
      "Epoch [2/10], batch: [23/66, loss: 0.4388]\n",
      "Epoch [2/10], batch: [24/66, loss: 0.4342]\n",
      "Epoch [2/10], batch: [25/66, loss: 0.4864]\n",
      "Epoch [2/10], batch: [26/66, loss: 0.4168]\n",
      "Epoch [2/10], batch: [27/66, loss: 0.3639]\n",
      "Epoch [2/10], batch: [28/66, loss: 0.4022]\n",
      "Epoch [2/10], batch: [29/66, loss: 0.4966]\n",
      "Epoch [2/10], batch: [30/66, loss: 0.5351]\n",
      "Epoch [2/10], batch: [31/66, loss: 0.5316]\n",
      "Epoch [2/10], batch: [32/66, loss: 0.5700]\n",
      "Epoch [2/10], batch: [33/66, loss: 0.4957]\n",
      "Epoch [2/10], batch: [34/66, loss: 0.2984]\n",
      "Epoch [2/10], batch: [35/66, loss: 0.5643]\n",
      "Epoch [2/10], batch: [36/66, loss: 0.4059]\n",
      "Epoch [2/10], batch: [37/66, loss: 0.4244]\n",
      "Epoch [2/10], batch: [38/66, loss: 0.5145]\n",
      "Epoch [2/10], batch: [39/66, loss: 0.3583]\n",
      "Epoch [2/10], batch: [40/66, loss: 0.4046]\n",
      "Epoch [2/10], batch: [41/66, loss: 0.4421]\n",
      "Epoch [2/10], batch: [42/66, loss: 0.5188]\n",
      "Epoch [2/10], batch: [43/66, loss: 0.4138]\n",
      "Epoch [2/10], batch: [44/66, loss: 0.4012]\n",
      "Epoch [2/10], batch: [45/66, loss: 0.4581]\n",
      "Epoch [2/10], batch: [46/66, loss: 0.4073]\n",
      "Epoch [2/10], batch: [47/66, loss: 0.4187]\n",
      "Epoch [2/10], batch: [48/66, loss: 0.4595]\n",
      "Epoch [2/10], batch: [49/66, loss: 0.4656]\n",
      "Epoch [2/10], batch: [50/66, loss: 0.4546]\n",
      "Epoch [2/10], batch: [51/66, loss: 0.3841]\n",
      "Epoch [2/10], batch: [52/66, loss: 0.2613]\n",
      "Epoch [2/10], batch: [53/66, loss: 0.4027]\n",
      "Epoch [2/10], batch: [54/66, loss: 0.3230]\n",
      "Epoch [2/10], batch: [55/66, loss: 0.3278]\n",
      "Epoch [2/10], batch: [56/66, loss: 0.5276]\n",
      "Epoch [2/10], batch: [57/66, loss: 0.4115]\n",
      "Epoch [2/10], batch: [58/66, loss: 0.2714]\n",
      "Epoch [2/10], batch: [59/66, loss: 0.4753]\n",
      "Epoch [2/10], batch: [60/66, loss: 0.5153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], batch: [61/66, loss: 0.4181]\n",
      "Epoch [2/10], batch: [62/66, loss: 0.2660]\n",
      "Epoch [2/10], batch: [63/66, loss: 0.4394]\n",
      "Epoch [2/10], batch: [64/66, loss: 0.3892]\n",
      "Epoch [2/10], batch: [65/66, loss: 0.3522]\n",
      "Epoch [3/10], batch: [0/66, loss: 0.2013]\n",
      "Epoch [3/10], batch: [1/66, loss: 0.2680]\n",
      "Epoch [3/10], batch: [2/66, loss: 0.1953]\n",
      "Epoch [3/10], batch: [3/66, loss: 0.2866]\n",
      "Epoch [3/10], batch: [4/66, loss: 0.2342]\n",
      "Epoch [3/10], batch: [5/66, loss: 0.2198]\n",
      "Epoch [3/10], batch: [6/66, loss: 0.1902]\n",
      "Epoch [3/10], batch: [7/66, loss: 0.2189]\n",
      "Epoch [3/10], batch: [8/66, loss: 0.2260]\n",
      "Epoch [3/10], batch: [9/66, loss: 0.2288]\n",
      "Epoch [3/10], batch: [10/66, loss: 0.2139]\n",
      "Epoch [3/10], batch: [11/66, loss: 0.2109]\n",
      "Epoch [3/10], batch: [12/66, loss: 0.1826]\n",
      "Epoch [3/10], batch: [13/66, loss: 0.3720]\n",
      "Epoch [3/10], batch: [14/66, loss: 0.1964]\n",
      "Epoch [3/10], batch: [15/66, loss: 0.1971]\n",
      "Epoch [3/10], batch: [16/66, loss: 0.0882]\n",
      "Epoch [3/10], batch: [17/66, loss: 0.4183]\n",
      "Epoch [3/10], batch: [18/66, loss: 0.1409]\n",
      "Epoch [3/10], batch: [19/66, loss: 0.2872]\n",
      "Epoch [3/10], batch: [20/66, loss: 0.3201]\n",
      "Epoch [3/10], batch: [21/66, loss: 0.1026]\n",
      "Epoch [3/10], batch: [22/66, loss: 0.2303]\n",
      "Epoch [3/10], batch: [23/66, loss: 0.2299]\n",
      "Epoch [3/10], batch: [24/66, loss: 0.2133]\n",
      "Epoch [3/10], batch: [25/66, loss: 0.2316]\n",
      "Epoch [3/10], batch: [26/66, loss: 0.1506]\n",
      "Epoch [3/10], batch: [27/66, loss: 0.3599]\n",
      "Epoch [3/10], batch: [28/66, loss: 0.1030]\n",
      "Epoch [3/10], batch: [29/66, loss: 0.2590]\n",
      "Epoch [3/10], batch: [30/66, loss: 0.2198]\n",
      "Epoch [3/10], batch: [31/66, loss: 0.2878]\n",
      "Epoch [3/10], batch: [32/66, loss: 0.1043]\n",
      "Epoch [3/10], batch: [33/66, loss: 0.0879]\n",
      "Epoch [3/10], batch: [34/66, loss: 0.2926]\n",
      "Epoch [3/10], batch: [35/66, loss: 0.2686]\n",
      "Epoch [3/10], batch: [36/66, loss: 0.2228]\n",
      "Epoch [3/10], batch: [37/66, loss: 0.1698]\n",
      "Epoch [3/10], batch: [38/66, loss: 0.0959]\n",
      "Epoch [3/10], batch: [39/66, loss: 0.2069]\n",
      "Epoch [3/10], batch: [40/66, loss: 0.1062]\n",
      "Epoch [3/10], batch: [41/66, loss: 0.1142]\n",
      "Epoch [3/10], batch: [42/66, loss: 0.2932]\n",
      "Epoch [3/10], batch: [43/66, loss: 0.1367]\n",
      "Epoch [3/10], batch: [44/66, loss: 0.1263]\n",
      "Epoch [3/10], batch: [45/66, loss: 0.1912]\n",
      "Epoch [3/10], batch: [46/66, loss: 0.1648]\n",
      "Epoch [3/10], batch: [47/66, loss: 0.1039]\n",
      "Epoch [3/10], batch: [48/66, loss: 0.4373]\n",
      "Epoch [3/10], batch: [49/66, loss: 0.2151]\n",
      "Epoch [3/10], batch: [50/66, loss: 0.1028]\n",
      "Epoch [3/10], batch: [51/66, loss: 0.3158]\n",
      "Epoch [3/10], batch: [52/66, loss: 0.1770]\n",
      "Epoch [3/10], batch: [53/66, loss: 0.3658]\n",
      "Epoch [3/10], batch: [54/66, loss: 0.2529]\n",
      "Epoch [3/10], batch: [55/66, loss: 0.1943]\n",
      "Epoch [3/10], batch: [56/66, loss: 0.2882]\n",
      "Epoch [3/10], batch: [57/66, loss: 0.1971]\n",
      "Epoch [3/10], batch: [58/66, loss: 0.2306]\n",
      "Epoch [3/10], batch: [59/66, loss: 0.2574]\n",
      "Epoch [3/10], batch: [60/66, loss: 0.4690]\n",
      "Epoch [3/10], batch: [61/66, loss: 0.1398]\n",
      "Epoch [3/10], batch: [62/66, loss: 0.3597]\n",
      "Epoch [3/10], batch: [63/66, loss: 0.2861]\n",
      "Epoch [3/10], batch: [64/66, loss: 0.4010]\n",
      "Epoch [3/10], batch: [65/66, loss: 0.3051]\n",
      "Epoch [4/10], batch: [0/66, loss: 0.0739]\n",
      "Epoch [4/10], batch: [1/66, loss: 0.2355]\n",
      "Epoch [4/10], batch: [2/66, loss: 0.0888]\n",
      "Epoch [4/10], batch: [3/66, loss: 0.1620]\n",
      "Epoch [4/10], batch: [4/66, loss: 0.0534]\n",
      "Epoch [4/10], batch: [5/66, loss: 0.3083]\n",
      "Epoch [4/10], batch: [6/66, loss: 0.1841]\n",
      "Epoch [4/10], batch: [7/66, loss: 0.1424]\n",
      "Epoch [4/10], batch: [8/66, loss: 0.1388]\n",
      "Epoch [4/10], batch: [9/66, loss: 0.2755]\n",
      "Epoch [4/10], batch: [10/66, loss: 0.1178]\n",
      "Epoch [4/10], batch: [11/66, loss: 0.0757]\n",
      "Epoch [4/10], batch: [12/66, loss: 0.1231]\n",
      "Epoch [4/10], batch: [13/66, loss: 0.2679]\n",
      "Epoch [4/10], batch: [14/66, loss: 0.1425]\n",
      "Epoch [4/10], batch: [15/66, loss: 0.0832]\n",
      "Epoch [4/10], batch: [16/66, loss: 0.1263]\n",
      "Epoch [4/10], batch: [17/66, loss: 0.1574]\n",
      "Epoch [4/10], batch: [18/66, loss: 0.2065]\n",
      "Epoch [4/10], batch: [19/66, loss: 0.0756]\n",
      "Epoch [4/10], batch: [20/66, loss: 0.1014]\n",
      "Epoch [4/10], batch: [21/66, loss: 0.0608]\n",
      "Epoch [4/10], batch: [22/66, loss: 0.0921]\n",
      "Epoch [4/10], batch: [23/66, loss: 0.1127]\n",
      "Epoch [4/10], batch: [24/66, loss: 0.1503]\n",
      "Epoch [4/10], batch: [25/66, loss: 0.0718]\n",
      "Epoch [4/10], batch: [26/66, loss: 0.2873]\n",
      "Epoch [4/10], batch: [27/66, loss: 0.0623]\n",
      "Epoch [4/10], batch: [28/66, loss: 0.0773]\n",
      "Epoch [4/10], batch: [29/66, loss: 0.1234]\n",
      "Epoch [4/10], batch: [30/66, loss: 0.0977]\n",
      "Epoch [4/10], batch: [31/66, loss: 0.0712]\n",
      "Epoch [4/10], batch: [32/66, loss: 0.0952]\n",
      "Epoch [4/10], batch: [33/66, loss: 0.1072]\n",
      "Epoch [4/10], batch: [34/66, loss: 0.1106]\n",
      "Epoch [4/10], batch: [35/66, loss: 0.0922]\n",
      "Epoch [4/10], batch: [36/66, loss: 0.1468]\n",
      "Epoch [4/10], batch: [37/66, loss: 0.1182]\n",
      "Epoch [4/10], batch: [38/66, loss: 0.0805]\n",
      "Epoch [4/10], batch: [39/66, loss: 0.0842]\n",
      "Epoch [4/10], batch: [40/66, loss: 0.0949]\n",
      "Epoch [4/10], batch: [41/66, loss: 0.3206]\n",
      "Epoch [4/10], batch: [42/66, loss: 0.1108]\n",
      "Epoch [4/10], batch: [43/66, loss: 0.0984]\n",
      "Epoch [4/10], batch: [44/66, loss: 0.1355]\n",
      "Epoch [4/10], batch: [45/66, loss: 0.3059]\n",
      "Epoch [4/10], batch: [46/66, loss: 0.1042]\n",
      "Epoch [4/10], batch: [47/66, loss: 0.0646]\n",
      "Epoch [4/10], batch: [48/66, loss: 0.0737]\n",
      "Epoch [4/10], batch: [49/66, loss: 0.0215]\n",
      "Epoch [4/10], batch: [50/66, loss: 0.0965]\n",
      "Epoch [4/10], batch: [51/66, loss: 0.0410]\n",
      "Epoch [4/10], batch: [52/66, loss: 0.1145]\n",
      "Epoch [4/10], batch: [53/66, loss: 0.1320]\n",
      "Epoch [4/10], batch: [54/66, loss: 0.0659]\n",
      "Epoch [4/10], batch: [55/66, loss: 0.0911]\n",
      "Epoch [4/10], batch: [56/66, loss: 0.2870]\n",
      "Epoch [4/10], batch: [57/66, loss: 0.0628]\n",
      "Epoch [4/10], batch: [58/66, loss: 0.0921]\n",
      "Epoch [4/10], batch: [59/66, loss: 0.0714]\n",
      "Epoch [4/10], batch: [60/66, loss: 0.1556]\n",
      "Epoch [4/10], batch: [61/66, loss: 0.0295]\n",
      "Epoch [4/10], batch: [62/66, loss: 0.1012]\n",
      "Epoch [4/10], batch: [63/66, loss: 0.0588]\n",
      "Epoch [4/10], batch: [64/66, loss: 0.0640]\n",
      "Epoch [4/10], batch: [65/66, loss: 0.2333]\n",
      "Epoch [5/10], batch: [0/66, loss: 0.0165]\n",
      "Epoch [5/10], batch: [1/66, loss: 0.0848]\n",
      "Epoch [5/10], batch: [2/66, loss: 0.2136]\n",
      "Epoch [5/10], batch: [3/66, loss: 0.0596]\n",
      "Epoch [5/10], batch: [4/66, loss: 0.0591]\n",
      "Epoch [5/10], batch: [5/66, loss: 0.0529]\n",
      "Epoch [5/10], batch: [6/66, loss: 0.0212]\n",
      "Epoch [5/10], batch: [7/66, loss: 0.0176]\n",
      "Epoch [5/10], batch: [8/66, loss: 0.0542]\n",
      "Epoch [5/10], batch: [9/66, loss: 0.0676]\n",
      "Epoch [5/10], batch: [10/66, loss: 0.1945]\n",
      "Epoch [5/10], batch: [11/66, loss: 0.0241]\n",
      "Epoch [5/10], batch: [12/66, loss: 0.0172]\n",
      "Epoch [5/10], batch: [13/66, loss: 0.0566]\n",
      "Epoch [5/10], batch: [14/66, loss: 0.0201]\n",
      "Epoch [5/10], batch: [15/66, loss: 0.0741]\n",
      "Epoch [5/10], batch: [16/66, loss: 0.0431]\n",
      "Epoch [5/10], batch: [17/66, loss: 0.0786]\n",
      "Epoch [5/10], batch: [18/66, loss: 0.0954]\n",
      "Epoch [5/10], batch: [19/66, loss: 0.0332]\n",
      "Epoch [5/10], batch: [20/66, loss: 0.1459]\n",
      "Epoch [5/10], batch: [21/66, loss: 0.0184]\n",
      "Epoch [5/10], batch: [22/66, loss: 0.0766]\n",
      "Epoch [5/10], batch: [23/66, loss: 0.0431]\n",
      "Epoch [5/10], batch: [24/66, loss: 0.0604]\n",
      "Epoch [5/10], batch: [25/66, loss: 0.0091]\n",
      "Epoch [5/10], batch: [26/66, loss: 0.1236]\n",
      "Epoch [5/10], batch: [27/66, loss: 0.1382]\n",
      "Epoch [5/10], batch: [28/66, loss: 0.0335]\n",
      "Epoch [5/10], batch: [29/66, loss: 0.4914]\n",
      "Epoch [5/10], batch: [30/66, loss: 0.0098]\n",
      "Epoch [5/10], batch: [31/66, loss: 0.0095]\n",
      "Epoch [5/10], batch: [32/66, loss: 0.0516]\n",
      "Epoch [5/10], batch: [33/66, loss: 0.0284]\n",
      "Epoch [5/10], batch: [34/66, loss: 0.0293]\n",
      "Epoch [5/10], batch: [35/66, loss: 0.4679]\n",
      "Epoch [5/10], batch: [36/66, loss: 0.0397]\n",
      "Epoch [5/10], batch: [37/66, loss: 0.1517]\n",
      "Epoch [5/10], batch: [38/66, loss: 0.2635]\n",
      "Epoch [5/10], batch: [39/66, loss: 0.1885]\n",
      "Epoch [5/10], batch: [40/66, loss: 0.0329]\n",
      "Epoch [5/10], batch: [41/66, loss: 0.1254]\n",
      "Epoch [5/10], batch: [42/66, loss: 0.0639]\n",
      "Epoch [5/10], batch: [43/66, loss: 0.0313]\n",
      "Epoch [5/10], batch: [44/66, loss: 0.0604]\n",
      "Epoch [5/10], batch: [45/66, loss: 0.0160]\n",
      "Epoch [5/10], batch: [46/66, loss: 0.0161]\n",
      "Epoch [5/10], batch: [47/66, loss: 0.0783]\n",
      "Epoch [5/10], batch: [48/66, loss: 0.0169]\n",
      "Epoch [5/10], batch: [49/66, loss: 0.0316]\n",
      "Epoch [5/10], batch: [50/66, loss: 0.0192]\n",
      "Epoch [5/10], batch: [51/66, loss: 0.0162]\n",
      "Epoch [5/10], batch: [52/66, loss: 0.0589]\n",
      "Epoch [5/10], batch: [53/66, loss: 0.0205]\n",
      "Epoch [5/10], batch: [54/66, loss: 0.1116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], batch: [55/66, loss: 0.0132]\n",
      "Epoch [5/10], batch: [56/66, loss: 0.0942]\n",
      "Epoch [5/10], batch: [57/66, loss: 0.0253]\n",
      "Epoch [5/10], batch: [58/66, loss: 0.0777]\n",
      "Epoch [5/10], batch: [59/66, loss: 0.0472]\n",
      "Epoch [5/10], batch: [60/66, loss: 0.2643]\n",
      "Epoch [5/10], batch: [61/66, loss: 0.0196]\n",
      "Epoch [5/10], batch: [62/66, loss: 0.0506]\n",
      "Epoch [5/10], batch: [63/66, loss: 0.2215]\n",
      "Epoch [5/10], batch: [64/66, loss: 0.0460]\n",
      "Epoch [5/10], batch: [65/66, loss: 0.0293]\n",
      "Epoch [6/10], batch: [0/66, loss: 0.0253]\n",
      "Epoch [6/10], batch: [1/66, loss: 0.1855]\n",
      "Epoch [6/10], batch: [2/66, loss: 0.1013]\n",
      "Epoch [6/10], batch: [3/66, loss: 0.0146]\n",
      "Epoch [6/10], batch: [4/66, loss: 0.0651]\n",
      "Epoch [6/10], batch: [5/66, loss: 0.1044]\n",
      "Epoch [6/10], batch: [6/66, loss: 0.0179]\n",
      "Epoch [6/10], batch: [7/66, loss: 0.1927]\n",
      "Epoch [6/10], batch: [8/66, loss: 0.0467]\n",
      "Epoch [6/10], batch: [9/66, loss: 0.0870]\n",
      "Epoch [6/10], batch: [10/66, loss: 0.0469]\n",
      "Epoch [6/10], batch: [11/66, loss: 0.0092]\n",
      "Epoch [6/10], batch: [12/66, loss: 0.1860]\n",
      "Epoch [6/10], batch: [13/66, loss: 0.1060]\n",
      "Epoch [6/10], batch: [14/66, loss: 0.0321]\n",
      "Epoch [6/10], batch: [15/66, loss: 0.0140]\n",
      "Epoch [6/10], batch: [16/66, loss: 0.2443]\n",
      "Epoch [6/10], batch: [17/66, loss: 0.1090]\n",
      "Epoch [6/10], batch: [18/66, loss: 0.0279]\n",
      "Epoch [6/10], batch: [19/66, loss: 0.0156]\n",
      "Epoch [6/10], batch: [20/66, loss: 0.0688]\n",
      "Epoch [6/10], batch: [21/66, loss: 0.0408]\n",
      "Epoch [6/10], batch: [22/66, loss: 0.0267]\n",
      "Epoch [6/10], batch: [23/66, loss: 0.0121]\n",
      "Epoch [6/10], batch: [24/66, loss: 0.0083]\n",
      "Epoch [6/10], batch: [25/66, loss: 0.0247]\n",
      "Epoch [6/10], batch: [26/66, loss: 0.0442]\n",
      "Epoch [6/10], batch: [27/66, loss: 0.0080]\n",
      "Epoch [6/10], batch: [28/66, loss: 0.0055]\n",
      "Epoch [6/10], batch: [29/66, loss: 0.0504]\n",
      "Epoch [6/10], batch: [30/66, loss: 0.1473]\n",
      "Epoch [6/10], batch: [31/66, loss: 0.0165]\n",
      "Epoch [6/10], batch: [32/66, loss: 0.0116]\n",
      "Epoch [6/10], batch: [33/66, loss: 0.0850]\n",
      "Epoch [6/10], batch: [34/66, loss: 0.2168]\n",
      "Epoch [6/10], batch: [35/66, loss: 0.0126]\n",
      "Epoch [6/10], batch: [36/66, loss: 0.0155]\n",
      "Epoch [6/10], batch: [37/66, loss: 0.0384]\n",
      "Epoch [6/10], batch: [38/66, loss: 0.0070]\n",
      "Epoch [6/10], batch: [39/66, loss: 0.0351]\n",
      "Epoch [6/10], batch: [40/66, loss: 0.0337]\n",
      "Epoch [6/10], batch: [41/66, loss: 0.0178]\n",
      "Epoch [6/10], batch: [42/66, loss: 0.0777]\n",
      "Epoch [6/10], batch: [43/66, loss: 0.0229]\n",
      "Epoch [6/10], batch: [44/66, loss: 0.0108]\n",
      "Epoch [6/10], batch: [45/66, loss: 0.0828]\n",
      "Epoch [6/10], batch: [46/66, loss: 0.0043]\n",
      "Epoch [6/10], batch: [47/66, loss: 0.0101]\n",
      "Epoch [6/10], batch: [48/66, loss: 0.0293]\n",
      "Epoch [6/10], batch: [49/66, loss: 0.0113]\n",
      "Epoch [6/10], batch: [50/66, loss: 0.0093]\n",
      "Epoch [6/10], batch: [51/66, loss: 0.0158]\n",
      "Epoch [6/10], batch: [52/66, loss: 0.1023]\n",
      "Epoch [6/10], batch: [53/66, loss: 0.0316]\n",
      "Epoch [6/10], batch: [54/66, loss: 0.0967]\n",
      "Epoch [6/10], batch: [55/66, loss: 0.1340]\n",
      "Epoch [6/10], batch: [56/66, loss: 0.0361]\n",
      "Epoch [6/10], batch: [57/66, loss: 0.0201]\n",
      "Epoch [6/10], batch: [58/66, loss: 0.0343]\n",
      "Epoch [6/10], batch: [59/66, loss: 0.0246]\n",
      "Epoch [6/10], batch: [60/66, loss: 0.1015]\n",
      "Epoch [6/10], batch: [61/66, loss: 0.0540]\n",
      "Epoch [6/10], batch: [62/66, loss: 0.0806]\n",
      "Epoch [6/10], batch: [63/66, loss: 0.0564]\n",
      "Epoch [6/10], batch: [64/66, loss: 0.0126]\n",
      "Epoch [6/10], batch: [65/66, loss: 0.0164]\n",
      "Epoch [7/10], batch: [0/66, loss: 0.0622]\n",
      "Epoch [7/10], batch: [1/66, loss: 0.0242]\n",
      "Epoch [7/10], batch: [2/66, loss: 0.2263]\n",
      "Epoch [7/10], batch: [3/66, loss: 0.0523]\n",
      "Epoch [7/10], batch: [4/66, loss: 0.0139]\n",
      "Epoch [7/10], batch: [5/66, loss: 0.1492]\n",
      "Epoch [7/10], batch: [6/66, loss: 0.0416]\n",
      "Epoch [7/10], batch: [7/66, loss: 0.0212]\n",
      "Epoch [7/10], batch: [8/66, loss: 0.0147]\n",
      "Epoch [7/10], batch: [9/66, loss: 0.0254]\n",
      "Epoch [7/10], batch: [10/66, loss: 0.0042]\n",
      "Epoch [7/10], batch: [11/66, loss: 0.2044]\n",
      "Epoch [7/10], batch: [12/66, loss: 0.0357]\n",
      "Epoch [7/10], batch: [13/66, loss: 0.0064]\n",
      "Epoch [7/10], batch: [14/66, loss: 0.0137]\n",
      "Epoch [7/10], batch: [15/66, loss: 0.0074]\n",
      "Epoch [7/10], batch: [16/66, loss: 0.1648]\n",
      "Epoch [7/10], batch: [17/66, loss: 0.2411]\n",
      "Epoch [7/10], batch: [18/66, loss: 0.0216]\n",
      "Epoch [7/10], batch: [19/66, loss: 0.0063]\n",
      "Epoch [7/10], batch: [20/66, loss: 0.0071]\n",
      "Epoch [7/10], batch: [21/66, loss: 0.0075]\n",
      "Epoch [7/10], batch: [22/66, loss: 0.0900]\n",
      "Epoch [7/10], batch: [23/66, loss: 0.0226]\n",
      "Epoch [7/10], batch: [24/66, loss: 0.0099]\n",
      "Epoch [7/10], batch: [25/66, loss: 0.0122]\n",
      "Epoch [7/10], batch: [26/66, loss: 0.0549]\n",
      "Epoch [7/10], batch: [27/66, loss: 0.0232]\n",
      "Epoch [7/10], batch: [28/66, loss: 0.0740]\n",
      "Epoch [7/10], batch: [29/66, loss: 0.0221]\n",
      "Epoch [7/10], batch: [30/66, loss: 0.0275]\n",
      "Epoch [7/10], batch: [31/66, loss: 0.0254]\n",
      "Epoch [7/10], batch: [32/66, loss: 0.0251]\n",
      "Epoch [7/10], batch: [33/66, loss: 0.0151]\n",
      "Epoch [7/10], batch: [34/66, loss: 0.0149]\n",
      "Epoch [7/10], batch: [35/66, loss: 0.0122]\n",
      "Epoch [7/10], batch: [36/66, loss: 0.0213]\n",
      "Epoch [7/10], batch: [37/66, loss: 0.0040]\n",
      "Epoch [7/10], batch: [38/66, loss: 0.0254]\n",
      "Epoch [7/10], batch: [39/66, loss: 0.0017]\n",
      "Epoch [7/10], batch: [40/66, loss: 0.0146]\n",
      "Epoch [7/10], batch: [41/66, loss: 0.0092]\n",
      "Epoch [7/10], batch: [42/66, loss: 0.0200]\n",
      "Epoch [7/10], batch: [43/66, loss: 0.0239]\n",
      "Epoch [7/10], batch: [44/66, loss: 0.0057]\n",
      "Epoch [7/10], batch: [45/66, loss: 0.0759]\n",
      "Epoch [7/10], batch: [46/66, loss: 0.0081]\n",
      "Epoch [7/10], batch: [47/66, loss: 0.0219]\n",
      "Epoch [7/10], batch: [48/66, loss: 0.0112]\n",
      "Epoch [7/10], batch: [49/66, loss: 0.0175]\n",
      "Epoch [7/10], batch: [50/66, loss: 0.0245]\n",
      "Epoch [7/10], batch: [51/66, loss: 0.0818]\n",
      "Epoch [7/10], batch: [52/66, loss: 0.1309]\n",
      "Epoch [7/10], batch: [53/66, loss: 0.0105]\n",
      "Epoch [7/10], batch: [54/66, loss: 0.0683]\n",
      "Epoch [7/10], batch: [55/66, loss: 0.0447]\n",
      "Epoch [7/10], batch: [56/66, loss: 0.0026]\n",
      "Epoch [7/10], batch: [57/66, loss: 0.0235]\n",
      "Epoch [7/10], batch: [58/66, loss: 0.0090]\n",
      "Epoch [7/10], batch: [59/66, loss: 0.0057]\n",
      "Epoch [7/10], batch: [60/66, loss: 0.0012]\n",
      "Epoch [7/10], batch: [61/66, loss: 0.0051]\n",
      "Epoch [7/10], batch: [62/66, loss: 0.0030]\n",
      "Epoch [7/10], batch: [63/66, loss: 0.0046]\n",
      "Epoch [7/10], batch: [64/66, loss: 0.0030]\n",
      "Epoch [7/10], batch: [65/66, loss: 0.0061]\n",
      "Epoch [8/10], batch: [0/66, loss: 0.0044]\n",
      "Epoch [8/10], batch: [1/66, loss: 0.0082]\n",
      "Epoch [8/10], batch: [2/66, loss: 0.0267]\n",
      "Epoch [8/10], batch: [3/66, loss: 0.0034]\n",
      "Epoch [8/10], batch: [4/66, loss: 0.0421]\n",
      "Epoch [8/10], batch: [5/66, loss: 0.0292]\n",
      "Epoch [8/10], batch: [6/66, loss: 0.0092]\n",
      "Epoch [8/10], batch: [7/66, loss: 0.0022]\n",
      "Epoch [8/10], batch: [8/66, loss: 0.0080]\n",
      "Epoch [8/10], batch: [9/66, loss: 0.0013]\n",
      "Epoch [8/10], batch: [10/66, loss: 0.1497]\n",
      "Epoch [8/10], batch: [11/66, loss: 0.0089]\n",
      "Epoch [8/10], batch: [12/66, loss: 0.2146]\n",
      "Epoch [8/10], batch: [13/66, loss: 0.0044]\n",
      "Epoch [8/10], batch: [14/66, loss: 0.0073]\n",
      "Epoch [8/10], batch: [15/66, loss: 0.0068]\n",
      "Epoch [8/10], batch: [16/66, loss: 0.0072]\n",
      "Epoch [8/10], batch: [17/66, loss: 0.0050]\n",
      "Epoch [8/10], batch: [18/66, loss: 0.0039]\n",
      "Epoch [8/10], batch: [19/66, loss: 0.0049]\n",
      "Epoch [8/10], batch: [20/66, loss: 0.0065]\n",
      "Epoch [8/10], batch: [21/66, loss: 0.0163]\n",
      "Epoch [8/10], batch: [22/66, loss: 0.0139]\n",
      "Epoch [8/10], batch: [23/66, loss: 0.0009]\n",
      "Epoch [8/10], batch: [24/66, loss: 0.0022]\n",
      "Epoch [8/10], batch: [25/66, loss: 0.0008]\n",
      "Epoch [8/10], batch: [26/66, loss: 0.0637]\n",
      "Epoch [8/10], batch: [27/66, loss: 0.0124]\n",
      "Epoch [8/10], batch: [28/66, loss: 0.0022]\n",
      "Epoch [8/10], batch: [29/66, loss: 0.0020]\n",
      "Epoch [8/10], batch: [30/66, loss: 0.0041]\n",
      "Epoch [8/10], batch: [31/66, loss: 0.0037]\n",
      "Epoch [8/10], batch: [32/66, loss: 0.0108]\n",
      "Epoch [8/10], batch: [33/66, loss: 0.0015]\n",
      "Epoch [8/10], batch: [34/66, loss: 0.0071]\n",
      "Epoch [8/10], batch: [35/66, loss: 0.0553]\n",
      "Epoch [8/10], batch: [36/66, loss: 0.0017]\n",
      "Epoch [8/10], batch: [37/66, loss: 0.0630]\n",
      "Epoch [8/10], batch: [38/66, loss: 0.1127]\n",
      "Epoch [8/10], batch: [39/66, loss: 0.0058]\n",
      "Epoch [8/10], batch: [40/66, loss: 0.0332]\n",
      "Epoch [8/10], batch: [41/66, loss: 0.0091]\n",
      "Epoch [8/10], batch: [42/66, loss: 0.0180]\n",
      "Epoch [8/10], batch: [43/66, loss: 0.0022]\n",
      "Epoch [8/10], batch: [44/66, loss: 0.0492]\n",
      "Epoch [8/10], batch: [45/66, loss: 0.0013]\n",
      "Epoch [8/10], batch: [46/66, loss: 0.0012]\n",
      "Epoch [8/10], batch: [47/66, loss: 0.0020]\n",
      "Epoch [8/10], batch: [48/66, loss: 0.0010]\n",
      "Epoch [8/10], batch: [49/66, loss: 0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], batch: [50/66, loss: 0.0010]\n",
      "Epoch [8/10], batch: [51/66, loss: 0.0043]\n",
      "Epoch [8/10], batch: [52/66, loss: 0.0071]\n",
      "Epoch [8/10], batch: [53/66, loss: 0.0155]\n",
      "Epoch [8/10], batch: [54/66, loss: 0.0329]\n",
      "Epoch [8/10], batch: [55/66, loss: 0.0039]\n",
      "Epoch [8/10], batch: [56/66, loss: 0.0050]\n",
      "Epoch [8/10], batch: [57/66, loss: 0.0083]\n",
      "Epoch [8/10], batch: [58/66, loss: 0.0635]\n",
      "Epoch [8/10], batch: [59/66, loss: 0.0037]\n",
      "Epoch [8/10], batch: [60/66, loss: 0.0010]\n",
      "Epoch [8/10], batch: [61/66, loss: 0.0735]\n",
      "Epoch [8/10], batch: [62/66, loss: 0.0068]\n",
      "Epoch [8/10], batch: [63/66, loss: 0.0019]\n",
      "Epoch [8/10], batch: [64/66, loss: 0.0031]\n",
      "Epoch [8/10], batch: [65/66, loss: 0.0033]\n",
      "Epoch [9/10], batch: [0/66, loss: 0.0049]\n",
      "Epoch [9/10], batch: [1/66, loss: 0.0074]\n",
      "Epoch [9/10], batch: [2/66, loss: 0.0029]\n",
      "Epoch [9/10], batch: [3/66, loss: 0.0020]\n",
      "Epoch [9/10], batch: [4/66, loss: 0.0123]\n",
      "Epoch [9/10], batch: [5/66, loss: 0.0337]\n",
      "Epoch [9/10], batch: [6/66, loss: 0.0027]\n",
      "Epoch [9/10], batch: [7/66, loss: 0.0014]\n",
      "Epoch [9/10], batch: [8/66, loss: 0.0011]\n",
      "Epoch [9/10], batch: [9/66, loss: 0.0356]\n",
      "Epoch [9/10], batch: [10/66, loss: 0.2224]\n",
      "Epoch [9/10], batch: [11/66, loss: 0.0017]\n",
      "Epoch [9/10], batch: [12/66, loss: 0.0012]\n",
      "Epoch [9/10], batch: [13/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [14/66, loss: 0.0055]\n",
      "Epoch [9/10], batch: [15/66, loss: 0.0077]\n",
      "Epoch [9/10], batch: [16/66, loss: 0.0244]\n",
      "Epoch [9/10], batch: [17/66, loss: 0.0025]\n",
      "Epoch [9/10], batch: [18/66, loss: 0.0391]\n",
      "Epoch [9/10], batch: [19/66, loss: 0.0017]\n",
      "Epoch [9/10], batch: [20/66, loss: 0.0034]\n",
      "Epoch [9/10], batch: [21/66, loss: 0.0067]\n",
      "Epoch [9/10], batch: [22/66, loss: 0.2242]\n",
      "Epoch [9/10], batch: [23/66, loss: 0.0170]\n",
      "Epoch [9/10], batch: [24/66, loss: 0.0009]\n",
      "Epoch [9/10], batch: [25/66, loss: 0.0056]\n",
      "Epoch [9/10], batch: [26/66, loss: 0.0075]\n",
      "Epoch [9/10], batch: [27/66, loss: 0.0060]\n",
      "Epoch [9/10], batch: [28/66, loss: 0.0019]\n",
      "Epoch [9/10], batch: [29/66, loss: 0.0037]\n",
      "Epoch [9/10], batch: [30/66, loss: 0.0015]\n",
      "Epoch [9/10], batch: [31/66, loss: 0.0038]\n",
      "Epoch [9/10], batch: [32/66, loss: 0.0013]\n",
      "Epoch [9/10], batch: [33/66, loss: 0.0096]\n",
      "Epoch [9/10], batch: [34/66, loss: 0.0072]\n",
      "Epoch [9/10], batch: [35/66, loss: 0.0053]\n",
      "Epoch [9/10], batch: [36/66, loss: 0.0033]\n",
      "Epoch [9/10], batch: [37/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [38/66, loss: 0.0015]\n",
      "Epoch [9/10], batch: [39/66, loss: 0.0016]\n",
      "Epoch [9/10], batch: [40/66, loss: 0.0041]\n",
      "Epoch [9/10], batch: [41/66, loss: 0.0016]\n",
      "Epoch [9/10], batch: [42/66, loss: 0.0041]\n",
      "Epoch [9/10], batch: [43/66, loss: 0.0008]\n",
      "Epoch [9/10], batch: [44/66, loss: 0.0010]\n",
      "Epoch [9/10], batch: [45/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [46/66, loss: 0.0063]\n",
      "Epoch [9/10], batch: [47/66, loss: 0.0009]\n",
      "Epoch [9/10], batch: [48/66, loss: 0.0009]\n",
      "Epoch [9/10], batch: [49/66, loss: 0.0056]\n",
      "Epoch [9/10], batch: [50/66, loss: 0.0007]\n",
      "Epoch [9/10], batch: [51/66, loss: 0.0027]\n",
      "Epoch [9/10], batch: [52/66, loss: 0.0014]\n",
      "Epoch [9/10], batch: [53/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [54/66, loss: 0.0007]\n",
      "Epoch [9/10], batch: [55/66, loss: 0.0089]\n",
      "Epoch [9/10], batch: [56/66, loss: 0.0055]\n",
      "Epoch [9/10], batch: [57/66, loss: 0.0126]\n",
      "Epoch [9/10], batch: [58/66, loss: 0.0019]\n",
      "Epoch [9/10], batch: [59/66, loss: 0.0442]\n",
      "Epoch [9/10], batch: [60/66, loss: 0.0704]\n",
      "Epoch [9/10], batch: [61/66, loss: 0.0043]\n",
      "Epoch [9/10], batch: [62/66, loss: 0.0013]\n",
      "Epoch [9/10], batch: [63/66, loss: 0.0251]\n",
      "Epoch [9/10], batch: [64/66, loss: 0.0361]\n",
      "Epoch [9/10], batch: [65/66, loss: 0.0248]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_v1(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e580ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 1.260\n",
      "Acccuracy  : 0.619\n",
      "F1 Score: 0.395\n"
     ]
    }
   ],
   "source": [
    "calc_loss_acc(model_v1, loss_func, dataloader_val_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4acc29",
   "metadata": {},
   "source": [
    "Before going to task 2, let's do the same for the other dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac0b1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_v2 = my_model(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_v2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa98788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], batch: [0/49, loss: 0.6960]\n",
      "Epoch [0/10], batch: [1/49, loss: 0.6824]\n",
      "Epoch [0/10], batch: [2/49, loss: 0.6708]\n",
      "Epoch [0/10], batch: [3/49, loss: 0.7575]\n",
      "Epoch [0/10], batch: [4/49, loss: 0.6610]\n",
      "Epoch [0/10], batch: [5/49, loss: 0.6911]\n",
      "Epoch [0/10], batch: [6/49, loss: 0.6920]\n",
      "Epoch [0/10], batch: [7/49, loss: 0.7035]\n",
      "Epoch [0/10], batch: [8/49, loss: 0.7175]\n",
      "Epoch [0/10], batch: [9/49, loss: 0.6835]\n",
      "Epoch [0/10], batch: [10/49, loss: 0.6329]\n",
      "Epoch [0/10], batch: [11/49, loss: 0.7672]\n",
      "Epoch [0/10], batch: [12/49, loss: 0.6350]\n",
      "Epoch [0/10], batch: [13/49, loss: 0.6699]\n",
      "Epoch [0/10], batch: [14/49, loss: 0.7420]\n",
      "Epoch [0/10], batch: [15/49, loss: 0.6911]\n",
      "Epoch [0/10], batch: [16/49, loss: 0.7797]\n",
      "Epoch [0/10], batch: [17/49, loss: 0.7311]\n",
      "Epoch [0/10], batch: [18/49, loss: 0.7098]\n",
      "Epoch [0/10], batch: [19/49, loss: 0.6794]\n",
      "Epoch [0/10], batch: [20/49, loss: 0.7351]\n",
      "Epoch [0/10], batch: [21/49, loss: 0.6988]\n",
      "Epoch [0/10], batch: [22/49, loss: 0.6931]\n",
      "Epoch [0/10], batch: [23/49, loss: 0.6646]\n",
      "Epoch [0/10], batch: [24/49, loss: 0.6919]\n",
      "Epoch [0/10], batch: [25/49, loss: 0.7000]\n",
      "Epoch [0/10], batch: [26/49, loss: 0.6754]\n",
      "Epoch [0/10], batch: [27/49, loss: 0.6520]\n",
      "Epoch [0/10], batch: [28/49, loss: 0.6789]\n",
      "Epoch [0/10], batch: [29/49, loss: 0.7574]\n",
      "Epoch [0/10], batch: [30/49, loss: 0.6869]\n",
      "Epoch [0/10], batch: [31/49, loss: 0.7021]\n",
      "Epoch [0/10], batch: [32/49, loss: 0.7355]\n",
      "Epoch [0/10], batch: [33/49, loss: 0.6289]\n",
      "Epoch [0/10], batch: [34/49, loss: 0.7681]\n",
      "Epoch [0/10], batch: [35/49, loss: 0.7442]\n",
      "Epoch [0/10], batch: [36/49, loss: 0.6853]\n",
      "Epoch [0/10], batch: [37/49, loss: 0.6944]\n",
      "Epoch [0/10], batch: [38/49, loss: 0.7317]\n",
      "Epoch [0/10], batch: [39/49, loss: 0.6840]\n",
      "Epoch [0/10], batch: [40/49, loss: 0.6793]\n",
      "Epoch [0/10], batch: [41/49, loss: 0.6758]\n",
      "Epoch [0/10], batch: [42/49, loss: 0.6758]\n",
      "Epoch [0/10], batch: [43/49, loss: 0.6857]\n",
      "Epoch [0/10], batch: [44/49, loss: 0.7109]\n",
      "Epoch [0/10], batch: [45/49, loss: 0.7157]\n",
      "Epoch [0/10], batch: [46/49, loss: 0.7227]\n",
      "Epoch [0/10], batch: [47/49, loss: 0.6949]\n",
      "Epoch [0/10], batch: [48/49, loss: 0.7166]\n",
      "Epoch [1/10], batch: [0/49, loss: 0.7091]\n",
      "Epoch [1/10], batch: [1/49, loss: 0.6661]\n",
      "Epoch [1/10], batch: [2/49, loss: 0.6792]\n",
      "Epoch [1/10], batch: [3/49, loss: 0.6426]\n",
      "Epoch [1/10], batch: [4/49, loss: 0.7111]\n",
      "Epoch [1/10], batch: [5/49, loss: 0.6456]\n",
      "Epoch [1/10], batch: [6/49, loss: 0.6723]\n",
      "Epoch [1/10], batch: [7/49, loss: 0.6558]\n",
      "Epoch [1/10], batch: [8/49, loss: 0.6409]\n",
      "Epoch [1/10], batch: [9/49, loss: 0.6589]\n",
      "Epoch [1/10], batch: [10/49, loss: 0.6517]\n",
      "Epoch [1/10], batch: [11/49, loss: 0.6558]\n",
      "Epoch [1/10], batch: [12/49, loss: 0.6939]\n",
      "Epoch [1/10], batch: [13/49, loss: 0.6637]\n",
      "Epoch [1/10], batch: [14/49, loss: 0.6288]\n",
      "Epoch [1/10], batch: [15/49, loss: 0.6666]\n",
      "Epoch [1/10], batch: [16/49, loss: 0.6312]\n",
      "Epoch [1/10], batch: [17/49, loss: 0.6284]\n",
      "Epoch [1/10], batch: [18/49, loss: 0.6708]\n",
      "Epoch [1/10], batch: [19/49, loss: 0.6385]\n",
      "Epoch [1/10], batch: [20/49, loss: 0.5659]\n",
      "Epoch [1/10], batch: [21/49, loss: 0.6122]\n",
      "Epoch [1/10], batch: [22/49, loss: 0.6087]\n",
      "Epoch [1/10], batch: [23/49, loss: 0.6509]\n",
      "Epoch [1/10], batch: [24/49, loss: 0.6715]\n",
      "Epoch [1/10], batch: [25/49, loss: 0.6300]\n",
      "Epoch [1/10], batch: [26/49, loss: 0.6402]\n",
      "Epoch [1/10], batch: [27/49, loss: 0.6083]\n",
      "Epoch [1/10], batch: [28/49, loss: 0.6301]\n",
      "Epoch [1/10], batch: [29/49, loss: 0.6224]\n",
      "Epoch [1/10], batch: [30/49, loss: 0.6244]\n",
      "Epoch [1/10], batch: [31/49, loss: 0.6483]\n",
      "Epoch [1/10], batch: [32/49, loss: 0.5175]\n",
      "Epoch [1/10], batch: [33/49, loss: 0.6609]\n",
      "Epoch [1/10], batch: [34/49, loss: 0.7113]\n",
      "Epoch [1/10], batch: [35/49, loss: 0.5860]\n",
      "Epoch [1/10], batch: [36/49, loss: 0.6616]\n",
      "Epoch [1/10], batch: [37/49, loss: 0.6437]\n",
      "Epoch [1/10], batch: [38/49, loss: 0.6154]\n",
      "Epoch [1/10], batch: [39/49, loss: 0.6306]\n",
      "Epoch [1/10], batch: [40/49, loss: 0.5879]\n",
      "Epoch [1/10], batch: [41/49, loss: 0.6428]\n",
      "Epoch [1/10], batch: [42/49, loss: 0.5871]\n",
      "Epoch [1/10], batch: [43/49, loss: 0.5867]\n",
      "Epoch [1/10], batch: [44/49, loss: 0.6118]\n",
      "Epoch [1/10], batch: [45/49, loss: 0.6025]\n",
      "Epoch [1/10], batch: [46/49, loss: 0.5917]\n",
      "Epoch [1/10], batch: [47/49, loss: 0.7068]\n",
      "Epoch [1/10], batch: [48/49, loss: 0.5711]\n",
      "Epoch [2/10], batch: [0/49, loss: 0.4990]\n",
      "Epoch [2/10], batch: [1/49, loss: 0.5189]\n",
      "Epoch [2/10], batch: [2/49, loss: 0.5172]\n",
      "Epoch [2/10], batch: [3/49, loss: 0.5465]\n",
      "Epoch [2/10], batch: [4/49, loss: 0.4992]\n",
      "Epoch [2/10], batch: [5/49, loss: 0.4936]\n",
      "Epoch [2/10], batch: [6/49, loss: 0.4954]\n",
      "Epoch [2/10], batch: [7/49, loss: 0.5534]\n",
      "Epoch [2/10], batch: [8/49, loss: 0.4822]\n",
      "Epoch [2/10], batch: [9/49, loss: 0.4520]\n",
      "Epoch [2/10], batch: [10/49, loss: 0.4588]\n",
      "Epoch [2/10], batch: [11/49, loss: 0.4157]\n",
      "Epoch [2/10], batch: [12/49, loss: 0.4084]\n",
      "Epoch [2/10], batch: [13/49, loss: 0.3584]\n",
      "Epoch [2/10], batch: [14/49, loss: 0.4353]\n",
      "Epoch [2/10], batch: [15/49, loss: 0.5112]\n",
      "Epoch [2/10], batch: [16/49, loss: 0.4387]\n",
      "Epoch [2/10], batch: [17/49, loss: 0.4033]\n",
      "Epoch [2/10], batch: [18/49, loss: 0.3637]\n",
      "Epoch [2/10], batch: [19/49, loss: 0.4658]\n",
      "Epoch [2/10], batch: [20/49, loss: 0.3036]\n",
      "Epoch [2/10], batch: [21/49, loss: 0.4473]\n",
      "Epoch [2/10], batch: [22/49, loss: 0.3429]\n",
      "Epoch [2/10], batch: [23/49, loss: 0.3802]\n",
      "Epoch [2/10], batch: [24/49, loss: 0.4272]\n",
      "Epoch [2/10], batch: [25/49, loss: 0.4135]\n",
      "Epoch [2/10], batch: [26/49, loss: 0.4569]\n",
      "Epoch [2/10], batch: [27/49, loss: 0.6659]\n",
      "Epoch [2/10], batch: [28/49, loss: 0.3001]\n",
      "Epoch [2/10], batch: [29/49, loss: 0.4279]\n",
      "Epoch [2/10], batch: [30/49, loss: 0.3819]\n",
      "Epoch [2/10], batch: [31/49, loss: 0.4522]\n",
      "Epoch [2/10], batch: [32/49, loss: 0.4173]\n",
      "Epoch [2/10], batch: [33/49, loss: 0.3988]\n",
      "Epoch [2/10], batch: [34/49, loss: 0.3938]\n",
      "Epoch [2/10], batch: [35/49, loss: 0.4230]\n",
      "Epoch [2/10], batch: [36/49, loss: 0.5004]\n",
      "Epoch [2/10], batch: [37/49, loss: 0.2844]\n",
      "Epoch [2/10], batch: [38/49, loss: 0.4508]\n",
      "Epoch [2/10], batch: [39/49, loss: 0.5883]\n",
      "Epoch [2/10], batch: [40/49, loss: 0.3971]\n",
      "Epoch [2/10], batch: [41/49, loss: 0.6758]\n",
      "Epoch [2/10], batch: [42/49, loss: 0.4202]\n",
      "Epoch [2/10], batch: [43/49, loss: 0.5164]\n",
      "Epoch [2/10], batch: [44/49, loss: 0.4736]\n",
      "Epoch [2/10], batch: [45/49, loss: 0.4423]\n",
      "Epoch [2/10], batch: [46/49, loss: 0.4891]\n",
      "Epoch [2/10], batch: [47/49, loss: 0.3968]\n",
      "Epoch [2/10], batch: [48/49, loss: 0.4043]\n",
      "Epoch [3/10], batch: [0/49, loss: 0.3555]\n",
      "Epoch [3/10], batch: [1/49, loss: 0.1773]\n",
      "Epoch [3/10], batch: [2/49, loss: 0.2370]\n",
      "Epoch [3/10], batch: [3/49, loss: 0.2174]\n",
      "Epoch [3/10], batch: [4/49, loss: 0.3088]\n",
      "Epoch [3/10], batch: [5/49, loss: 0.1615]\n",
      "Epoch [3/10], batch: [6/49, loss: 0.2132]\n",
      "Epoch [3/10], batch: [7/49, loss: 0.1723]\n",
      "Epoch [3/10], batch: [8/49, loss: 0.2085]\n",
      "Epoch [3/10], batch: [9/49, loss: 0.2793]\n",
      "Epoch [3/10], batch: [10/49, loss: 0.1403]\n",
      "Epoch [3/10], batch: [11/49, loss: 0.1464]\n",
      "Epoch [3/10], batch: [12/49, loss: 0.4316]\n",
      "Epoch [3/10], batch: [13/49, loss: 0.1761]\n",
      "Epoch [3/10], batch: [14/49, loss: 0.2639]\n",
      "Epoch [3/10], batch: [15/49, loss: 0.1333]\n",
      "Epoch [3/10], batch: [16/49, loss: 0.1734]\n",
      "Epoch [3/10], batch: [17/49, loss: 0.1429]\n",
      "Epoch [3/10], batch: [18/49, loss: 0.1645]\n",
      "Epoch [3/10], batch: [19/49, loss: 0.1472]\n",
      "Epoch [3/10], batch: [20/49, loss: 0.2059]\n",
      "Epoch [3/10], batch: [21/49, loss: 0.1255]\n",
      "Epoch [3/10], batch: [22/49, loss: 0.0899]\n",
      "Epoch [3/10], batch: [23/49, loss: 0.1878]\n",
      "Epoch [3/10], batch: [24/49, loss: 0.1542]\n",
      "Epoch [3/10], batch: [25/49, loss: 0.1148]\n",
      "Epoch [3/10], batch: [26/49, loss: 0.2717]\n",
      "Epoch [3/10], batch: [27/49, loss: 0.1057]\n",
      "Epoch [3/10], batch: [28/49, loss: 0.1236]\n",
      "Epoch [3/10], batch: [29/49, loss: 0.1111]\n",
      "Epoch [3/10], batch: [30/49, loss: 0.1830]\n",
      "Epoch [3/10], batch: [31/49, loss: 0.1710]\n",
      "Epoch [3/10], batch: [32/49, loss: 0.0769]\n",
      "Epoch [3/10], batch: [33/49, loss: 0.2255]\n",
      "Epoch [3/10], batch: [34/49, loss: 0.2662]\n",
      "Epoch [3/10], batch: [35/49, loss: 0.1565]\n",
      "Epoch [3/10], batch: [36/49, loss: 0.1632]\n",
      "Epoch [3/10], batch: [37/49, loss: 0.3721]\n",
      "Epoch [3/10], batch: [38/49, loss: 0.1209]\n",
      "Epoch [3/10], batch: [39/49, loss: 0.3157]\n",
      "Epoch [3/10], batch: [40/49, loss: 0.4241]\n",
      "Epoch [3/10], batch: [41/49, loss: 0.2156]\n",
      "Epoch [3/10], batch: [42/49, loss: 0.4503]\n",
      "Epoch [3/10], batch: [43/49, loss: 0.2958]\n",
      "Epoch [3/10], batch: [44/49, loss: 0.1944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], batch: [45/49, loss: 0.1587]\n",
      "Epoch [3/10], batch: [46/49, loss: 0.3590]\n",
      "Epoch [3/10], batch: [47/49, loss: 0.1778]\n",
      "Epoch [3/10], batch: [48/49, loss: 0.1767]\n",
      "Epoch [4/10], batch: [0/49, loss: 0.3426]\n",
      "Epoch [4/10], batch: [1/49, loss: 0.0753]\n",
      "Epoch [4/10], batch: [2/49, loss: 0.0924]\n",
      "Epoch [4/10], batch: [3/49, loss: 0.0638]\n",
      "Epoch [4/10], batch: [4/49, loss: 0.0924]\n",
      "Epoch [4/10], batch: [5/49, loss: 0.2562]\n",
      "Epoch [4/10], batch: [6/49, loss: 0.0592]\n",
      "Epoch [4/10], batch: [7/49, loss: 0.1063]\n",
      "Epoch [4/10], batch: [8/49, loss: 0.1093]\n",
      "Epoch [4/10], batch: [9/49, loss: 0.1094]\n",
      "Epoch [4/10], batch: [10/49, loss: 0.0323]\n",
      "Epoch [4/10], batch: [11/49, loss: 0.0541]\n",
      "Epoch [4/10], batch: [12/49, loss: 0.0842]\n",
      "Epoch [4/10], batch: [13/49, loss: 0.0614]\n",
      "Epoch [4/10], batch: [14/49, loss: 0.1037]\n",
      "Epoch [4/10], batch: [15/49, loss: 0.0667]\n",
      "Epoch [4/10], batch: [16/49, loss: 0.1489]\n",
      "Epoch [4/10], batch: [17/49, loss: 0.0379]\n",
      "Epoch [4/10], batch: [18/49, loss: 0.1034]\n",
      "Epoch [4/10], batch: [19/49, loss: 0.0781]\n",
      "Epoch [4/10], batch: [20/49, loss: 0.0341]\n",
      "Epoch [4/10], batch: [21/49, loss: 0.1005]\n",
      "Epoch [4/10], batch: [22/49, loss: 0.0480]\n",
      "Epoch [4/10], batch: [23/49, loss: 0.0892]\n",
      "Epoch [4/10], batch: [24/49, loss: 0.0512]\n",
      "Epoch [4/10], batch: [25/49, loss: 0.1019]\n",
      "Epoch [4/10], batch: [26/49, loss: 0.0281]\n",
      "Epoch [4/10], batch: [27/49, loss: 0.0445]\n",
      "Epoch [4/10], batch: [28/49, loss: 0.0488]\n",
      "Epoch [4/10], batch: [29/49, loss: 0.0485]\n",
      "Epoch [4/10], batch: [30/49, loss: 0.0524]\n",
      "Epoch [4/10], batch: [31/49, loss: 0.1531]\n",
      "Epoch [4/10], batch: [32/49, loss: 0.0385]\n",
      "Epoch [4/10], batch: [33/49, loss: 0.0869]\n",
      "Epoch [4/10], batch: [34/49, loss: 0.0504]\n",
      "Epoch [4/10], batch: [35/49, loss: 0.0368]\n",
      "Epoch [4/10], batch: [36/49, loss: 0.1675]\n",
      "Epoch [4/10], batch: [37/49, loss: 0.0750]\n",
      "Epoch [4/10], batch: [38/49, loss: 0.0407]\n",
      "Epoch [4/10], batch: [39/49, loss: 0.3014]\n",
      "Epoch [4/10], batch: [40/49, loss: 0.2094]\n",
      "Epoch [4/10], batch: [41/49, loss: 0.1146]\n",
      "Epoch [4/10], batch: [42/49, loss: 0.2865]\n",
      "Epoch [4/10], batch: [43/49, loss: 0.0366]\n",
      "Epoch [4/10], batch: [44/49, loss: 0.0956]\n",
      "Epoch [4/10], batch: [45/49, loss: 0.0792]\n",
      "Epoch [4/10], batch: [46/49, loss: 0.1540]\n",
      "Epoch [4/10], batch: [47/49, loss: 0.0292]\n",
      "Epoch [4/10], batch: [48/49, loss: 0.1968]\n",
      "Epoch [5/10], batch: [0/49, loss: 0.0499]\n",
      "Epoch [5/10], batch: [1/49, loss: 0.0252]\n",
      "Epoch [5/10], batch: [2/49, loss: 0.1054]\n",
      "Epoch [5/10], batch: [3/49, loss: 0.0253]\n",
      "Epoch [5/10], batch: [4/49, loss: 0.3010]\n",
      "Epoch [5/10], batch: [5/49, loss: 0.0566]\n",
      "Epoch [5/10], batch: [6/49, loss: 0.0454]\n",
      "Epoch [5/10], batch: [7/49, loss: 0.0526]\n",
      "Epoch [5/10], batch: [8/49, loss: 0.1302]\n",
      "Epoch [5/10], batch: [9/49, loss: 0.1883]\n",
      "Epoch [5/10], batch: [10/49, loss: 0.0223]\n",
      "Epoch [5/10], batch: [11/49, loss: 0.0826]\n",
      "Epoch [5/10], batch: [12/49, loss: 0.1557]\n",
      "Epoch [5/10], batch: [13/49, loss: 0.2394]\n",
      "Epoch [5/10], batch: [14/49, loss: 0.0177]\n",
      "Epoch [5/10], batch: [15/49, loss: 0.1706]\n",
      "Epoch [5/10], batch: [16/49, loss: 0.1834]\n",
      "Epoch [5/10], batch: [17/49, loss: 0.1338]\n",
      "Epoch [5/10], batch: [18/49, loss: 0.0700]\n",
      "Epoch [5/10], batch: [19/49, loss: 0.0757]\n",
      "Epoch [5/10], batch: [20/49, loss: 0.0320]\n",
      "Epoch [5/10], batch: [21/49, loss: 0.1594]\n",
      "Epoch [5/10], batch: [22/49, loss: 0.0443]\n",
      "Epoch [5/10], batch: [23/49, loss: 0.0358]\n",
      "Epoch [5/10], batch: [24/49, loss: 0.0812]\n",
      "Epoch [5/10], batch: [25/49, loss: 0.0451]\n",
      "Epoch [5/10], batch: [26/49, loss: 0.0531]\n",
      "Epoch [5/10], batch: [27/49, loss: 0.0701]\n",
      "Epoch [5/10], batch: [28/49, loss: 0.0253]\n",
      "Epoch [5/10], batch: [29/49, loss: 0.0185]\n",
      "Epoch [5/10], batch: [30/49, loss: 0.0185]\n",
      "Epoch [5/10], batch: [31/49, loss: 0.1141]\n",
      "Epoch [5/10], batch: [32/49, loss: 0.0136]\n",
      "Epoch [5/10], batch: [33/49, loss: 0.0893]\n",
      "Epoch [5/10], batch: [34/49, loss: 0.0455]\n",
      "Epoch [5/10], batch: [35/49, loss: 0.0261]\n",
      "Epoch [5/10], batch: [36/49, loss: 0.0392]\n",
      "Epoch [5/10], batch: [37/49, loss: 0.0251]\n",
      "Epoch [5/10], batch: [38/49, loss: 0.1528]\n",
      "Epoch [5/10], batch: [39/49, loss: 0.0357]\n",
      "Epoch [5/10], batch: [40/49, loss: 0.0327]\n",
      "Epoch [5/10], batch: [41/49, loss: 0.0665]\n",
      "Epoch [5/10], batch: [42/49, loss: 0.0207]\n",
      "Epoch [5/10], batch: [43/49, loss: 0.0284]\n",
      "Epoch [5/10], batch: [44/49, loss: 0.0401]\n",
      "Epoch [5/10], batch: [45/49, loss: 0.0193]\n",
      "Epoch [5/10], batch: [46/49, loss: 0.0364]\n",
      "Epoch [5/10], batch: [47/49, loss: 0.0131]\n",
      "Epoch [5/10], batch: [48/49, loss: 0.0970]\n",
      "Epoch [6/10], batch: [0/49, loss: 0.0285]\n",
      "Epoch [6/10], batch: [1/49, loss: 0.0350]\n",
      "Epoch [6/10], batch: [2/49, loss: 0.0183]\n",
      "Epoch [6/10], batch: [3/49, loss: 0.0269]\n",
      "Epoch [6/10], batch: [4/49, loss: 0.0170]\n",
      "Epoch [6/10], batch: [5/49, loss: 0.0102]\n",
      "Epoch [6/10], batch: [6/49, loss: 0.0793]\n",
      "Epoch [6/10], batch: [7/49, loss: 0.0148]\n",
      "Epoch [6/10], batch: [8/49, loss: 0.0465]\n",
      "Epoch [6/10], batch: [9/49, loss: 0.0073]\n",
      "Epoch [6/10], batch: [10/49, loss: 0.0085]\n",
      "Epoch [6/10], batch: [11/49, loss: 0.0072]\n",
      "Epoch [6/10], batch: [12/49, loss: 0.0182]\n",
      "Epoch [6/10], batch: [13/49, loss: 0.0871]\n",
      "Epoch [6/10], batch: [14/49, loss: 0.0576]\n",
      "Epoch [6/10], batch: [15/49, loss: 0.0201]\n",
      "Epoch [6/10], batch: [16/49, loss: 0.1045]\n",
      "Epoch [6/10], batch: [17/49, loss: 0.0186]\n",
      "Epoch [6/10], batch: [18/49, loss: 0.0112]\n",
      "Epoch [6/10], batch: [19/49, loss: 0.0057]\n",
      "Epoch [6/10], batch: [20/49, loss: 0.0203]\n",
      "Epoch [6/10], batch: [21/49, loss: 0.0350]\n",
      "Epoch [6/10], batch: [22/49, loss: 0.0132]\n",
      "Epoch [6/10], batch: [23/49, loss: 0.0126]\n",
      "Epoch [6/10], batch: [24/49, loss: 0.0151]\n",
      "Epoch [6/10], batch: [25/49, loss: 0.0090]\n",
      "Epoch [6/10], batch: [26/49, loss: 0.0179]\n",
      "Epoch [6/10], batch: [27/49, loss: 0.0450]\n",
      "Epoch [6/10], batch: [28/49, loss: 0.0095]\n",
      "Epoch [6/10], batch: [29/49, loss: 0.0115]\n",
      "Epoch [6/10], batch: [30/49, loss: 0.0054]\n",
      "Epoch [6/10], batch: [31/49, loss: 0.1344]\n",
      "Epoch [6/10], batch: [32/49, loss: 0.0582]\n",
      "Epoch [6/10], batch: [33/49, loss: 0.0103]\n",
      "Epoch [6/10], batch: [34/49, loss: 0.0091]\n",
      "Epoch [6/10], batch: [35/49, loss: 0.0130]\n",
      "Epoch [6/10], batch: [36/49, loss: 0.0037]\n",
      "Epoch [6/10], batch: [37/49, loss: 0.0443]\n",
      "Epoch [6/10], batch: [38/49, loss: 0.0070]\n",
      "Epoch [6/10], batch: [39/49, loss: 0.0105]\n",
      "Epoch [6/10], batch: [40/49, loss: 0.0116]\n",
      "Epoch [6/10], batch: [41/49, loss: 0.0072]\n",
      "Epoch [6/10], batch: [42/49, loss: 0.0171]\n",
      "Epoch [6/10], batch: [43/49, loss: 0.0289]\n",
      "Epoch [6/10], batch: [44/49, loss: 0.0775]\n",
      "Epoch [6/10], batch: [45/49, loss: 0.0110]\n",
      "Epoch [6/10], batch: [46/49, loss: 0.0117]\n",
      "Epoch [6/10], batch: [47/49, loss: 0.0352]\n",
      "Epoch [6/10], batch: [48/49, loss: 0.0434]\n",
      "Epoch [7/10], batch: [0/49, loss: 0.0068]\n",
      "Epoch [7/10], batch: [1/49, loss: 0.0050]\n",
      "Epoch [7/10], batch: [2/49, loss: 0.0054]\n",
      "Epoch [7/10], batch: [3/49, loss: 0.0084]\n",
      "Epoch [7/10], batch: [4/49, loss: 0.0191]\n",
      "Epoch [7/10], batch: [5/49, loss: 0.0238]\n",
      "Epoch [7/10], batch: [6/49, loss: 0.0050]\n",
      "Epoch [7/10], batch: [7/49, loss: 0.0441]\n",
      "Epoch [7/10], batch: [8/49, loss: 0.0020]\n",
      "Epoch [7/10], batch: [9/49, loss: 0.0108]\n",
      "Epoch [7/10], batch: [10/49, loss: 0.0115]\n",
      "Epoch [7/10], batch: [11/49, loss: 0.0149]\n",
      "Epoch [7/10], batch: [12/49, loss: 0.0139]\n",
      "Epoch [7/10], batch: [13/49, loss: 0.0828]\n",
      "Epoch [7/10], batch: [14/49, loss: 0.0364]\n",
      "Epoch [7/10], batch: [15/49, loss: 0.0051]\n",
      "Epoch [7/10], batch: [16/49, loss: 0.0042]\n",
      "Epoch [7/10], batch: [17/49, loss: 0.0449]\n",
      "Epoch [7/10], batch: [18/49, loss: 0.1324]\n",
      "Epoch [7/10], batch: [19/49, loss: 0.0133]\n",
      "Epoch [7/10], batch: [20/49, loss: 0.0119]\n",
      "Epoch [7/10], batch: [21/49, loss: 0.1413]\n",
      "Epoch [7/10], batch: [22/49, loss: 0.0933]\n",
      "Epoch [7/10], batch: [23/49, loss: 0.0043]\n",
      "Epoch [7/10], batch: [24/49, loss: 0.1531]\n",
      "Epoch [7/10], batch: [25/49, loss: 0.0020]\n",
      "Epoch [7/10], batch: [26/49, loss: 0.0090]\n",
      "Epoch [7/10], batch: [27/49, loss: 0.0070]\n",
      "Epoch [7/10], batch: [28/49, loss: 0.0016]\n",
      "Epoch [7/10], batch: [29/49, loss: 0.0110]\n",
      "Epoch [7/10], batch: [30/49, loss: 0.0545]\n",
      "Epoch [7/10], batch: [31/49, loss: 0.0016]\n",
      "Epoch [7/10], batch: [32/49, loss: 0.0103]\n",
      "Epoch [7/10], batch: [33/49, loss: 0.0062]\n",
      "Epoch [7/10], batch: [34/49, loss: 0.0273]\n",
      "Epoch [7/10], batch: [35/49, loss: 0.0064]\n",
      "Epoch [7/10], batch: [36/49, loss: 0.0110]\n",
      "Epoch [7/10], batch: [37/49, loss: 0.0074]\n",
      "Epoch [7/10], batch: [38/49, loss: 0.0074]\n",
      "Epoch [7/10], batch: [39/49, loss: 0.0147]\n",
      "Epoch [7/10], batch: [40/49, loss: 0.0484]\n",
      "Epoch [7/10], batch: [41/49, loss: 0.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], batch: [42/49, loss: 0.0148]\n",
      "Epoch [7/10], batch: [43/49, loss: 0.0037]\n",
      "Epoch [7/10], batch: [44/49, loss: 0.0081]\n",
      "Epoch [7/10], batch: [45/49, loss: 0.0797]\n",
      "Epoch [7/10], batch: [46/49, loss: 0.2199]\n",
      "Epoch [7/10], batch: [47/49, loss: 0.0100]\n",
      "Epoch [7/10], batch: [48/49, loss: 0.0074]\n",
      "Epoch [8/10], batch: [0/49, loss: 0.0592]\n",
      "Epoch [8/10], batch: [1/49, loss: 0.0339]\n",
      "Epoch [8/10], batch: [2/49, loss: 0.0030]\n",
      "Epoch [8/10], batch: [3/49, loss: 0.0231]\n",
      "Epoch [8/10], batch: [4/49, loss: 0.0059]\n",
      "Epoch [8/10], batch: [5/49, loss: 0.0072]\n",
      "Epoch [8/10], batch: [6/49, loss: 0.0033]\n",
      "Epoch [8/10], batch: [7/49, loss: 0.0116]\n",
      "Epoch [8/10], batch: [8/49, loss: 0.0214]\n",
      "Epoch [8/10], batch: [9/49, loss: 0.0081]\n",
      "Epoch [8/10], batch: [10/49, loss: 0.0042]\n",
      "Epoch [8/10], batch: [11/49, loss: 0.0053]\n",
      "Epoch [8/10], batch: [12/49, loss: 0.0074]\n",
      "Epoch [8/10], batch: [13/49, loss: 0.0091]\n",
      "Epoch [8/10], batch: [14/49, loss: 0.0030]\n",
      "Epoch [8/10], batch: [15/49, loss: 0.0790]\n",
      "Epoch [8/10], batch: [16/49, loss: 0.0046]\n",
      "Epoch [8/10], batch: [17/49, loss: 0.0370]\n",
      "Epoch [8/10], batch: [18/49, loss: 0.0731]\n",
      "Epoch [8/10], batch: [19/49, loss: 0.0179]\n",
      "Epoch [8/10], batch: [20/49, loss: 0.0044]\n",
      "Epoch [8/10], batch: [21/49, loss: 0.0013]\n",
      "Epoch [8/10], batch: [22/49, loss: 0.0581]\n",
      "Epoch [8/10], batch: [23/49, loss: 0.0137]\n",
      "Epoch [8/10], batch: [24/49, loss: 0.0433]\n",
      "Epoch [8/10], batch: [25/49, loss: 0.0037]\n",
      "Epoch [8/10], batch: [26/49, loss: 0.0126]\n",
      "Epoch [8/10], batch: [27/49, loss: 0.0039]\n",
      "Epoch [8/10], batch: [28/49, loss: 0.0020]\n",
      "Epoch [8/10], batch: [29/49, loss: 0.0416]\n",
      "Epoch [8/10], batch: [30/49, loss: 0.0076]\n",
      "Epoch [8/10], batch: [31/49, loss: 0.0035]\n",
      "Epoch [8/10], batch: [32/49, loss: 0.0531]\n",
      "Epoch [8/10], batch: [33/49, loss: 0.1554]\n",
      "Epoch [8/10], batch: [34/49, loss: 0.0292]\n",
      "Epoch [8/10], batch: [35/49, loss: 0.0132]\n",
      "Epoch [8/10], batch: [36/49, loss: 0.0757]\n",
      "Epoch [8/10], batch: [37/49, loss: 0.0209]\n",
      "Epoch [8/10], batch: [38/49, loss: 0.0059]\n",
      "Epoch [8/10], batch: [39/49, loss: 0.0070]\n",
      "Epoch [8/10], batch: [40/49, loss: 0.0242]\n",
      "Epoch [8/10], batch: [41/49, loss: 0.0653]\n",
      "Epoch [8/10], batch: [42/49, loss: 0.0115]\n",
      "Epoch [8/10], batch: [43/49, loss: 0.0807]\n",
      "Epoch [8/10], batch: [44/49, loss: 0.0013]\n",
      "Epoch [8/10], batch: [45/49, loss: 0.0041]\n",
      "Epoch [8/10], batch: [46/49, loss: 0.1661]\n",
      "Epoch [8/10], batch: [47/49, loss: 0.1521]\n",
      "Epoch [8/10], batch: [48/49, loss: 0.0033]\n",
      "Epoch [9/10], batch: [0/49, loss: 0.0021]\n",
      "Epoch [9/10], batch: [1/49, loss: 0.0726]\n",
      "Epoch [9/10], batch: [2/49, loss: 0.1735]\n",
      "Epoch [9/10], batch: [3/49, loss: 0.0886]\n",
      "Epoch [9/10], batch: [4/49, loss: 0.0107]\n",
      "Epoch [9/10], batch: [5/49, loss: 0.0018]\n",
      "Epoch [9/10], batch: [6/49, loss: 0.0793]\n",
      "Epoch [9/10], batch: [7/49, loss: 0.1442]\n",
      "Epoch [9/10], batch: [8/49, loss: 0.0169]\n",
      "Epoch [9/10], batch: [9/49, loss: 0.0117]\n",
      "Epoch [9/10], batch: [10/49, loss: 0.0191]\n",
      "Epoch [9/10], batch: [11/49, loss: 0.1630]\n",
      "Epoch [9/10], batch: [12/49, loss: 0.0218]\n",
      "Epoch [9/10], batch: [13/49, loss: 0.0488]\n",
      "Epoch [9/10], batch: [14/49, loss: 0.0184]\n",
      "Epoch [9/10], batch: [15/49, loss: 0.0463]\n",
      "Epoch [9/10], batch: [16/49, loss: 0.0113]\n",
      "Epoch [9/10], batch: [17/49, loss: 0.0031]\n",
      "Epoch [9/10], batch: [18/49, loss: 0.0130]\n",
      "Epoch [9/10], batch: [19/49, loss: 0.0244]\n",
      "Epoch [9/10], batch: [20/49, loss: 0.0392]\n",
      "Epoch [9/10], batch: [21/49, loss: 0.0017]\n",
      "Epoch [9/10], batch: [22/49, loss: 0.0144]\n",
      "Epoch [9/10], batch: [23/49, loss: 0.0033]\n",
      "Epoch [9/10], batch: [24/49, loss: 0.0112]\n",
      "Epoch [9/10], batch: [25/49, loss: 0.0049]\n",
      "Epoch [9/10], batch: [26/49, loss: 0.0033]\n",
      "Epoch [9/10], batch: [27/49, loss: 0.0067]\n",
      "Epoch [9/10], batch: [28/49, loss: 0.0379]\n",
      "Epoch [9/10], batch: [29/49, loss: 0.0055]\n",
      "Epoch [9/10], batch: [30/49, loss: 0.1120]\n",
      "Epoch [9/10], batch: [31/49, loss: 0.0113]\n",
      "Epoch [9/10], batch: [32/49, loss: 0.0027]\n",
      "Epoch [9/10], batch: [33/49, loss: 0.0025]\n",
      "Epoch [9/10], batch: [34/49, loss: 0.0039]\n",
      "Epoch [9/10], batch: [35/49, loss: 0.0171]\n",
      "Epoch [9/10], batch: [36/49, loss: 0.0079]\n",
      "Epoch [9/10], batch: [37/49, loss: 0.0151]\n",
      "Epoch [9/10], batch: [38/49, loss: 0.0071]\n",
      "Epoch [9/10], batch: [39/49, loss: 0.0074]\n",
      "Epoch [9/10], batch: [40/49, loss: 0.0115]\n",
      "Epoch [9/10], batch: [41/49, loss: 0.0030]\n",
      "Epoch [9/10], batch: [42/49, loss: 0.0113]\n",
      "Epoch [9/10], batch: [43/49, loss: 0.0031]\n",
      "Epoch [9/10], batch: [44/49, loss: 0.0054]\n",
      "Epoch [9/10], batch: [45/49, loss: 0.0014]\n",
      "Epoch [9/10], batch: [46/49, loss: 0.0021]\n",
      "Epoch [9/10], batch: [47/49, loss: 0.0704]\n",
      "Epoch [9/10], batch: [48/49, loss: 0.0058]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_batches = len(dataloader_train_V2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, d in enumerate(dataloader_train_V2):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_v2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46cf733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 1.752\n",
      "Acccuracy  : 0.597\n",
      "F1 Score: 0.522\n"
     ]
    }
   ],
   "source": [
    "calc_loss_acc(model_v2, loss_func, dataloader_val_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82b953",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "#### Now that we managed to get the models running, we try to improve the models using the train and validation sets. One thing we will do is to remove some layers, as I have a feeling that we have to many convolutional layers for a problem that doesn't need such a deep structure. As mentioned in the task description, we only need to do so with one of the two datasets. In my case this is the V1 dataset (anger and joy).\n",
    "#### The first model in the race is the same as above. We will train with 15 epochs though and report the validation error at end of every epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae6560",
   "metadata": {},
   "source": [
    "### Training and evaluating Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544d28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=5, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(64, 16, kernel_size=3, padding=\"same\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.linear = nn.Linear(16, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # pool the 16 dimension to 1\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2659a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_1 = my_model_1(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d024c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.673\n",
      "Acccuracy  : 0.623\n",
      "F1 Score: 0.126\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.608\n",
      "Acccuracy  : 0.670\n",
      "F1 Score: 0.157\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.677\n",
      "Acccuracy  : 0.607\n",
      "F1 Score: 0.019\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.547\n",
      "Acccuracy  : 0.675\n",
      "F1 Score: 0.149\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.679\n",
      "Acccuracy  : 0.580\n",
      "F1 Score: 0.270\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.407\n",
      "Acccuracy  : 0.835\n",
      "F1 Score: 0.723\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.822\n",
      "Acccuracy  : 0.568\n",
      "F1 Score: 0.419\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.268\n",
      "Acccuracy  : 0.919\n",
      "F1 Score: 0.877\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.963\n",
      "Acccuracy  : 0.580\n",
      "F1 Score: 0.471\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.152\n",
      "Acccuracy  : 0.951\n",
      "F1 Score: 0.928\n",
      "################################\n",
      "--------EPOCH: 6 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.242\n",
      "Acccuracy  : 0.553\n",
      "F1 Score: 0.465\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.090\n",
      "Acccuracy  : 0.980\n",
      "F1 Score: 0.970\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_1(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_1, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_1, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0e603",
   "metadata": {},
   "source": [
    "### Training and evalating Model 2\n",
    "#### Let's make some changes to the model and the parameters and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90f560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 256, kernel_size=10, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(256, 128, kernel_size=8, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(128, 64, kernel_size=6, padding=\"same\")\n",
    "        self.conv4 = nn.Conv1d(64, 32, kernel_size=3, padding=\"same\")\n",
    "\n",
    "\n",
    "        self.linear2 = nn.Linear(32, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x, _ = x.max(dim=-1)\n",
    "        #rint(x.shape)\n",
    "        \n",
    "        \n",
    "       # print(x.shape)\n",
    "\n",
    "\n",
    "        y_out = self.linear2(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1769b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model_2 = my_model_2(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b07ee381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damja\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:297: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ..\\aten\\src\\ATen\\native\\Convolution.cpp:647.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.672\n",
      "Acccuracy  : 0.623\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.635\n",
      "Acccuracy  : 0.664\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.696\n",
      "Acccuracy  : 0.623\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.516\n",
      "Acccuracy  : 0.664\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.781\n",
      "Acccuracy  : 0.553\n",
      "F1 Score: 0.470\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.247\n",
      "Acccuracy  : 0.912\n",
      "F1 Score: 0.880\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.027\n",
      "Acccuracy  : 0.572\n",
      "F1 Score: 0.439\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.100\n",
      "Acccuracy  : 0.963\n",
      "F1 Score: 0.947\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.337\n",
      "Acccuracy  : 0.595\n",
      "F1 Score: 0.422\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.039\n",
      "Acccuracy  : 0.989\n",
      "F1 Score: 0.983\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_2, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_2, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68223f",
   "metadata": {},
   "source": [
    "### Training and evalating Model 3\n",
    "#### Let's see what additional changes we can make to make the model perform better. It is really difficult as I do not have a intuition what makes the model perform well and what not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08948473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_3(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 64, kernel_size=10, padding=\"same\")\n",
    "\n",
    "\n",
    "        \n",
    "        self.linear2 = nn.Linear(64, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear2(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eb8e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_3 = my_model_3(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_3.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0af6a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.687\n",
      "Acccuracy  : 0.576\n",
      "F1 Score: 0.384\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.402\n",
      "Acccuracy  : 0.828\n",
      "F1 Score: 0.743\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.703\n",
      "Acccuracy  : 0.599\n",
      "F1 Score: 0.309\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.248\n",
      "Acccuracy  : 0.941\n",
      "F1 Score: 0.907\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.800\n",
      "Acccuracy  : 0.611\n",
      "F1 Score: 0.231\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.161\n",
      "Acccuracy  : 0.954\n",
      "F1 Score: 0.927\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.777\n",
      "Acccuracy  : 0.615\n",
      "F1 Score: 0.377\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.076\n",
      "Acccuracy  : 0.991\n",
      "F1 Score: 0.987\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.858\n",
      "Acccuracy  : 0.654\n",
      "F1 Score: 0.440\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.048\n",
      "Acccuracy  : 0.995\n",
      "F1 Score: 0.993\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_3(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_3, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_3, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341fa87",
   "metadata": {},
   "source": [
    "## Testing all 3 models on the validation sets, to determine which model is the best.\n",
    "### Now, I will use all the trained models and evaluate them on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fd3f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### EVALUATING MODEL 1 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.195\n",
      "Acccuracy  : 0.521\n",
      "F1 Score: 0.428\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 2 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.337\n",
      "Acccuracy  : 0.595\n",
      "F1 Score: 0.422\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 3 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.858\n",
      "Acccuracy  : 0.654\n",
      "F1 Score: 0.440\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"#### EVALUATING MODEL 1 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_1, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(\"#### EVALUATING MODEL 2 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_2, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(\"#### EVALUATING MODEL 3 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_3, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cc9ca",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "#### Let's look at the performance of our best model, which seems to be model 3 on our other dataset. For this, we of course need to train the model again on the other dataset and evaluate it. Note that we should evaluate it on the test set at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2b1e2",
   "metadata": {},
   "source": [
    "First, we will initialize the model and then train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68de598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model_2_v2 = my_model_2(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1f21e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.710\n",
      "Acccuracy  : 0.478\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.690\n",
      "Acccuracy  : 0.547\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.710\n",
      "Acccuracy  : 0.478\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.690\n",
      "Acccuracy  : 0.547\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.710\n",
      "Acccuracy  : 0.478\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.690\n",
      "Acccuracy  : 0.547\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.710\n",
      "Acccuracy  : 0.478\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.690\n",
      "Acccuracy  : 0.547\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.710\n",
      "Acccuracy  : 0.478\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.690\n",
      "Acccuracy  : 0.547\n",
      "F1 Score: 0.000\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V2)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V2):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_2_v2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_2_v2, loss_func, dataloader_val_V2)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_2_v2, loss_func, dataloader_train_V2)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc64c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### EVALUATING MODEL 2 on TEST SET of dataset 1 ####\n",
      "Metrics on Test Set:\n",
      "Loss : 1.262\n",
      "Acccuracy  : 0.655\n",
      "F1 Score: 0.503\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 2 on TEST SET of dataset 2 ####\n",
      "Metrics on Test Set:\n",
      "Loss : 0.699\n",
      "Acccuracy  : 0.516\n",
      "F1 Score: 0.000\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"#### EVALUATING MODEL 2 on TEST SET of dataset 1 ####\")\n",
    "print('Metrics on Test Set:')\n",
    "calc_loss_acc(model_2, loss_func, dataloader_test_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#### EVALUATING MODEL 2 on TEST SET of dataset 2 ####\")\n",
    "print('Metrics on Test Set:')\n",
    "calc_loss_acc(model_2_v2, loss_func, dataloader_test_V2)\n",
    "print(\"----------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9fc31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
