{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facd63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score   \n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b9b02",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe5c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['mapping', 'test_labels', 'test_text', 'train_labels', 'train_text', 'val_labels', 'val_text']\n",
    "f1 = lambda file_name: f'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/{file_name}.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec56bc1",
   "metadata": {},
   "source": [
    "Below, we create all the variables containing the data. Also a variable called 'all_inputs' is created (after the preprocessing), which stores all the input data together. We will use this for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da24bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv(f1('mapping'), sep='\\t', names=['mapping', 'sentiment'])\n",
    "test_labels = pd.read_csv(f1('test_labels'), sep='\\t', names=['sentiment'])\n",
    "train_labels = pd.read_csv(f1('train_labels'), sep='\\t', names=['sentiment'])\n",
    "val_labels = pd.read_csv(f1('val_labels'), sep='\\t', names=['sentiment'])\n",
    "test_text = pd.read_csv(f1('test_text'), sep='\\t', names = ['input'])\n",
    "train_text = pd.read_csv(f1('train_text'), sep='\\t', names = ['input'])\n",
    "val_text = pd.read_csv(f1('val_text'), sep='\\t', names = ['input'])\n",
    "\n",
    "all_input = pd.concat([train_text, val_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253ff72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>â€œWorry is a down payment on a problem you may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No but that's so cute. Atsu was probably shy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooneys fucking untouchable isn't he? Been fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's pretty depressing when u hit pan on ur fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>I get discouraged because I try for 5 fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>The @user are in contention and hosting @user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>@user @user @user @user @user as a fellow UP g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>You have a #problem? Yes! Can you do #somethin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>@user @user i will fight this guy! Don't insul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3257 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     â€œWorry is a down payment on a problem you may ...\n",
       "1     My roommate: it's okay that we can't spell bec...\n",
       "2     No but that's so cute. Atsu was probably shy a...\n",
       "3     Rooneys fucking untouchable isn't he? Been fuc...\n",
       "4     it's pretty depressing when u hit pan on ur fa...\n",
       "...                                                 ...\n",
       "3252  I get discouraged because I try for 5 fucking ...\n",
       "3253  The @user are in contention and hosting @user ...\n",
       "3254  @user @user @user @user @user as a fellow UP g...\n",
       "3255  You have a #problem? Yes! Can you do #somethin...\n",
       "3256  @user @user i will fight this guy! Don't insul...\n",
       "\n",
       "[3257 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de413a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapping</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mapping sentiment\n",
       "0        0     anger\n",
       "1        1       joy\n",
       "2        2  optimism\n",
       "3        3   sadness"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64adc39f",
   "metadata": {},
   "source": [
    "As a first step, we will use only the 2 sentiments anger and joy (0 and 1). And then, as asked in the task description, we will exchange one of the sentiments. Hence, we will use joy and sadness in a second step (1 and 3). We will need this in later tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf6faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_joy_test_idx = test_labels['sentiment'].isin([0, 1])\n",
    "anger_joy_train_idx = train_labels['sentiment'].isin([0, 1])\n",
    "anger_joy_val_idx = val_labels['sentiment'].isin([0, 1])\n",
    "\n",
    "sadness_joy_test_idx = test_labels['sentiment'].isin([3, 1])\n",
    "sadness_joy_train_idx = train_labels['sentiment'].isin([3, 1])\n",
    "sadness_joy_val_idx = val_labels['sentiment'].isin([3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59455bcc",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Before we actually split the datasets, we will apply our preprocessing pipeline as it equally affects all the inputs, independently of the sentiment.\n",
    "\n",
    "Here, we apply following preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c968945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Deppression is real. Partners w/ #depressed p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user Interesting choice of words... Are you c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My visit to hospital for care triggered #traum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user Welcome to #MPSVT! We are delighted to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What makes you feel #joyful?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>I need a sparkling bodysuit . No occasion. Jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>@user I've finished reading it; simply mind-bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>shaft abrasions from panties merely shifted to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>All this fake outrage. Y'all need to stop ðŸ¤£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>Would be ever so grateful if you could record ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1421 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     #Deppression is real. Partners w/ #depressed p...\n",
       "1     @user Interesting choice of words... Are you c...\n",
       "2     My visit to hospital for care triggered #traum...\n",
       "3     @user Welcome to #MPSVT! We are delighted to h...\n",
       "4                         What makes you feel #joyful? \n",
       "...                                                 ...\n",
       "1416  I need a sparkling bodysuit . No occasion. Jus...\n",
       "1417  @user I've finished reading it; simply mind-bl...\n",
       "1418  shaft abrasions from panties merely shifted to...\n",
       "1419       All this fake outrage. Y'all need to stop ðŸ¤£ \n",
       "1420  Would be ever so grateful if you could record ...\n",
       "\n",
       "[1421 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d7499",
   "metadata": {},
   "source": [
    "The preprocessing steps are the same that we did in previous tasks. Hence, I will not go into detail about the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.word_to_ix = {}\n",
    "        self.ix_to_word = {}\n",
    "        self.context_dataset = []\n",
    "        self.vocab_size = None\n",
    "        \n",
    "    def convert_lowercase(self, x):\n",
    "        x = x.lower()\n",
    "        return x\n",
    "        \n",
    "    def remove_emoji(self, x):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', x)\n",
    "        \n",
    "    exclude = string.punctuation\n",
    "    \n",
    "    def remove_punc(self, x):\n",
    "        exclude = string.punctuation\n",
    "        return x.translate(str.maketrans('', '', exclude))\n",
    "    \n",
    "    def remove_special_chars(self, x):\n",
    "        x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "        return x\n",
    "\n",
    "    def remove_one_letter_words(self, x):\n",
    "        x = re.sub(r'(?:^| )\\w(?:$| )', ' ', x).strip()\n",
    "        return x\n",
    "    \n",
    "    # default is to apply all these preprocessing steps\n",
    "    def apply_preprocessing(self,\n",
    "                            lowercase=True,\n",
    "                            remove_emoji=True,\n",
    "                            remove_punc=True,\n",
    "                            remove_special_chars=True,\n",
    "                            remove_one_letter_words=True):\n",
    "        if lowercase:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.convert_lowercase)\n",
    "        if remove_emoji:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_emoji)\n",
    "        if remove_punc:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_punc)\n",
    "        if remove_special_chars:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_special_chars)\n",
    "        if remove_one_letter_words:\n",
    "            self.full_dataset['input'] = self.full_dataset['input'].apply(self.remove_one_letter_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbf947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = Preprocessing(test_text)\n",
    "cl1.apply_preprocessing()\n",
    "test_text_preprocessed = cl1.full_dataset\n",
    "\n",
    "cl2 = Preprocessing(val_text)\n",
    "cl2.apply_preprocessing()\n",
    "val_text_preprocessed = cl2.full_dataset\n",
    "\n",
    "cl3 = Preprocessing(train_text)\n",
    "cl3.apply_preprocessing()\n",
    "train_text_preprocessed = cl3.full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b56a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_preprocessed = pd.DataFrame(test_text['input'].apply(custom_tokenize))\n",
    "train_text_preprocessed = pd.DataFrame(train_text['input'].apply(custom_tokenize))\n",
    "val_text_preprocessed = pd.DataFrame(val_text['input'].apply(custom_tokenize))\n",
    "all_input_preprocessed = pd.concat([train_text_preprocessed, val_text_preprocessed, test_text_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d292665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[worry, is, down, payment, on, problem, you, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[my, roommate, its, okay, that, we, cant, spel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[no, but, thats, so, cute, atsu, was, probably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rooneys, fucking, untouchable, isnt, he, been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[its, pretty, depressing, when, hit, pan, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>[need, sparkling, bodysuit, no, occasion, just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>[user, ive, finished, reading, it, simply, min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>[shaft, abrasions, from, panties, merely, shif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>[all, this, fake, outrage, yall, need, to, stop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>[would, be, ever, so, grateful, if, you, could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5052 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input\n",
       "0     [worry, is, down, payment, on, problem, you, m...\n",
       "1     [my, roommate, its, okay, that, we, cant, spel...\n",
       "2     [no, but, thats, so, cute, atsu, was, probably...\n",
       "3     [rooneys, fucking, untouchable, isnt, he, been...\n",
       "4     [its, pretty, depressing, when, hit, pan, on, ...\n",
       "...                                                 ...\n",
       "1416  [need, sparkling, bodysuit, no, occasion, just...\n",
       "1417  [user, ive, finished, reading, it, simply, min...\n",
       "1418  [shaft, abrasions, from, panties, merely, shif...\n",
       "1419   [all, this, fake, outrage, yall, need, to, stop]\n",
       "1420  [would, be, ever, so, grateful, if, you, could...\n",
       "\n",
       "[5052 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fcd19",
   "metadata": {},
   "source": [
    "Get the the length of the largest tweet, so we can create a padding for all tweets that contain fewer words. Also, let us define the needed funtion to create the padding. This is done as pytorch needs the inputs to be of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8eed954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_tweet = max(all_input_preprocessed['input'].str.len())\n",
    "max_len_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9427bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding(dataset, max_len):\n",
    "    \"\"\"\n",
    "    Creates a padding on the whole dataset such that each datapoint is of same length.\n",
    "    The length is given as input by max_len.\n",
    "    \"\"\"\n",
    "    for idx, row in enumerate(dataset['input']):\n",
    "        if len(row) < max_len:\n",
    "            tmp = len(row)\n",
    "            pad1 = (max_len - tmp) // 2\n",
    "            row = ['PADDING'] * pad1 + row\n",
    "            row = row + ['PADDING'] * (30 - tmp - pad1)\n",
    "            dataset['input'].iloc[idx] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "010f3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_padding(test_text_preprocessed, max_len_tweet)\n",
    "create_padding(train_text_preprocessed, max_len_tweet)\n",
    "create_padding(val_text_preprocessed, max_len_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde4367",
   "metadata": {},
   "source": [
    "Let's see an example of how a padded input datapoint looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4160014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'am',\n",
       " 'revolting',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_preprocessed.loc[5, 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bada739",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_preprocessed = pd.concat([train_text_preprocessed, val_text_preprocessed, test_text_preprocessed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea16dc5",
   "metadata": {},
   "source": [
    "Now, our data is in a somewhat nice format to work with. Every word is separated by a comma. As a next step, we want to create the word_to_ix and ix_to_word dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d666065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "cur_idx = 0\n",
    "for l in all_input_preprocessed['input'].tolist():\n",
    "    for el in l:\n",
    "        if el not in word_to_ix:\n",
    "            word_to_ix[el] = cur_idx\n",
    "            cur_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5160b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_word = dict([(v, k) for k, v in word_to_ix.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6f89a",
   "metadata": {},
   "source": [
    "The following function creates a numeric vector out of every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eadb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_vec(x, word_to_ix):\n",
    "    \"\"\"\n",
    "    Takes a tokenized tweet as input and returns a numeric vector.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for el in x:\n",
    "        res.append(word_to_ix[el])\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b69e2e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,   15, 1482,   73, 4488,  170, 2262, 9364, 9365,   68,\n",
       "        9366, 9367, 1262,   84, 9080,  147,   15, 2179,  778,  182,  229, 1438,\n",
       "        9368,  147,  590,    0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_num_vec(test_text_preprocessed['input'].tolist()[2], word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7e23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    The data is passed as lists\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, word_to_ix):\n",
    "        self.x = x['input'].tolist()\n",
    "        self.y = y['sentiment'].tolist()\n",
    "        self.word_to_ix = word_to_ix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = create_num_vec(self.x[idx], self.word_to_ix)\n",
    "        y = torch.tensor(self.y[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0bc7a",
   "metadata": {},
   "source": [
    "Let's see what shapes our data has, for example, so we can better design our model pipeline.\n",
    "As expected, the input data is of shape (batch_size, max_len_tweet), and the labels are of shape (batch_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c172c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=5, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(64, 16, kernel_size=3, padding=\"same\")\n",
    "\n",
    "        self.linear = nn.Linear(16, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # pool the 16 dimension to 1\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7c3d9",
   "metadata": {},
   "source": [
    "Before coming to the actual tasks, we will write some more functions which may be helpful. One of these functions is a function that calculates the loss and the accuracy (i.e. on the validation set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a77511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_acc(model, loss_fn, data_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in data_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Acccuracy  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "        print(\"F1 Score: {:.3f}\".format(f1_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b416e",
   "metadata": {},
   "source": [
    "# Task 1 -- Simple training on both datasets\n",
    "### Let's train and train the data! As a first step on the anger - joy dataset\n",
    "As a first step we need to create the datasets. Also, a small step we undertake is to change the 'sadness' sentiment, which is encoded as 3 to 0, as the cross entropy loss only accepts 0 and 1 as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25126dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_V1 = test_labels[anger_joy_test_idx]\n",
    "test_text_V1 = test_text_preprocessed[anger_joy_test_idx]\n",
    "\n",
    "val_labels_V1 = val_labels[anger_joy_val_idx]\n",
    "val_text_V1 = val_text_preprocessed[anger_joy_val_idx]\n",
    "\n",
    "train_labels_V1 = train_labels[anger_joy_train_idx]\n",
    "train_text_V1 = train_text_preprocessed[anger_joy_train_idx]\n",
    "\n",
    "\n",
    "# the same for V2\n",
    "test_labels_V2 = test_labels[sadness_joy_test_idx].replace(3, 0)\n",
    "test_text_V2 = test_text_preprocessed[sadness_joy_test_idx]\n",
    "\n",
    "val_labels_V2 = val_labels[sadness_joy_val_idx].replace(3, 0)\n",
    "val_text_V2 = val_text_preprocessed[sadness_joy_val_idx]\n",
    "\n",
    "train_labels_V2 = train_labels[sadness_joy_train_idx].replace(3, 0)\n",
    "train_text_V2 = train_text_preprocessed[sadness_joy_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f44e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111df3df",
   "metadata": {},
   "source": [
    "Let's define our datasets. We call the datasets containing the labels anger and joy 'V1' and the other datasets 'V2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61df3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_V1 = Model_Dataset(train_text_V1, train_labels_V1, word_to_ix)\n",
    "dataloader_train_V1 = DataLoader(dataloader_train_V1, batch_size=32, shuffle=True)\n",
    "\n",
    "dataloader_train_V2 = Model_Dataset(train_text_V2, train_labels_V2, word_to_ix)\n",
    "dataloader_train_V2 = DataLoader(dataloader_train_V2, batch_size=32, shuffle=True)\n",
    "\n",
    "dataloader_val_V1 = Model_Dataset(val_text_V1, val_labels_V1, word_to_ix)\n",
    "dataloader_val_V1 = DataLoader(dataloader_val_V1)\n",
    "\n",
    "dataloader_val_V2 = Model_Dataset(val_text_V2, val_labels_V2, word_to_ix)\n",
    "dataloader_val_V2 = DataLoader(dataloader_val_V2)\n",
    "\n",
    "dataloader_test_V1 = Model_Dataset(test_text_V1, test_labels_V1, word_to_ix)\n",
    "dataloader_test_V1 = DataLoader(dataloader_test_V1)\n",
    "\n",
    "dataloader_test_V2 = Model_Dataset(test_text_V2, test_labels_V2, word_to_ix)\n",
    "dataloader_test_V2 = DataLoader(dataloader_test_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2c124",
   "metadata": {},
   "source": [
    "### Training the model on the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31b4e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ed06e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_v1 = my_model(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_v1.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa18273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], batch: [0/66, loss: 0.7593]\n",
      "Epoch [0/10], batch: [1/66, loss: 0.6991]\n",
      "Epoch [0/10], batch: [2/66, loss: 0.6632]\n",
      "Epoch [0/10], batch: [3/66, loss: 0.6971]\n",
      "Epoch [0/10], batch: [4/66, loss: 0.5347]\n",
      "Epoch [0/10], batch: [5/66, loss: 0.7926]\n",
      "Epoch [0/10], batch: [6/66, loss: 0.7010]\n",
      "Epoch [0/10], batch: [7/66, loss: 0.6243]\n",
      "Epoch [0/10], batch: [8/66, loss: 0.7126]\n",
      "Epoch [0/10], batch: [9/66, loss: 0.6264]\n",
      "Epoch [0/10], batch: [10/66, loss: 0.6217]\n",
      "Epoch [0/10], batch: [11/66, loss: 0.6548]\n",
      "Epoch [0/10], batch: [12/66, loss: 0.6829]\n",
      "Epoch [0/10], batch: [13/66, loss: 0.5683]\n",
      "Epoch [0/10], batch: [14/66, loss: 0.7107]\n",
      "Epoch [0/10], batch: [15/66, loss: 0.6240]\n",
      "Epoch [0/10], batch: [16/66, loss: 0.7511]\n",
      "Epoch [0/10], batch: [17/66, loss: 0.6844]\n",
      "Epoch [0/10], batch: [18/66, loss: 0.6545]\n",
      "Epoch [0/10], batch: [19/66, loss: 0.6818]\n",
      "Epoch [0/10], batch: [20/66, loss: 0.6469]\n",
      "Epoch [0/10], batch: [21/66, loss: 0.6898]\n",
      "Epoch [0/10], batch: [22/66, loss: 0.6748]\n",
      "Epoch [0/10], batch: [23/66, loss: 0.6508]\n",
      "Epoch [0/10], batch: [24/66, loss: 0.5613]\n",
      "Epoch [0/10], batch: [25/66, loss: 0.6094]\n",
      "Epoch [0/10], batch: [26/66, loss: 0.6670]\n",
      "Epoch [0/10], batch: [27/66, loss: 0.5120]\n",
      "Epoch [0/10], batch: [28/66, loss: 0.7734]\n",
      "Epoch [0/10], batch: [29/66, loss: 0.5790]\n",
      "Epoch [0/10], batch: [30/66, loss: 0.6584]\n",
      "Epoch [0/10], batch: [31/66, loss: 0.5537]\n",
      "Epoch [0/10], batch: [32/66, loss: 0.6071]\n",
      "Epoch [0/10], batch: [33/66, loss: 0.7557]\n",
      "Epoch [0/10], batch: [34/66, loss: 0.6007]\n",
      "Epoch [0/10], batch: [35/66, loss: 0.6574]\n",
      "Epoch [0/10], batch: [36/66, loss: 0.6203]\n",
      "Epoch [0/10], batch: [37/66, loss: 0.6913]\n",
      "Epoch [0/10], batch: [38/66, loss: 0.6491]\n",
      "Epoch [0/10], batch: [39/66, loss: 0.7085]\n",
      "Epoch [0/10], batch: [40/66, loss: 0.6279]\n",
      "Epoch [0/10], batch: [41/66, loss: 0.6252]\n",
      "Epoch [0/10], batch: [42/66, loss: 0.6095]\n",
      "Epoch [0/10], batch: [43/66, loss: 0.4997]\n",
      "Epoch [0/10], batch: [44/66, loss: 0.6105]\n",
      "Epoch [0/10], batch: [45/66, loss: 0.7978]\n",
      "Epoch [0/10], batch: [46/66, loss: 0.6781]\n",
      "Epoch [0/10], batch: [47/66, loss: 0.6415]\n",
      "Epoch [0/10], batch: [48/66, loss: 0.6033]\n",
      "Epoch [0/10], batch: [49/66, loss: 0.6138]\n",
      "Epoch [0/10], batch: [50/66, loss: 0.6458]\n",
      "Epoch [0/10], batch: [51/66, loss: 0.5900]\n",
      "Epoch [0/10], batch: [52/66, loss: 0.6471]\n",
      "Epoch [0/10], batch: [53/66, loss: 0.6200]\n",
      "Epoch [0/10], batch: [54/66, loss: 0.6371]\n",
      "Epoch [0/10], batch: [55/66, loss: 0.6974]\n",
      "Epoch [0/10], batch: [56/66, loss: 0.6012]\n",
      "Epoch [0/10], batch: [57/66, loss: 0.5966]\n",
      "Epoch [0/10], batch: [58/66, loss: 0.5901]\n",
      "Epoch [0/10], batch: [59/66, loss: 0.6967]\n",
      "Epoch [0/10], batch: [60/66, loss: 0.6953]\n",
      "Epoch [0/10], batch: [61/66, loss: 0.5602]\n",
      "Epoch [0/10], batch: [62/66, loss: 0.5195]\n",
      "Epoch [0/10], batch: [63/66, loss: 0.6547]\n",
      "Epoch [0/10], batch: [64/66, loss: 0.5703]\n",
      "Epoch [0/10], batch: [65/66, loss: 0.5381]\n",
      "Epoch [1/10], batch: [0/66, loss: 0.6802]\n",
      "Epoch [1/10], batch: [1/66, loss: 0.6585]\n",
      "Epoch [1/10], batch: [2/66, loss: 0.5872]\n",
      "Epoch [1/10], batch: [3/66, loss: 0.5646]\n",
      "Epoch [1/10], batch: [4/66, loss: 0.6084]\n",
      "Epoch [1/10], batch: [5/66, loss: 0.5974]\n",
      "Epoch [1/10], batch: [6/66, loss: 0.5813]\n",
      "Epoch [1/10], batch: [7/66, loss: 0.6333]\n",
      "Epoch [1/10], batch: [8/66, loss: 0.5178]\n",
      "Epoch [1/10], batch: [9/66, loss: 0.5507]\n",
      "Epoch [1/10], batch: [10/66, loss: 0.5750]\n",
      "Epoch [1/10], batch: [11/66, loss: 0.5987]\n",
      "Epoch [1/10], batch: [12/66, loss: 0.4819]\n",
      "Epoch [1/10], batch: [13/66, loss: 0.6040]\n",
      "Epoch [1/10], batch: [14/66, loss: 0.4874]\n",
      "Epoch [1/10], batch: [15/66, loss: 0.7320]\n",
      "Epoch [1/10], batch: [16/66, loss: 0.5278]\n",
      "Epoch [1/10], batch: [17/66, loss: 0.6858]\n",
      "Epoch [1/10], batch: [18/66, loss: 0.5617]\n",
      "Epoch [1/10], batch: [19/66, loss: 0.5506]\n",
      "Epoch [1/10], batch: [20/66, loss: 0.5596]\n",
      "Epoch [1/10], batch: [21/66, loss: 0.5551]\n",
      "Epoch [1/10], batch: [22/66, loss: 0.5531]\n",
      "Epoch [1/10], batch: [23/66, loss: 0.6931]\n",
      "Epoch [1/10], batch: [24/66, loss: 0.4713]\n",
      "Epoch [1/10], batch: [25/66, loss: 0.5476]\n",
      "Epoch [1/10], batch: [26/66, loss: 0.6330]\n",
      "Epoch [1/10], batch: [27/66, loss: 0.5552]\n",
      "Epoch [1/10], batch: [28/66, loss: 0.5397]\n",
      "Epoch [1/10], batch: [29/66, loss: 0.6728]\n",
      "Epoch [1/10], batch: [30/66, loss: 0.5468]\n",
      "Epoch [1/10], batch: [31/66, loss: 0.5374]\n",
      "Epoch [1/10], batch: [32/66, loss: 0.5251]\n",
      "Epoch [1/10], batch: [33/66, loss: 0.6655]\n",
      "Epoch [1/10], batch: [34/66, loss: 0.8224]\n",
      "Epoch [1/10], batch: [35/66, loss: 0.6214]\n",
      "Epoch [1/10], batch: [36/66, loss: 0.6772]\n",
      "Epoch [1/10], batch: [37/66, loss: 0.6919]\n",
      "Epoch [1/10], batch: [38/66, loss: 0.5758]\n",
      "Epoch [1/10], batch: [39/66, loss: 0.5667]\n",
      "Epoch [1/10], batch: [40/66, loss: 0.5349]\n",
      "Epoch [1/10], batch: [41/66, loss: 0.4962]\n",
      "Epoch [1/10], batch: [42/66, loss: 0.6248]\n",
      "Epoch [1/10], batch: [43/66, loss: 0.5510]\n",
      "Epoch [1/10], batch: [44/66, loss: 0.5191]\n",
      "Epoch [1/10], batch: [45/66, loss: 0.6489]\n",
      "Epoch [1/10], batch: [46/66, loss: 0.4878]\n",
      "Epoch [1/10], batch: [47/66, loss: 0.4892]\n",
      "Epoch [1/10], batch: [48/66, loss: 0.5053]\n",
      "Epoch [1/10], batch: [49/66, loss: 0.5020]\n",
      "Epoch [1/10], batch: [50/66, loss: 0.6731]\n",
      "Epoch [1/10], batch: [51/66, loss: 0.4952]\n",
      "Epoch [1/10], batch: [52/66, loss: 0.5156]\n",
      "Epoch [1/10], batch: [53/66, loss: 0.5262]\n",
      "Epoch [1/10], batch: [54/66, loss: 0.6127]\n",
      "Epoch [1/10], batch: [55/66, loss: 0.4941]\n",
      "Epoch [1/10], batch: [56/66, loss: 0.5036]\n",
      "Epoch [1/10], batch: [57/66, loss: 0.5858]\n",
      "Epoch [1/10], batch: [58/66, loss: 0.5533]\n",
      "Epoch [1/10], batch: [59/66, loss: 0.5814]\n",
      "Epoch [1/10], batch: [60/66, loss: 0.5748]\n",
      "Epoch [1/10], batch: [61/66, loss: 0.6427]\n",
      "Epoch [1/10], batch: [62/66, loss: 0.6283]\n",
      "Epoch [1/10], batch: [63/66, loss: 0.4827]\n",
      "Epoch [1/10], batch: [64/66, loss: 0.4653]\n",
      "Epoch [1/10], batch: [65/66, loss: 0.4982]\n",
      "Epoch [2/10], batch: [0/66, loss: 0.7291]\n",
      "Epoch [2/10], batch: [1/66, loss: 0.5978]\n",
      "Epoch [2/10], batch: [2/66, loss: 0.4908]\n",
      "Epoch [2/10], batch: [3/66, loss: 0.4024]\n",
      "Epoch [2/10], batch: [4/66, loss: 0.4953]\n",
      "Epoch [2/10], batch: [5/66, loss: 0.6059]\n",
      "Epoch [2/10], batch: [6/66, loss: 0.4237]\n",
      "Epoch [2/10], batch: [7/66, loss: 0.4427]\n",
      "Epoch [2/10], batch: [8/66, loss: 0.4609]\n",
      "Epoch [2/10], batch: [9/66, loss: 0.4659]\n",
      "Epoch [2/10], batch: [10/66, loss: 0.4178]\n",
      "Epoch [2/10], batch: [11/66, loss: 0.4369]\n",
      "Epoch [2/10], batch: [12/66, loss: 0.4286]\n",
      "Epoch [2/10], batch: [13/66, loss: 0.4453]\n",
      "Epoch [2/10], batch: [14/66, loss: 0.3704]\n",
      "Epoch [2/10], batch: [15/66, loss: 0.4098]\n",
      "Epoch [2/10], batch: [16/66, loss: 0.4089]\n",
      "Epoch [2/10], batch: [17/66, loss: 0.3418]\n",
      "Epoch [2/10], batch: [18/66, loss: 0.5331]\n",
      "Epoch [2/10], batch: [19/66, loss: 0.2661]\n",
      "Epoch [2/10], batch: [20/66, loss: 0.3104]\n",
      "Epoch [2/10], batch: [21/66, loss: 0.3590]\n",
      "Epoch [2/10], batch: [22/66, loss: 0.4504]\n",
      "Epoch [2/10], batch: [23/66, loss: 0.4634]\n",
      "Epoch [2/10], batch: [24/66, loss: 0.4471]\n",
      "Epoch [2/10], batch: [25/66, loss: 0.3732]\n",
      "Epoch [2/10], batch: [26/66, loss: 0.4494]\n",
      "Epoch [2/10], batch: [27/66, loss: 0.4729]\n",
      "Epoch [2/10], batch: [28/66, loss: 0.2577]\n",
      "Epoch [2/10], batch: [29/66, loss: 0.5015]\n",
      "Epoch [2/10], batch: [30/66, loss: 0.4098]\n",
      "Epoch [2/10], batch: [31/66, loss: 0.3181]\n",
      "Epoch [2/10], batch: [32/66, loss: 0.4838]\n",
      "Epoch [2/10], batch: [33/66, loss: 0.3708]\n",
      "Epoch [2/10], batch: [34/66, loss: 0.5208]\n",
      "Epoch [2/10], batch: [35/66, loss: 0.4158]\n",
      "Epoch [2/10], batch: [36/66, loss: 0.3678]\n",
      "Epoch [2/10], batch: [37/66, loss: 0.7050]\n",
      "Epoch [2/10], batch: [38/66, loss: 0.3705]\n",
      "Epoch [2/10], batch: [39/66, loss: 0.3195]\n",
      "Epoch [2/10], batch: [40/66, loss: 0.3105]\n",
      "Epoch [2/10], batch: [41/66, loss: 0.3530]\n",
      "Epoch [2/10], batch: [42/66, loss: 0.3973]\n",
      "Epoch [2/10], batch: [43/66, loss: 0.4314]\n",
      "Epoch [2/10], batch: [44/66, loss: 0.2682]\n",
      "Epoch [2/10], batch: [45/66, loss: 0.4829]\n",
      "Epoch [2/10], batch: [46/66, loss: 0.5068]\n",
      "Epoch [2/10], batch: [47/66, loss: 0.3283]\n",
      "Epoch [2/10], batch: [48/66, loss: 0.3104]\n",
      "Epoch [2/10], batch: [49/66, loss: 0.2591]\n",
      "Epoch [2/10], batch: [50/66, loss: 0.3126]\n",
      "Epoch [2/10], batch: [51/66, loss: 0.2641]\n",
      "Epoch [2/10], batch: [52/66, loss: 0.2648]\n",
      "Epoch [2/10], batch: [53/66, loss: 0.3409]\n",
      "Epoch [2/10], batch: [54/66, loss: 0.3606]\n",
      "Epoch [2/10], batch: [55/66, loss: 0.3033]\n",
      "Epoch [2/10], batch: [56/66, loss: 0.4184]\n",
      "Epoch [2/10], batch: [57/66, loss: 0.2548]\n",
      "Epoch [2/10], batch: [58/66, loss: 0.2244]\n",
      "Epoch [2/10], batch: [59/66, loss: 0.3733]\n",
      "Epoch [2/10], batch: [60/66, loss: 0.2184]\n",
      "Epoch [2/10], batch: [61/66, loss: 0.3113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], batch: [62/66, loss: 0.3082]\n",
      "Epoch [2/10], batch: [63/66, loss: 0.3181]\n",
      "Epoch [2/10], batch: [64/66, loss: 0.1749]\n",
      "Epoch [2/10], batch: [65/66, loss: 0.5549]\n",
      "Epoch [3/10], batch: [0/66, loss: 0.1199]\n",
      "Epoch [3/10], batch: [1/66, loss: 0.1456]\n",
      "Epoch [3/10], batch: [2/66, loss: 0.2826]\n",
      "Epoch [3/10], batch: [3/66, loss: 0.2298]\n",
      "Epoch [3/10], batch: [4/66, loss: 0.0646]\n",
      "Epoch [3/10], batch: [5/66, loss: 0.3216]\n",
      "Epoch [3/10], batch: [6/66, loss: 0.1320]\n",
      "Epoch [3/10], batch: [7/66, loss: 0.1190]\n",
      "Epoch [3/10], batch: [8/66, loss: 0.1402]\n",
      "Epoch [3/10], batch: [9/66, loss: 0.1575]\n",
      "Epoch [3/10], batch: [10/66, loss: 0.1541]\n",
      "Epoch [3/10], batch: [11/66, loss: 0.1555]\n",
      "Epoch [3/10], batch: [12/66, loss: 0.1235]\n",
      "Epoch [3/10], batch: [13/66, loss: 0.2215]\n",
      "Epoch [3/10], batch: [14/66, loss: 0.1303]\n",
      "Epoch [3/10], batch: [15/66, loss: 0.2087]\n",
      "Epoch [3/10], batch: [16/66, loss: 0.1555]\n",
      "Epoch [3/10], batch: [17/66, loss: 0.0966]\n",
      "Epoch [3/10], batch: [18/66, loss: 0.1186]\n",
      "Epoch [3/10], batch: [19/66, loss: 0.0355]\n",
      "Epoch [3/10], batch: [20/66, loss: 0.0379]\n",
      "Epoch [3/10], batch: [21/66, loss: 0.1510]\n",
      "Epoch [3/10], batch: [22/66, loss: 0.1907]\n",
      "Epoch [3/10], batch: [23/66, loss: 0.0850]\n",
      "Epoch [3/10], batch: [24/66, loss: 0.2859]\n",
      "Epoch [3/10], batch: [25/66, loss: 0.0759]\n",
      "Epoch [3/10], batch: [26/66, loss: 0.1404]\n",
      "Epoch [3/10], batch: [27/66, loss: 0.2324]\n",
      "Epoch [3/10], batch: [28/66, loss: 0.1726]\n",
      "Epoch [3/10], batch: [29/66, loss: 0.1938]\n",
      "Epoch [3/10], batch: [30/66, loss: 0.1001]\n",
      "Epoch [3/10], batch: [31/66, loss: 0.2965]\n",
      "Epoch [3/10], batch: [32/66, loss: 0.1600]\n",
      "Epoch [3/10], batch: [33/66, loss: 0.1417]\n",
      "Epoch [3/10], batch: [34/66, loss: 0.0768]\n",
      "Epoch [3/10], batch: [35/66, loss: 0.2707]\n",
      "Epoch [3/10], batch: [36/66, loss: 0.1792]\n",
      "Epoch [3/10], batch: [37/66, loss: 0.1337]\n",
      "Epoch [3/10], batch: [38/66, loss: 0.4440]\n",
      "Epoch [3/10], batch: [39/66, loss: 0.2581]\n",
      "Epoch [3/10], batch: [40/66, loss: 0.2865]\n",
      "Epoch [3/10], batch: [41/66, loss: 1.5954]\n",
      "Epoch [3/10], batch: [42/66, loss: 0.7666]\n",
      "Epoch [3/10], batch: [43/66, loss: 0.1473]\n",
      "Epoch [3/10], batch: [44/66, loss: 1.0083]\n",
      "Epoch [3/10], batch: [45/66, loss: 0.1661]\n",
      "Epoch [3/10], batch: [46/66, loss: 0.0882]\n",
      "Epoch [3/10], batch: [47/66, loss: 0.3572]\n",
      "Epoch [3/10], batch: [48/66, loss: 0.3758]\n",
      "Epoch [3/10], batch: [49/66, loss: 0.1817]\n",
      "Epoch [3/10], batch: [50/66, loss: 0.1235]\n",
      "Epoch [3/10], batch: [51/66, loss: 0.2014]\n",
      "Epoch [3/10], batch: [52/66, loss: 0.3979]\n",
      "Epoch [3/10], batch: [53/66, loss: 0.3884]\n",
      "Epoch [3/10], batch: [54/66, loss: 0.1685]\n",
      "Epoch [3/10], batch: [55/66, loss: 0.1731]\n",
      "Epoch [3/10], batch: [56/66, loss: 0.2279]\n",
      "Epoch [3/10], batch: [57/66, loss: 0.2596]\n",
      "Epoch [3/10], batch: [58/66, loss: 0.3229]\n",
      "Epoch [3/10], batch: [59/66, loss: 0.1327]\n",
      "Epoch [3/10], batch: [60/66, loss: 0.1898]\n",
      "Epoch [3/10], batch: [61/66, loss: 0.2070]\n",
      "Epoch [3/10], batch: [62/66, loss: 0.2105]\n",
      "Epoch [3/10], batch: [63/66, loss: 0.1566]\n",
      "Epoch [3/10], batch: [64/66, loss: 0.1581]\n",
      "Epoch [3/10], batch: [65/66, loss: 0.1389]\n",
      "Epoch [4/10], batch: [0/66, loss: 0.0644]\n",
      "Epoch [4/10], batch: [1/66, loss: 0.0574]\n",
      "Epoch [4/10], batch: [2/66, loss: 0.1589]\n",
      "Epoch [4/10], batch: [3/66, loss: 0.1540]\n",
      "Epoch [4/10], batch: [4/66, loss: 0.0867]\n",
      "Epoch [4/10], batch: [5/66, loss: 0.0945]\n",
      "Epoch [4/10], batch: [6/66, loss: 0.1306]\n",
      "Epoch [4/10], batch: [7/66, loss: 0.0504]\n",
      "Epoch [4/10], batch: [8/66, loss: 0.0897]\n",
      "Epoch [4/10], batch: [9/66, loss: 0.0988]\n",
      "Epoch [4/10], batch: [10/66, loss: 0.0905]\n",
      "Epoch [4/10], batch: [11/66, loss: 0.0761]\n",
      "Epoch [4/10], batch: [12/66, loss: 0.2598]\n",
      "Epoch [4/10], batch: [13/66, loss: 0.1307]\n",
      "Epoch [4/10], batch: [14/66, loss: 0.0975]\n",
      "Epoch [4/10], batch: [15/66, loss: 0.0834]\n",
      "Epoch [4/10], batch: [16/66, loss: 0.0898]\n",
      "Epoch [4/10], batch: [17/66, loss: 0.0990]\n",
      "Epoch [4/10], batch: [18/66, loss: 0.0713]\n",
      "Epoch [4/10], batch: [19/66, loss: 0.0582]\n",
      "Epoch [4/10], batch: [20/66, loss: 0.1001]\n",
      "Epoch [4/10], batch: [21/66, loss: 0.0492]\n",
      "Epoch [4/10], batch: [22/66, loss: 0.1078]\n",
      "Epoch [4/10], batch: [23/66, loss: 0.0781]\n",
      "Epoch [4/10], batch: [24/66, loss: 0.0675]\n",
      "Epoch [4/10], batch: [25/66, loss: 0.0541]\n",
      "Epoch [4/10], batch: [26/66, loss: 0.1412]\n",
      "Epoch [4/10], batch: [27/66, loss: 0.1020]\n",
      "Epoch [4/10], batch: [28/66, loss: 0.0564]\n",
      "Epoch [4/10], batch: [29/66, loss: 0.1559]\n",
      "Epoch [4/10], batch: [30/66, loss: 0.2170]\n",
      "Epoch [4/10], batch: [31/66, loss: 0.0501]\n",
      "Epoch [4/10], batch: [32/66, loss: 0.0350]\n",
      "Epoch [4/10], batch: [33/66, loss: 0.0770]\n",
      "Epoch [4/10], batch: [34/66, loss: 0.1884]\n",
      "Epoch [4/10], batch: [35/66, loss: 0.0675]\n",
      "Epoch [4/10], batch: [36/66, loss: 0.0126]\n",
      "Epoch [4/10], batch: [37/66, loss: 0.1010]\n",
      "Epoch [4/10], batch: [38/66, loss: 0.0414]\n",
      "Epoch [4/10], batch: [39/66, loss: 0.0294]\n",
      "Epoch [4/10], batch: [40/66, loss: 0.0336]\n",
      "Epoch [4/10], batch: [41/66, loss: 0.0599]\n",
      "Epoch [4/10], batch: [42/66, loss: 0.0490]\n",
      "Epoch [4/10], batch: [43/66, loss: 0.0546]\n",
      "Epoch [4/10], batch: [44/66, loss: 0.1335]\n",
      "Epoch [4/10], batch: [45/66, loss: 0.0304]\n",
      "Epoch [4/10], batch: [46/66, loss: 0.0860]\n",
      "Epoch [4/10], batch: [47/66, loss: 0.0825]\n",
      "Epoch [4/10], batch: [48/66, loss: 0.1436]\n",
      "Epoch [4/10], batch: [49/66, loss: 0.1277]\n",
      "Epoch [4/10], batch: [50/66, loss: 0.0183]\n",
      "Epoch [4/10], batch: [51/66, loss: 0.0593]\n",
      "Epoch [4/10], batch: [52/66, loss: 0.0842]\n",
      "Epoch [4/10], batch: [53/66, loss: 0.0234]\n",
      "Epoch [4/10], batch: [54/66, loss: 0.0143]\n",
      "Epoch [4/10], batch: [55/66, loss: 0.1419]\n",
      "Epoch [4/10], batch: [56/66, loss: 0.0655]\n",
      "Epoch [4/10], batch: [57/66, loss: 0.1513]\n",
      "Epoch [4/10], batch: [58/66, loss: 0.0904]\n",
      "Epoch [4/10], batch: [59/66, loss: 0.1386]\n",
      "Epoch [4/10], batch: [60/66, loss: 0.1057]\n",
      "Epoch [4/10], batch: [61/66, loss: 0.0308]\n",
      "Epoch [4/10], batch: [62/66, loss: 0.1092]\n",
      "Epoch [4/10], batch: [63/66, loss: 0.0594]\n",
      "Epoch [4/10], batch: [64/66, loss: 0.0148]\n",
      "Epoch [4/10], batch: [65/66, loss: 0.1608]\n",
      "Epoch [5/10], batch: [0/66, loss: 0.0726]\n",
      "Epoch [5/10], batch: [1/66, loss: 0.0393]\n",
      "Epoch [5/10], batch: [2/66, loss: 0.0798]\n",
      "Epoch [5/10], batch: [3/66, loss: 0.0133]\n",
      "Epoch [5/10], batch: [4/66, loss: 0.1082]\n",
      "Epoch [5/10], batch: [5/66, loss: 0.0337]\n",
      "Epoch [5/10], batch: [6/66, loss: 0.0128]\n",
      "Epoch [5/10], batch: [7/66, loss: 0.0712]\n",
      "Epoch [5/10], batch: [8/66, loss: 0.1581]\n",
      "Epoch [5/10], batch: [9/66, loss: 0.0606]\n",
      "Epoch [5/10], batch: [10/66, loss: 0.0063]\n",
      "Epoch [5/10], batch: [11/66, loss: 0.0209]\n",
      "Epoch [5/10], batch: [12/66, loss: 0.0117]\n",
      "Epoch [5/10], batch: [13/66, loss: 0.0186]\n",
      "Epoch [5/10], batch: [14/66, loss: 0.1448]\n",
      "Epoch [5/10], batch: [15/66, loss: 0.0401]\n",
      "Epoch [5/10], batch: [16/66, loss: 0.0371]\n",
      "Epoch [5/10], batch: [17/66, loss: 0.0576]\n",
      "Epoch [5/10], batch: [18/66, loss: 0.0347]\n",
      "Epoch [5/10], batch: [19/66, loss: 0.0460]\n",
      "Epoch [5/10], batch: [20/66, loss: 0.0501]\n",
      "Epoch [5/10], batch: [21/66, loss: 0.0294]\n",
      "Epoch [5/10], batch: [22/66, loss: 0.0347]\n",
      "Epoch [5/10], batch: [23/66, loss: 0.0068]\n",
      "Epoch [5/10], batch: [24/66, loss: 0.0057]\n",
      "Epoch [5/10], batch: [25/66, loss: 0.0105]\n",
      "Epoch [5/10], batch: [26/66, loss: 0.0167]\n",
      "Epoch [5/10], batch: [27/66, loss: 0.0676]\n",
      "Epoch [5/10], batch: [28/66, loss: 0.0155]\n",
      "Epoch [5/10], batch: [29/66, loss: 0.0332]\n",
      "Epoch [5/10], batch: [30/66, loss: 0.0645]\n",
      "Epoch [5/10], batch: [31/66, loss: 0.0671]\n",
      "Epoch [5/10], batch: [32/66, loss: 0.1568]\n",
      "Epoch [5/10], batch: [33/66, loss: 0.0111]\n",
      "Epoch [5/10], batch: [34/66, loss: 0.0188]\n",
      "Epoch [5/10], batch: [35/66, loss: 0.0343]\n",
      "Epoch [5/10], batch: [36/66, loss: 0.0138]\n",
      "Epoch [5/10], batch: [37/66, loss: 0.0532]\n",
      "Epoch [5/10], batch: [38/66, loss: 0.0362]\n",
      "Epoch [5/10], batch: [39/66, loss: 0.0215]\n",
      "Epoch [5/10], batch: [40/66, loss: 0.0140]\n",
      "Epoch [5/10], batch: [41/66, loss: 0.0271]\n",
      "Epoch [5/10], batch: [42/66, loss: 0.1349]\n",
      "Epoch [5/10], batch: [43/66, loss: 0.0211]\n",
      "Epoch [5/10], batch: [44/66, loss: 0.0349]\n",
      "Epoch [5/10], batch: [45/66, loss: 0.0093]\n",
      "Epoch [5/10], batch: [46/66, loss: 0.0262]\n",
      "Epoch [5/10], batch: [47/66, loss: 0.0650]\n",
      "Epoch [5/10], batch: [48/66, loss: 0.0468]\n",
      "Epoch [5/10], batch: [49/66, loss: 0.0195]\n",
      "Epoch [5/10], batch: [50/66, loss: 0.2372]\n",
      "Epoch [5/10], batch: [51/66, loss: 0.0604]\n",
      "Epoch [5/10], batch: [52/66, loss: 0.0167]\n",
      "Epoch [5/10], batch: [53/66, loss: 0.1412]\n",
      "Epoch [5/10], batch: [54/66, loss: 0.1224]\n",
      "Epoch [5/10], batch: [55/66, loss: 0.0171]\n",
      "Epoch [5/10], batch: [56/66, loss: 0.0629]\n",
      "Epoch [5/10], batch: [57/66, loss: 0.3351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], batch: [58/66, loss: 0.0836]\n",
      "Epoch [5/10], batch: [59/66, loss: 0.1558]\n",
      "Epoch [5/10], batch: [60/66, loss: 0.0138]\n",
      "Epoch [5/10], batch: [61/66, loss: 0.0359]\n",
      "Epoch [5/10], batch: [62/66, loss: 0.0355]\n",
      "Epoch [5/10], batch: [63/66, loss: 0.0628]\n",
      "Epoch [5/10], batch: [64/66, loss: 0.0739]\n",
      "Epoch [5/10], batch: [65/66, loss: 0.0693]\n",
      "Epoch [6/10], batch: [0/66, loss: 0.0140]\n",
      "Epoch [6/10], batch: [1/66, loss: 0.0106]\n",
      "Epoch [6/10], batch: [2/66, loss: 0.0056]\n",
      "Epoch [6/10], batch: [3/66, loss: 0.0059]\n",
      "Epoch [6/10], batch: [4/66, loss: 0.0070]\n",
      "Epoch [6/10], batch: [5/66, loss: 0.1216]\n",
      "Epoch [6/10], batch: [6/66, loss: 0.0851]\n",
      "Epoch [6/10], batch: [7/66, loss: 0.0183]\n",
      "Epoch [6/10], batch: [8/66, loss: 0.0024]\n",
      "Epoch [6/10], batch: [9/66, loss: 0.3361]\n",
      "Epoch [6/10], batch: [10/66, loss: 0.0387]\n",
      "Epoch [6/10], batch: [11/66, loss: 0.0051]\n",
      "Epoch [6/10], batch: [12/66, loss: 0.0433]\n",
      "Epoch [6/10], batch: [13/66, loss: 0.0263]\n",
      "Epoch [6/10], batch: [14/66, loss: 0.1292]\n",
      "Epoch [6/10], batch: [15/66, loss: 0.0555]\n",
      "Epoch [6/10], batch: [16/66, loss: 0.0202]\n",
      "Epoch [6/10], batch: [17/66, loss: 0.0070]\n",
      "Epoch [6/10], batch: [18/66, loss: 0.0262]\n",
      "Epoch [6/10], batch: [19/66, loss: 0.0852]\n",
      "Epoch [6/10], batch: [20/66, loss: 0.1313]\n",
      "Epoch [6/10], batch: [21/66, loss: 0.0304]\n",
      "Epoch [6/10], batch: [22/66, loss: 0.0234]\n",
      "Epoch [6/10], batch: [23/66, loss: 0.0194]\n",
      "Epoch [6/10], batch: [24/66, loss: 0.0627]\n",
      "Epoch [6/10], batch: [25/66, loss: 0.0432]\n",
      "Epoch [6/10], batch: [26/66, loss: 0.0818]\n",
      "Epoch [6/10], batch: [27/66, loss: 0.0171]\n",
      "Epoch [6/10], batch: [28/66, loss: 0.0110]\n",
      "Epoch [6/10], batch: [29/66, loss: 0.0190]\n",
      "Epoch [6/10], batch: [30/66, loss: 0.0336]\n",
      "Epoch [6/10], batch: [31/66, loss: 0.0081]\n",
      "Epoch [6/10], batch: [32/66, loss: 0.0684]\n",
      "Epoch [6/10], batch: [33/66, loss: 0.0755]\n",
      "Epoch [6/10], batch: [34/66, loss: 0.0155]\n",
      "Epoch [6/10], batch: [35/66, loss: 0.0335]\n",
      "Epoch [6/10], batch: [36/66, loss: 0.0624]\n",
      "Epoch [6/10], batch: [37/66, loss: 0.0362]\n",
      "Epoch [6/10], batch: [38/66, loss: 0.0266]\n",
      "Epoch [6/10], batch: [39/66, loss: 0.2031]\n",
      "Epoch [6/10], batch: [40/66, loss: 0.0044]\n",
      "Epoch [6/10], batch: [41/66, loss: 0.0244]\n",
      "Epoch [6/10], batch: [42/66, loss: 0.0285]\n",
      "Epoch [6/10], batch: [43/66, loss: 0.0672]\n",
      "Epoch [6/10], batch: [44/66, loss: 0.0955]\n",
      "Epoch [6/10], batch: [45/66, loss: 0.0032]\n",
      "Epoch [6/10], batch: [46/66, loss: 0.0206]\n",
      "Epoch [6/10], batch: [47/66, loss: 0.0773]\n",
      "Epoch [6/10], batch: [48/66, loss: 0.0659]\n",
      "Epoch [6/10], batch: [49/66, loss: 0.0274]\n",
      "Epoch [6/10], batch: [50/66, loss: 0.1315]\n",
      "Epoch [6/10], batch: [51/66, loss: 0.0484]\n",
      "Epoch [6/10], batch: [52/66, loss: 0.0075]\n",
      "Epoch [6/10], batch: [53/66, loss: 0.0134]\n",
      "Epoch [6/10], batch: [54/66, loss: 0.0130]\n",
      "Epoch [6/10], batch: [55/66, loss: 0.0272]\n",
      "Epoch [6/10], batch: [56/66, loss: 0.0037]\n",
      "Epoch [6/10], batch: [57/66, loss: 0.0149]\n",
      "Epoch [6/10], batch: [58/66, loss: 0.0233]\n",
      "Epoch [6/10], batch: [59/66, loss: 0.0048]\n",
      "Epoch [6/10], batch: [60/66, loss: 0.0218]\n",
      "Epoch [6/10], batch: [61/66, loss: 0.0323]\n",
      "Epoch [6/10], batch: [62/66, loss: 0.0103]\n",
      "Epoch [6/10], batch: [63/66, loss: 0.0058]\n",
      "Epoch [6/10], batch: [64/66, loss: 0.0041]\n",
      "Epoch [6/10], batch: [65/66, loss: 0.0021]\n",
      "Epoch [7/10], batch: [0/66, loss: 0.0038]\n",
      "Epoch [7/10], batch: [1/66, loss: 0.2058]\n",
      "Epoch [7/10], batch: [2/66, loss: 0.0034]\n",
      "Epoch [7/10], batch: [3/66, loss: 0.0516]\n",
      "Epoch [7/10], batch: [4/66, loss: 0.0090]\n",
      "Epoch [7/10], batch: [5/66, loss: 0.0151]\n",
      "Epoch [7/10], batch: [6/66, loss: 0.0188]\n",
      "Epoch [7/10], batch: [7/66, loss: 0.0023]\n",
      "Epoch [7/10], batch: [8/66, loss: 0.0045]\n",
      "Epoch [7/10], batch: [9/66, loss: 0.0275]\n",
      "Epoch [7/10], batch: [10/66, loss: 0.0047]\n",
      "Epoch [7/10], batch: [11/66, loss: 0.0349]\n",
      "Epoch [7/10], batch: [12/66, loss: 0.0139]\n",
      "Epoch [7/10], batch: [13/66, loss: 0.0174]\n",
      "Epoch [7/10], batch: [14/66, loss: 0.0140]\n",
      "Epoch [7/10], batch: [15/66, loss: 0.0055]\n",
      "Epoch [7/10], batch: [16/66, loss: 0.0057]\n",
      "Epoch [7/10], batch: [17/66, loss: 0.0073]\n",
      "Epoch [7/10], batch: [18/66, loss: 0.0205]\n",
      "Epoch [7/10], batch: [19/66, loss: 0.0440]\n",
      "Epoch [7/10], batch: [20/66, loss: 0.0348]\n",
      "Epoch [7/10], batch: [21/66, loss: 0.0028]\n",
      "Epoch [7/10], batch: [22/66, loss: 0.0029]\n",
      "Epoch [7/10], batch: [23/66, loss: 0.0082]\n",
      "Epoch [7/10], batch: [24/66, loss: 0.0409]\n",
      "Epoch [7/10], batch: [25/66, loss: 0.0640]\n",
      "Epoch [7/10], batch: [26/66, loss: 0.0141]\n",
      "Epoch [7/10], batch: [27/66, loss: 0.0187]\n",
      "Epoch [7/10], batch: [28/66, loss: 0.0223]\n",
      "Epoch [7/10], batch: [29/66, loss: 0.0296]\n",
      "Epoch [7/10], batch: [30/66, loss: 0.0437]\n",
      "Epoch [7/10], batch: [31/66, loss: 0.0070]\n",
      "Epoch [7/10], batch: [32/66, loss: 0.1124]\n",
      "Epoch [7/10], batch: [33/66, loss: 0.0038]\n",
      "Epoch [7/10], batch: [34/66, loss: 0.0354]\n",
      "Epoch [7/10], batch: [35/66, loss: 0.0030]\n",
      "Epoch [7/10], batch: [36/66, loss: 0.0008]\n",
      "Epoch [7/10], batch: [37/66, loss: 0.0548]\n",
      "Epoch [7/10], batch: [38/66, loss: 0.0585]\n",
      "Epoch [7/10], batch: [39/66, loss: 0.0099]\n",
      "Epoch [7/10], batch: [40/66, loss: 0.1172]\n",
      "Epoch [7/10], batch: [41/66, loss: 0.0108]\n",
      "Epoch [7/10], batch: [42/66, loss: 0.0217]\n",
      "Epoch [7/10], batch: [43/66, loss: 0.0106]\n",
      "Epoch [7/10], batch: [44/66, loss: 0.0561]\n",
      "Epoch [7/10], batch: [45/66, loss: 0.0007]\n",
      "Epoch [7/10], batch: [46/66, loss: 0.0125]\n",
      "Epoch [7/10], batch: [47/66, loss: 0.0072]\n",
      "Epoch [7/10], batch: [48/66, loss: 0.0519]\n",
      "Epoch [7/10], batch: [49/66, loss: 0.1367]\n",
      "Epoch [7/10], batch: [50/66, loss: 0.1239]\n",
      "Epoch [7/10], batch: [51/66, loss: 0.1337]\n",
      "Epoch [7/10], batch: [52/66, loss: 0.0430]\n",
      "Epoch [7/10], batch: [53/66, loss: 0.1414]\n",
      "Epoch [7/10], batch: [54/66, loss: 0.0498]\n",
      "Epoch [7/10], batch: [55/66, loss: 0.0085]\n",
      "Epoch [7/10], batch: [56/66, loss: 0.0080]\n",
      "Epoch [7/10], batch: [57/66, loss: 0.0306]\n",
      "Epoch [7/10], batch: [58/66, loss: 0.1760]\n",
      "Epoch [7/10], batch: [59/66, loss: 0.0009]\n",
      "Epoch [7/10], batch: [60/66, loss: 0.0116]\n",
      "Epoch [7/10], batch: [61/66, loss: 0.0011]\n",
      "Epoch [7/10], batch: [62/66, loss: 0.0063]\n",
      "Epoch [7/10], batch: [63/66, loss: 0.0023]\n",
      "Epoch [7/10], batch: [64/66, loss: 0.1380]\n",
      "Epoch [7/10], batch: [65/66, loss: 0.0441]\n",
      "Epoch [8/10], batch: [0/66, loss: 0.0208]\n",
      "Epoch [8/10], batch: [1/66, loss: 0.0522]\n",
      "Epoch [8/10], batch: [2/66, loss: 0.0198]\n",
      "Epoch [8/10], batch: [3/66, loss: 0.0056]\n",
      "Epoch [8/10], batch: [4/66, loss: 0.0183]\n",
      "Epoch [8/10], batch: [5/66, loss: 0.0187]\n",
      "Epoch [8/10], batch: [6/66, loss: 0.0024]\n",
      "Epoch [8/10], batch: [7/66, loss: 0.0047]\n",
      "Epoch [8/10], batch: [8/66, loss: 0.0346]\n",
      "Epoch [8/10], batch: [9/66, loss: 0.0136]\n",
      "Epoch [8/10], batch: [10/66, loss: 0.0060]\n",
      "Epoch [8/10], batch: [11/66, loss: 0.0984]\n",
      "Epoch [8/10], batch: [12/66, loss: 0.0012]\n",
      "Epoch [8/10], batch: [13/66, loss: 0.0026]\n",
      "Epoch [8/10], batch: [14/66, loss: 0.0087]\n",
      "Epoch [8/10], batch: [15/66, loss: 0.0041]\n",
      "Epoch [8/10], batch: [16/66, loss: 0.0017]\n",
      "Epoch [8/10], batch: [17/66, loss: 0.0200]\n",
      "Epoch [8/10], batch: [18/66, loss: 0.0029]\n",
      "Epoch [8/10], batch: [19/66, loss: 0.0028]\n",
      "Epoch [8/10], batch: [20/66, loss: 0.0047]\n",
      "Epoch [8/10], batch: [21/66, loss: 0.0063]\n",
      "Epoch [8/10], batch: [22/66, loss: 0.2640]\n",
      "Epoch [8/10], batch: [23/66, loss: 0.0018]\n",
      "Epoch [8/10], batch: [24/66, loss: 0.0091]\n",
      "Epoch [8/10], batch: [25/66, loss: 0.0127]\n",
      "Epoch [8/10], batch: [26/66, loss: 0.0104]\n",
      "Epoch [8/10], batch: [27/66, loss: 0.0021]\n",
      "Epoch [8/10], batch: [28/66, loss: 0.0374]\n",
      "Epoch [8/10], batch: [29/66, loss: 0.0457]\n",
      "Epoch [8/10], batch: [30/66, loss: 0.0028]\n",
      "Epoch [8/10], batch: [31/66, loss: 0.0019]\n",
      "Epoch [8/10], batch: [32/66, loss: 0.0103]\n",
      "Epoch [8/10], batch: [33/66, loss: 0.0011]\n",
      "Epoch [8/10], batch: [34/66, loss: 0.0146]\n",
      "Epoch [8/10], batch: [35/66, loss: 0.0022]\n",
      "Epoch [8/10], batch: [36/66, loss: 0.0462]\n",
      "Epoch [8/10], batch: [37/66, loss: 0.0098]\n",
      "Epoch [8/10], batch: [38/66, loss: 0.0205]\n",
      "Epoch [8/10], batch: [39/66, loss: 0.0054]\n",
      "Epoch [8/10], batch: [40/66, loss: 0.0069]\n",
      "Epoch [8/10], batch: [41/66, loss: 0.0092]\n",
      "Epoch [8/10], batch: [42/66, loss: 0.0193]\n",
      "Epoch [8/10], batch: [43/66, loss: 0.0094]\n",
      "Epoch [8/10], batch: [44/66, loss: 0.0166]\n",
      "Epoch [8/10], batch: [45/66, loss: 0.0017]\n",
      "Epoch [8/10], batch: [46/66, loss: 0.0031]\n",
      "Epoch [8/10], batch: [47/66, loss: 0.0027]\n",
      "Epoch [8/10], batch: [48/66, loss: 0.0675]\n",
      "Epoch [8/10], batch: [49/66, loss: 0.0013]\n",
      "Epoch [8/10], batch: [50/66, loss: 0.0016]\n",
      "Epoch [8/10], batch: [51/66, loss: 0.0046]\n",
      "Epoch [8/10], batch: [52/66, loss: 0.0024]\n",
      "Epoch [8/10], batch: [53/66, loss: 0.0357]\n",
      "Epoch [8/10], batch: [54/66, loss: 0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], batch: [55/66, loss: 0.0100]\n",
      "Epoch [8/10], batch: [56/66, loss: 0.0004]\n",
      "Epoch [8/10], batch: [57/66, loss: 0.0021]\n",
      "Epoch [8/10], batch: [58/66, loss: 0.0811]\n",
      "Epoch [8/10], batch: [59/66, loss: 0.0016]\n",
      "Epoch [8/10], batch: [60/66, loss: 0.1338]\n",
      "Epoch [8/10], batch: [61/66, loss: 0.0319]\n",
      "Epoch [8/10], batch: [62/66, loss: 0.0175]\n",
      "Epoch [8/10], batch: [63/66, loss: 0.0043]\n",
      "Epoch [8/10], batch: [64/66, loss: 0.0027]\n",
      "Epoch [8/10], batch: [65/66, loss: 0.0345]\n",
      "Epoch [9/10], batch: [0/66, loss: 0.1827]\n",
      "Epoch [9/10], batch: [1/66, loss: 0.0067]\n",
      "Epoch [9/10], batch: [2/66, loss: 0.0017]\n",
      "Epoch [9/10], batch: [3/66, loss: 0.0012]\n",
      "Epoch [9/10], batch: [4/66, loss: 0.0021]\n",
      "Epoch [9/10], batch: [5/66, loss: 0.0156]\n",
      "Epoch [9/10], batch: [6/66, loss: 0.0167]\n",
      "Epoch [9/10], batch: [7/66, loss: 0.0091]\n",
      "Epoch [9/10], batch: [8/66, loss: 0.0221]\n",
      "Epoch [9/10], batch: [9/66, loss: 0.0066]\n",
      "Epoch [9/10], batch: [10/66, loss: 0.0030]\n",
      "Epoch [9/10], batch: [11/66, loss: 0.0098]\n",
      "Epoch [9/10], batch: [12/66, loss: 0.0041]\n",
      "Epoch [9/10], batch: [13/66, loss: 0.0030]\n",
      "Epoch [9/10], batch: [14/66, loss: 0.0015]\n",
      "Epoch [9/10], batch: [15/66, loss: 0.0114]\n",
      "Epoch [9/10], batch: [16/66, loss: 0.0066]\n",
      "Epoch [9/10], batch: [17/66, loss: 0.1547]\n",
      "Epoch [9/10], batch: [18/66, loss: 0.0072]\n",
      "Epoch [9/10], batch: [19/66, loss: 0.0014]\n",
      "Epoch [9/10], batch: [20/66, loss: 0.0050]\n",
      "Epoch [9/10], batch: [21/66, loss: 0.0067]\n",
      "Epoch [9/10], batch: [22/66, loss: 0.1907]\n",
      "Epoch [9/10], batch: [23/66, loss: 0.0007]\n",
      "Epoch [9/10], batch: [24/66, loss: 0.0035]\n",
      "Epoch [9/10], batch: [25/66, loss: 0.2506]\n",
      "Epoch [9/10], batch: [26/66, loss: 0.0079]\n",
      "Epoch [9/10], batch: [27/66, loss: 0.0624]\n",
      "Epoch [9/10], batch: [28/66, loss: 0.0152]\n",
      "Epoch [9/10], batch: [29/66, loss: 0.1191]\n",
      "Epoch [9/10], batch: [30/66, loss: 0.0014]\n",
      "Epoch [9/10], batch: [31/66, loss: 0.0029]\n",
      "Epoch [9/10], batch: [32/66, loss: 0.0214]\n",
      "Epoch [9/10], batch: [33/66, loss: 0.0276]\n",
      "Epoch [9/10], batch: [34/66, loss: 0.1535]\n",
      "Epoch [9/10], batch: [35/66, loss: 0.0107]\n",
      "Epoch [9/10], batch: [36/66, loss: 0.0023]\n",
      "Epoch [9/10], batch: [37/66, loss: 0.0052]\n",
      "Epoch [9/10], batch: [38/66, loss: 0.0067]\n",
      "Epoch [9/10], batch: [39/66, loss: 0.0201]\n",
      "Epoch [9/10], batch: [40/66, loss: 0.0514]\n",
      "Epoch [9/10], batch: [41/66, loss: 0.0066]\n",
      "Epoch [9/10], batch: [42/66, loss: 0.0033]\n",
      "Epoch [9/10], batch: [43/66, loss: 0.0049]\n",
      "Epoch [9/10], batch: [44/66, loss: 0.0162]\n",
      "Epoch [9/10], batch: [45/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [46/66, loss: 0.0750]\n",
      "Epoch [9/10], batch: [47/66, loss: 0.0333]\n",
      "Epoch [9/10], batch: [48/66, loss: 0.0055]\n",
      "Epoch [9/10], batch: [49/66, loss: 0.0047]\n",
      "Epoch [9/10], batch: [50/66, loss: 0.0016]\n",
      "Epoch [9/10], batch: [51/66, loss: 0.0087]\n",
      "Epoch [9/10], batch: [52/66, loss: 0.1067]\n",
      "Epoch [9/10], batch: [53/66, loss: 0.0037]\n",
      "Epoch [9/10], batch: [54/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [55/66, loss: 0.0064]\n",
      "Epoch [9/10], batch: [56/66, loss: 0.0011]\n",
      "Epoch [9/10], batch: [57/66, loss: 0.1384]\n",
      "Epoch [9/10], batch: [58/66, loss: 0.0035]\n",
      "Epoch [9/10], batch: [59/66, loss: 0.0072]\n",
      "Epoch [9/10], batch: [60/66, loss: 0.0008]\n",
      "Epoch [9/10], batch: [61/66, loss: 0.0039]\n",
      "Epoch [9/10], batch: [62/66, loss: 0.0028]\n",
      "Epoch [9/10], batch: [63/66, loss: 0.0034]\n",
      "Epoch [9/10], batch: [64/66, loss: 0.0034]\n",
      "Epoch [9/10], batch: [65/66, loss: 0.2901]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_v1(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e580ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 1.730\n",
      "Acccuracy  : 0.603\n",
      "F1 Score: 0.469\n"
     ]
    }
   ],
   "source": [
    "calc_loss_acc(model_v1, loss_func, dataloader_val_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4acc29",
   "metadata": {},
   "source": [
    "Before going to task 2, let's do the same for the other dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac0b1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_v2 = my_model(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_v2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa98788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], batch: [0/49, loss: 0.6847]\n",
      "Epoch [0/10], batch: [1/49, loss: 0.7418]\n",
      "Epoch [0/10], batch: [2/49, loss: 0.6777]\n",
      "Epoch [0/10], batch: [3/49, loss: 0.6849]\n",
      "Epoch [0/10], batch: [4/49, loss: 0.8429]\n",
      "Epoch [0/10], batch: [5/49, loss: 0.6956]\n",
      "Epoch [0/10], batch: [6/49, loss: 0.7176]\n",
      "Epoch [0/10], batch: [7/49, loss: 0.7083]\n",
      "Epoch [0/10], batch: [8/49, loss: 0.7441]\n",
      "Epoch [0/10], batch: [9/49, loss: 0.6944]\n",
      "Epoch [0/10], batch: [10/49, loss: 0.6826]\n",
      "Epoch [0/10], batch: [11/49, loss: 0.7294]\n",
      "Epoch [0/10], batch: [12/49, loss: 0.7434]\n",
      "Epoch [0/10], batch: [13/49, loss: 0.7120]\n",
      "Epoch [0/10], batch: [14/49, loss: 0.6997]\n",
      "Epoch [0/10], batch: [15/49, loss: 0.6877]\n",
      "Epoch [0/10], batch: [16/49, loss: 0.7074]\n",
      "Epoch [0/10], batch: [17/49, loss: 0.6847]\n",
      "Epoch [0/10], batch: [18/49, loss: 0.6873]\n",
      "Epoch [0/10], batch: [19/49, loss: 0.6990]\n",
      "Epoch [0/10], batch: [20/49, loss: 0.6864]\n",
      "Epoch [0/10], batch: [21/49, loss: 0.6960]\n",
      "Epoch [0/10], batch: [22/49, loss: 0.6876]\n",
      "Epoch [0/10], batch: [23/49, loss: 0.6872]\n",
      "Epoch [0/10], batch: [24/49, loss: 0.6675]\n",
      "Epoch [0/10], batch: [25/49, loss: 0.6897]\n",
      "Epoch [0/10], batch: [26/49, loss: 0.7235]\n",
      "Epoch [0/10], batch: [27/49, loss: 0.6776]\n",
      "Epoch [0/10], batch: [28/49, loss: 0.6881]\n",
      "Epoch [0/10], batch: [29/49, loss: 0.7045]\n",
      "Epoch [0/10], batch: [30/49, loss: 0.6589]\n",
      "Epoch [0/10], batch: [31/49, loss: 0.6074]\n",
      "Epoch [0/10], batch: [32/49, loss: 0.7080]\n",
      "Epoch [0/10], batch: [33/49, loss: 0.6839]\n",
      "Epoch [0/10], batch: [34/49, loss: 0.7046]\n",
      "Epoch [0/10], batch: [35/49, loss: 0.5860]\n",
      "Epoch [0/10], batch: [36/49, loss: 0.7072]\n",
      "Epoch [0/10], batch: [37/49, loss: 0.6647]\n",
      "Epoch [0/10], batch: [38/49, loss: 0.6937]\n",
      "Epoch [0/10], batch: [39/49, loss: 0.6512]\n",
      "Epoch [0/10], batch: [40/49, loss: 0.6998]\n",
      "Epoch [0/10], batch: [41/49, loss: 0.6844]\n",
      "Epoch [0/10], batch: [42/49, loss: 0.6820]\n",
      "Epoch [0/10], batch: [43/49, loss: 0.6566]\n",
      "Epoch [0/10], batch: [44/49, loss: 0.7440]\n",
      "Epoch [0/10], batch: [45/49, loss: 0.6642]\n",
      "Epoch [0/10], batch: [46/49, loss: 0.6635]\n",
      "Epoch [0/10], batch: [47/49, loss: 0.6499]\n",
      "Epoch [0/10], batch: [48/49, loss: 0.6916]\n",
      "Epoch [1/10], batch: [0/49, loss: 0.5989]\n",
      "Epoch [1/10], batch: [1/49, loss: 0.6126]\n",
      "Epoch [1/10], batch: [2/49, loss: 0.6234]\n",
      "Epoch [1/10], batch: [3/49, loss: 0.6144]\n",
      "Epoch [1/10], batch: [4/49, loss: 0.7025]\n",
      "Epoch [1/10], batch: [5/49, loss: 0.6028]\n",
      "Epoch [1/10], batch: [6/49, loss: 0.5848]\n",
      "Epoch [1/10], batch: [7/49, loss: 0.6419]\n",
      "Epoch [1/10], batch: [8/49, loss: 0.7153]\n",
      "Epoch [1/10], batch: [9/49, loss: 0.6423]\n",
      "Epoch [1/10], batch: [10/49, loss: 0.6314]\n",
      "Epoch [1/10], batch: [11/49, loss: 0.6016]\n",
      "Epoch [1/10], batch: [12/49, loss: 0.5180]\n",
      "Epoch [1/10], batch: [13/49, loss: 0.9810]\n",
      "Epoch [1/10], batch: [14/49, loss: 0.9068]\n",
      "Epoch [1/10], batch: [15/49, loss: 0.7073]\n",
      "Epoch [1/10], batch: [16/49, loss: 0.6092]\n",
      "Epoch [1/10], batch: [17/49, loss: 0.6103]\n",
      "Epoch [1/10], batch: [18/49, loss: 0.7783]\n",
      "Epoch [1/10], batch: [19/49, loss: 0.7880]\n",
      "Epoch [1/10], batch: [20/49, loss: 0.6761]\n",
      "Epoch [1/10], batch: [21/49, loss: 0.6648]\n",
      "Epoch [1/10], batch: [22/49, loss: 0.6491]\n",
      "Epoch [1/10], batch: [23/49, loss: 0.6690]\n",
      "Epoch [1/10], batch: [24/49, loss: 0.6877]\n",
      "Epoch [1/10], batch: [25/49, loss: 0.6369]\n",
      "Epoch [1/10], batch: [26/49, loss: 0.6967]\n",
      "Epoch [1/10], batch: [27/49, loss: 0.6578]\n",
      "Epoch [1/10], batch: [28/49, loss: 0.6432]\n",
      "Epoch [1/10], batch: [29/49, loss: 0.6413]\n",
      "Epoch [1/10], batch: [30/49, loss: 0.6848]\n",
      "Epoch [1/10], batch: [31/49, loss: 0.6704]\n",
      "Epoch [1/10], batch: [32/49, loss: 0.6705]\n",
      "Epoch [1/10], batch: [33/49, loss: 0.6155]\n",
      "Epoch [1/10], batch: [34/49, loss: 0.6176]\n",
      "Epoch [1/10], batch: [35/49, loss: 0.6535]\n",
      "Epoch [1/10], batch: [36/49, loss: 0.6520]\n",
      "Epoch [1/10], batch: [37/49, loss: 0.6719]\n",
      "Epoch [1/10], batch: [38/49, loss: 0.6281]\n",
      "Epoch [1/10], batch: [39/49, loss: 0.6324]\n",
      "Epoch [1/10], batch: [40/49, loss: 0.6443]\n",
      "Epoch [1/10], batch: [41/49, loss: 0.6630]\n",
      "Epoch [1/10], batch: [42/49, loss: 0.6270]\n",
      "Epoch [1/10], batch: [43/49, loss: 0.5543]\n",
      "Epoch [1/10], batch: [44/49, loss: 0.6538]\n",
      "Epoch [1/10], batch: [45/49, loss: 0.6090]\n",
      "Epoch [1/10], batch: [46/49, loss: 0.6632]\n",
      "Epoch [1/10], batch: [47/49, loss: 0.6234]\n",
      "Epoch [1/10], batch: [48/49, loss: 0.5649]\n",
      "Epoch [2/10], batch: [0/49, loss: 0.5433]\n",
      "Epoch [2/10], batch: [1/49, loss: 0.4961]\n",
      "Epoch [2/10], batch: [2/49, loss: 0.5161]\n",
      "Epoch [2/10], batch: [3/49, loss: 0.5583]\n",
      "Epoch [2/10], batch: [4/49, loss: 0.4710]\n",
      "Epoch [2/10], batch: [5/49, loss: 0.4944]\n",
      "Epoch [2/10], batch: [6/49, loss: 0.5391]\n",
      "Epoch [2/10], batch: [7/49, loss: 0.4264]\n",
      "Epoch [2/10], batch: [8/49, loss: 0.5142]\n",
      "Epoch [2/10], batch: [9/49, loss: 0.5808]\n",
      "Epoch [2/10], batch: [10/49, loss: 0.4747]\n",
      "Epoch [2/10], batch: [11/49, loss: 0.4492]\n",
      "Epoch [2/10], batch: [12/49, loss: 0.4648]\n",
      "Epoch [2/10], batch: [13/49, loss: 0.4585]\n",
      "Epoch [2/10], batch: [14/49, loss: 0.4832]\n",
      "Epoch [2/10], batch: [15/49, loss: 0.4545]\n",
      "Epoch [2/10], batch: [16/49, loss: 0.6331]\n",
      "Epoch [2/10], batch: [17/49, loss: 0.5132]\n",
      "Epoch [2/10], batch: [18/49, loss: 0.6803]\n",
      "Epoch [2/10], batch: [19/49, loss: 0.5201]\n",
      "Epoch [2/10], batch: [20/49, loss: 0.4837]\n",
      "Epoch [2/10], batch: [21/49, loss: 0.4978]\n",
      "Epoch [2/10], batch: [22/49, loss: 0.5825]\n",
      "Epoch [2/10], batch: [23/49, loss: 0.4725]\n",
      "Epoch [2/10], batch: [24/49, loss: 0.4845]\n",
      "Epoch [2/10], batch: [25/49, loss: 0.5834]\n",
      "Epoch [2/10], batch: [26/49, loss: 0.4856]\n",
      "Epoch [2/10], batch: [27/49, loss: 0.6183]\n",
      "Epoch [2/10], batch: [28/49, loss: 0.5561]\n",
      "Epoch [2/10], batch: [29/49, loss: 0.4643]\n",
      "Epoch [2/10], batch: [30/49, loss: 0.6596]\n",
      "Epoch [2/10], batch: [31/49, loss: 0.4017]\n",
      "Epoch [2/10], batch: [32/49, loss: 0.5036]\n",
      "Epoch [2/10], batch: [33/49, loss: 0.4627]\n",
      "Epoch [2/10], batch: [34/49, loss: 0.4787]\n",
      "Epoch [2/10], batch: [35/49, loss: 0.4707]\n",
      "Epoch [2/10], batch: [36/49, loss: 0.5633]\n",
      "Epoch [2/10], batch: [37/49, loss: 0.4102]\n",
      "Epoch [2/10], batch: [38/49, loss: 0.4292]\n",
      "Epoch [2/10], batch: [39/49, loss: 0.5198]\n",
      "Epoch [2/10], batch: [40/49, loss: 0.3825]\n",
      "Epoch [2/10], batch: [41/49, loss: 0.5620]\n",
      "Epoch [2/10], batch: [42/49, loss: 0.4337]\n",
      "Epoch [2/10], batch: [43/49, loss: 0.7093]\n",
      "Epoch [2/10], batch: [44/49, loss: 0.3921]\n",
      "Epoch [2/10], batch: [45/49, loss: 0.4020]\n",
      "Epoch [2/10], batch: [46/49, loss: 0.4056]\n",
      "Epoch [2/10], batch: [47/49, loss: 0.3450]\n",
      "Epoch [2/10], batch: [48/49, loss: 0.3382]\n",
      "Epoch [3/10], batch: [0/49, loss: 0.2547]\n",
      "Epoch [3/10], batch: [1/49, loss: 0.1983]\n",
      "Epoch [3/10], batch: [2/49, loss: 0.3043]\n",
      "Epoch [3/10], batch: [3/49, loss: 0.3328]\n",
      "Epoch [3/10], batch: [4/49, loss: 0.2482]\n",
      "Epoch [3/10], batch: [5/49, loss: 0.3165]\n",
      "Epoch [3/10], batch: [6/49, loss: 0.3079]\n",
      "Epoch [3/10], batch: [7/49, loss: 0.2478]\n",
      "Epoch [3/10], batch: [8/49, loss: 0.3218]\n",
      "Epoch [3/10], batch: [9/49, loss: 0.2666]\n",
      "Epoch [3/10], batch: [10/49, loss: 0.5719]\n",
      "Epoch [3/10], batch: [11/49, loss: 0.1566]\n",
      "Epoch [3/10], batch: [12/49, loss: 0.2462]\n",
      "Epoch [3/10], batch: [13/49, loss: 0.2617]\n",
      "Epoch [3/10], batch: [14/49, loss: 0.1418]\n",
      "Epoch [3/10], batch: [15/49, loss: 0.2614]\n",
      "Epoch [3/10], batch: [16/49, loss: 0.2239]\n",
      "Epoch [3/10], batch: [17/49, loss: 0.2741]\n",
      "Epoch [3/10], batch: [18/49, loss: 0.5067]\n",
      "Epoch [3/10], batch: [19/49, loss: 0.2138]\n",
      "Epoch [3/10], batch: [20/49, loss: 0.2283]\n",
      "Epoch [3/10], batch: [21/49, loss: 0.3520]\n",
      "Epoch [3/10], batch: [22/49, loss: 0.1450]\n",
      "Epoch [3/10], batch: [23/49, loss: 0.2294]\n",
      "Epoch [3/10], batch: [24/49, loss: 0.4254]\n",
      "Epoch [3/10], batch: [25/49, loss: 0.2572]\n",
      "Epoch [3/10], batch: [26/49, loss: 0.5147]\n",
      "Epoch [3/10], batch: [27/49, loss: 0.3085]\n",
      "Epoch [3/10], batch: [28/49, loss: 0.2943]\n",
      "Epoch [3/10], batch: [29/49, loss: 0.1266]\n",
      "Epoch [3/10], batch: [30/49, loss: 0.4835]\n",
      "Epoch [3/10], batch: [31/49, loss: 0.2339]\n",
      "Epoch [3/10], batch: [32/49, loss: 0.3093]\n",
      "Epoch [3/10], batch: [33/49, loss: 0.1774]\n",
      "Epoch [3/10], batch: [34/49, loss: 0.4629]\n",
      "Epoch [3/10], batch: [35/49, loss: 0.2581]\n",
      "Epoch [3/10], batch: [36/49, loss: 0.3868]\n",
      "Epoch [3/10], batch: [37/49, loss: 0.2496]\n",
      "Epoch [3/10], batch: [38/49, loss: 0.2111]\n",
      "Epoch [3/10], batch: [39/49, loss: 0.3880]\n",
      "Epoch [3/10], batch: [40/49, loss: 0.1888]\n",
      "Epoch [3/10], batch: [41/49, loss: 0.3701]\n",
      "Epoch [3/10], batch: [42/49, loss: 0.2525]\n",
      "Epoch [3/10], batch: [43/49, loss: 0.1644]\n",
      "Epoch [3/10], batch: [44/49, loss: 0.1860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], batch: [45/49, loss: 0.3244]\n",
      "Epoch [3/10], batch: [46/49, loss: 0.4377]\n",
      "Epoch [3/10], batch: [47/49, loss: 0.2319]\n",
      "Epoch [3/10], batch: [48/49, loss: 0.1654]\n",
      "Epoch [4/10], batch: [0/49, loss: 0.1612]\n",
      "Epoch [4/10], batch: [1/49, loss: 0.1561]\n",
      "Epoch [4/10], batch: [2/49, loss: 0.0521]\n",
      "Epoch [4/10], batch: [3/49, loss: 0.0487]\n",
      "Epoch [4/10], batch: [4/49, loss: 0.1485]\n",
      "Epoch [4/10], batch: [5/49, loss: 0.1165]\n",
      "Epoch [4/10], batch: [6/49, loss: 0.1850]\n",
      "Epoch [4/10], batch: [7/49, loss: 0.1385]\n",
      "Epoch [4/10], batch: [8/49, loss: 0.1419]\n",
      "Epoch [4/10], batch: [9/49, loss: 0.1457]\n",
      "Epoch [4/10], batch: [10/49, loss: 0.1313]\n",
      "Epoch [4/10], batch: [11/49, loss: 0.0611]\n",
      "Epoch [4/10], batch: [12/49, loss: 0.0960]\n",
      "Epoch [4/10], batch: [13/49, loss: 0.1917]\n",
      "Epoch [4/10], batch: [14/49, loss: 0.1547]\n",
      "Epoch [4/10], batch: [15/49, loss: 0.0678]\n",
      "Epoch [4/10], batch: [16/49, loss: 0.1348]\n",
      "Epoch [4/10], batch: [17/49, loss: 0.1625]\n",
      "Epoch [4/10], batch: [18/49, loss: 0.1708]\n",
      "Epoch [4/10], batch: [19/49, loss: 0.1401]\n",
      "Epoch [4/10], batch: [20/49, loss: 0.1667]\n",
      "Epoch [4/10], batch: [21/49, loss: 0.1362]\n",
      "Epoch [4/10], batch: [22/49, loss: 0.1268]\n",
      "Epoch [4/10], batch: [23/49, loss: 0.0566]\n",
      "Epoch [4/10], batch: [24/49, loss: 0.0701]\n",
      "Epoch [4/10], batch: [25/49, loss: 0.1567]\n",
      "Epoch [4/10], batch: [26/49, loss: 0.0684]\n",
      "Epoch [4/10], batch: [27/49, loss: 0.1529]\n",
      "Epoch [4/10], batch: [28/49, loss: 0.1009]\n",
      "Epoch [4/10], batch: [29/49, loss: 0.0256]\n",
      "Epoch [4/10], batch: [30/49, loss: 0.2051]\n",
      "Epoch [4/10], batch: [31/49, loss: 0.3535]\n",
      "Epoch [4/10], batch: [32/49, loss: 0.0231]\n",
      "Epoch [4/10], batch: [33/49, loss: 0.2127]\n",
      "Epoch [4/10], batch: [34/49, loss: 0.3641]\n",
      "Epoch [4/10], batch: [35/49, loss: 0.1144]\n",
      "Epoch [4/10], batch: [36/49, loss: 0.5106]\n",
      "Epoch [4/10], batch: [37/49, loss: 0.1564]\n",
      "Epoch [4/10], batch: [38/49, loss: 0.0825]\n",
      "Epoch [4/10], batch: [39/49, loss: 0.1030]\n",
      "Epoch [4/10], batch: [40/49, loss: 0.2355]\n",
      "Epoch [4/10], batch: [41/49, loss: 0.2634]\n",
      "Epoch [4/10], batch: [42/49, loss: 0.0588]\n",
      "Epoch [4/10], batch: [43/49, loss: 0.1096]\n",
      "Epoch [4/10], batch: [44/49, loss: 0.0557]\n",
      "Epoch [4/10], batch: [45/49, loss: 0.2382]\n",
      "Epoch [4/10], batch: [46/49, loss: 0.0962]\n",
      "Epoch [4/10], batch: [47/49, loss: 0.0854]\n",
      "Epoch [4/10], batch: [48/49, loss: 0.1112]\n",
      "Epoch [5/10], batch: [0/49, loss: 0.1333]\n",
      "Epoch [5/10], batch: [1/49, loss: 0.0716]\n",
      "Epoch [5/10], batch: [2/49, loss: 0.0713]\n",
      "Epoch [5/10], batch: [3/49, loss: 0.1207]\n",
      "Epoch [5/10], batch: [4/49, loss: 0.0927]\n",
      "Epoch [5/10], batch: [5/49, loss: 0.0576]\n",
      "Epoch [5/10], batch: [6/49, loss: 0.0818]\n",
      "Epoch [5/10], batch: [7/49, loss: 0.1020]\n",
      "Epoch [5/10], batch: [8/49, loss: 0.0762]\n",
      "Epoch [5/10], batch: [9/49, loss: 0.0267]\n",
      "Epoch [5/10], batch: [10/49, loss: 0.0829]\n",
      "Epoch [5/10], batch: [11/49, loss: 0.0209]\n",
      "Epoch [5/10], batch: [12/49, loss: 0.0658]\n",
      "Epoch [5/10], batch: [13/49, loss: 0.0211]\n",
      "Epoch [5/10], batch: [14/49, loss: 0.0448]\n",
      "Epoch [5/10], batch: [15/49, loss: 0.0245]\n",
      "Epoch [5/10], batch: [16/49, loss: 0.0283]\n",
      "Epoch [5/10], batch: [17/49, loss: 0.0699]\n",
      "Epoch [5/10], batch: [18/49, loss: 0.0277]\n",
      "Epoch [5/10], batch: [19/49, loss: 0.0500]\n",
      "Epoch [5/10], batch: [20/49, loss: 0.0337]\n",
      "Epoch [5/10], batch: [21/49, loss: 0.0253]\n",
      "Epoch [5/10], batch: [22/49, loss: 0.0374]\n",
      "Epoch [5/10], batch: [23/49, loss: 0.0725]\n",
      "Epoch [5/10], batch: [24/49, loss: 0.0563]\n",
      "Epoch [5/10], batch: [25/49, loss: 0.0459]\n",
      "Epoch [5/10], batch: [26/49, loss: 0.0185]\n",
      "Epoch [5/10], batch: [27/49, loss: 0.0205]\n",
      "Epoch [5/10], batch: [28/49, loss: 0.1654]\n",
      "Epoch [5/10], batch: [29/49, loss: 0.0640]\n",
      "Epoch [5/10], batch: [30/49, loss: 0.0367]\n",
      "Epoch [5/10], batch: [31/49, loss: 0.2703]\n",
      "Epoch [5/10], batch: [32/49, loss: 0.0563]\n",
      "Epoch [5/10], batch: [33/49, loss: 0.1018]\n",
      "Epoch [5/10], batch: [34/49, loss: 0.0337]\n",
      "Epoch [5/10], batch: [35/49, loss: 0.1364]\n",
      "Epoch [5/10], batch: [36/49, loss: 0.0404]\n",
      "Epoch [5/10], batch: [37/49, loss: 0.2529]\n",
      "Epoch [5/10], batch: [38/49, loss: 0.1313]\n",
      "Epoch [5/10], batch: [39/49, loss: 0.0227]\n",
      "Epoch [5/10], batch: [40/49, loss: 0.1280]\n",
      "Epoch [5/10], batch: [41/49, loss: 0.3452]\n",
      "Epoch [5/10], batch: [42/49, loss: 0.0868]\n",
      "Epoch [5/10], batch: [43/49, loss: 0.3152]\n",
      "Epoch [5/10], batch: [44/49, loss: 0.2866]\n",
      "Epoch [5/10], batch: [45/49, loss: 0.0843]\n",
      "Epoch [5/10], batch: [46/49, loss: 0.1095]\n",
      "Epoch [5/10], batch: [47/49, loss: 0.1968]\n",
      "Epoch [5/10], batch: [48/49, loss: 0.0204]\n",
      "Epoch [6/10], batch: [0/49, loss: 0.0512]\n",
      "Epoch [6/10], batch: [1/49, loss: 0.0211]\n",
      "Epoch [6/10], batch: [2/49, loss: 0.0592]\n",
      "Epoch [6/10], batch: [3/49, loss: 0.0463]\n",
      "Epoch [6/10], batch: [4/49, loss: 0.0517]\n",
      "Epoch [6/10], batch: [5/49, loss: 0.0392]\n",
      "Epoch [6/10], batch: [6/49, loss: 0.0470]\n",
      "Epoch [6/10], batch: [7/49, loss: 0.0472]\n",
      "Epoch [6/10], batch: [8/49, loss: 0.0233]\n",
      "Epoch [6/10], batch: [9/49, loss: 0.1030]\n",
      "Epoch [6/10], batch: [10/49, loss: 0.0323]\n",
      "Epoch [6/10], batch: [11/49, loss: 0.0601]\n",
      "Epoch [6/10], batch: [12/49, loss: 0.0771]\n",
      "Epoch [6/10], batch: [13/49, loss: 0.0411]\n",
      "Epoch [6/10], batch: [14/49, loss: 0.0290]\n",
      "Epoch [6/10], batch: [15/49, loss: 0.0350]\n",
      "Epoch [6/10], batch: [16/49, loss: 0.1513]\n",
      "Epoch [6/10], batch: [17/49, loss: 0.0436]\n",
      "Epoch [6/10], batch: [18/49, loss: 0.0340]\n",
      "Epoch [6/10], batch: [19/49, loss: 0.0262]\n",
      "Epoch [6/10], batch: [20/49, loss: 0.0217]\n",
      "Epoch [6/10], batch: [21/49, loss: 0.0324]\n",
      "Epoch [6/10], batch: [22/49, loss: 0.0347]\n",
      "Epoch [6/10], batch: [23/49, loss: 0.0098]\n",
      "Epoch [6/10], batch: [24/49, loss: 0.1278]\n",
      "Epoch [6/10], batch: [25/49, loss: 0.0053]\n",
      "Epoch [6/10], batch: [26/49, loss: 0.0066]\n",
      "Epoch [6/10], batch: [27/49, loss: 0.0141]\n",
      "Epoch [6/10], batch: [28/49, loss: 0.0549]\n",
      "Epoch [6/10], batch: [29/49, loss: 0.0196]\n",
      "Epoch [6/10], batch: [30/49, loss: 0.0166]\n",
      "Epoch [6/10], batch: [31/49, loss: 0.0324]\n",
      "Epoch [6/10], batch: [32/49, loss: 0.0184]\n",
      "Epoch [6/10], batch: [33/49, loss: 0.0335]\n",
      "Epoch [6/10], batch: [34/49, loss: 0.1131]\n",
      "Epoch [6/10], batch: [35/49, loss: 0.0208]\n",
      "Epoch [6/10], batch: [36/49, loss: 0.0023]\n",
      "Epoch [6/10], batch: [37/49, loss: 0.0818]\n",
      "Epoch [6/10], batch: [38/49, loss: 0.0720]\n",
      "Epoch [6/10], batch: [39/49, loss: 0.0249]\n",
      "Epoch [6/10], batch: [40/49, loss: 0.0094]\n",
      "Epoch [6/10], batch: [41/49, loss: 0.0905]\n",
      "Epoch [6/10], batch: [42/49, loss: 0.0729]\n",
      "Epoch [6/10], batch: [43/49, loss: 0.0594]\n",
      "Epoch [6/10], batch: [44/49, loss: 0.0092]\n",
      "Epoch [6/10], batch: [45/49, loss: 0.0412]\n",
      "Epoch [6/10], batch: [46/49, loss: 0.1208]\n",
      "Epoch [6/10], batch: [47/49, loss: 0.0552]\n",
      "Epoch [6/10], batch: [48/49, loss: 0.0113]\n",
      "Epoch [7/10], batch: [0/49, loss: 0.0175]\n",
      "Epoch [7/10], batch: [1/49, loss: 0.0269]\n",
      "Epoch [7/10], batch: [2/49, loss: 0.0947]\n",
      "Epoch [7/10], batch: [3/49, loss: 0.1013]\n",
      "Epoch [7/10], batch: [4/49, loss: 0.0303]\n",
      "Epoch [7/10], batch: [5/49, loss: 0.0059]\n",
      "Epoch [7/10], batch: [6/49, loss: 0.0148]\n",
      "Epoch [7/10], batch: [7/49, loss: 0.0110]\n",
      "Epoch [7/10], batch: [8/49, loss: 0.0074]\n",
      "Epoch [7/10], batch: [9/49, loss: 0.2215]\n",
      "Epoch [7/10], batch: [10/49, loss: 0.0233]\n",
      "Epoch [7/10], batch: [11/49, loss: 0.0061]\n",
      "Epoch [7/10], batch: [12/49, loss: 0.0275]\n",
      "Epoch [7/10], batch: [13/49, loss: 0.0515]\n",
      "Epoch [7/10], batch: [14/49, loss: 0.0292]\n",
      "Epoch [7/10], batch: [15/49, loss: 0.0814]\n",
      "Epoch [7/10], batch: [16/49, loss: 0.0277]\n",
      "Epoch [7/10], batch: [17/49, loss: 0.0073]\n",
      "Epoch [7/10], batch: [18/49, loss: 0.0602]\n",
      "Epoch [7/10], batch: [19/49, loss: 0.2800]\n",
      "Epoch [7/10], batch: [20/49, loss: 0.0898]\n",
      "Epoch [7/10], batch: [21/49, loss: 0.0192]\n",
      "Epoch [7/10], batch: [22/49, loss: 0.0477]\n",
      "Epoch [7/10], batch: [23/49, loss: 0.5757]\n",
      "Epoch [7/10], batch: [24/49, loss: 0.0355]\n",
      "Epoch [7/10], batch: [25/49, loss: 0.0102]\n",
      "Epoch [7/10], batch: [26/49, loss: 0.1716]\n",
      "Epoch [7/10], batch: [27/49, loss: 0.0456]\n",
      "Epoch [7/10], batch: [28/49, loss: 0.4502]\n",
      "Epoch [7/10], batch: [29/49, loss: 0.1323]\n",
      "Epoch [7/10], batch: [30/49, loss: 0.0173]\n",
      "Epoch [7/10], batch: [31/49, loss: 0.0372]\n",
      "Epoch [7/10], batch: [32/49, loss: 0.4339]\n",
      "Epoch [7/10], batch: [33/49, loss: 0.2691]\n",
      "Epoch [7/10], batch: [34/49, loss: 0.0669]\n",
      "Epoch [7/10], batch: [35/49, loss: 0.1725]\n",
      "Epoch [7/10], batch: [36/49, loss: 0.0811]\n",
      "Epoch [7/10], batch: [37/49, loss: 0.0979]\n",
      "Epoch [7/10], batch: [38/49, loss: 0.3040]\n",
      "Epoch [7/10], batch: [39/49, loss: 0.2377]\n",
      "Epoch [7/10], batch: [40/49, loss: 0.0635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], batch: [41/49, loss: 0.0561]\n",
      "Epoch [7/10], batch: [42/49, loss: 0.2346]\n",
      "Epoch [7/10], batch: [43/49, loss: 0.1132]\n",
      "Epoch [7/10], batch: [44/49, loss: 0.2002]\n",
      "Epoch [7/10], batch: [45/49, loss: 0.0414]\n",
      "Epoch [7/10], batch: [46/49, loss: 0.0365]\n",
      "Epoch [7/10], batch: [47/49, loss: 0.0543]\n",
      "Epoch [7/10], batch: [48/49, loss: 0.1280]\n",
      "Epoch [8/10], batch: [0/49, loss: 0.1105]\n",
      "Epoch [8/10], batch: [1/49, loss: 0.0613]\n",
      "Epoch [8/10], batch: [2/49, loss: 0.0105]\n",
      "Epoch [8/10], batch: [3/49, loss: 0.0507]\n",
      "Epoch [8/10], batch: [4/49, loss: 0.0163]\n",
      "Epoch [8/10], batch: [5/49, loss: 0.0132]\n",
      "Epoch [8/10], batch: [6/49, loss: 0.0461]\n",
      "Epoch [8/10], batch: [7/49, loss: 0.1363]\n",
      "Epoch [8/10], batch: [8/49, loss: 0.0215]\n",
      "Epoch [8/10], batch: [9/49, loss: 0.0371]\n",
      "Epoch [8/10], batch: [10/49, loss: 0.0168]\n",
      "Epoch [8/10], batch: [11/49, loss: 0.0293]\n",
      "Epoch [8/10], batch: [12/49, loss: 0.0161]\n",
      "Epoch [8/10], batch: [13/49, loss: 0.0601]\n",
      "Epoch [8/10], batch: [14/49, loss: 0.0196]\n",
      "Epoch [8/10], batch: [15/49, loss: 0.0310]\n",
      "Epoch [8/10], batch: [16/49, loss: 0.0633]\n",
      "Epoch [8/10], batch: [17/49, loss: 0.0186]\n",
      "Epoch [8/10], batch: [18/49, loss: 0.0101]\n",
      "Epoch [8/10], batch: [19/49, loss: 0.0060]\n",
      "Epoch [8/10], batch: [20/49, loss: 0.0195]\n",
      "Epoch [8/10], batch: [21/49, loss: 0.0474]\n",
      "Epoch [8/10], batch: [22/49, loss: 0.0345]\n",
      "Epoch [8/10], batch: [23/49, loss: 0.0928]\n",
      "Epoch [8/10], batch: [24/49, loss: 0.0209]\n",
      "Epoch [8/10], batch: [25/49, loss: 0.0217]\n",
      "Epoch [8/10], batch: [26/49, loss: 0.0032]\n",
      "Epoch [8/10], batch: [27/49, loss: 0.0077]\n",
      "Epoch [8/10], batch: [28/49, loss: 0.1589]\n",
      "Epoch [8/10], batch: [29/49, loss: 0.0359]\n",
      "Epoch [8/10], batch: [30/49, loss: 0.0219]\n",
      "Epoch [8/10], batch: [31/49, loss: 0.0181]\n",
      "Epoch [8/10], batch: [32/49, loss: 0.0097]\n",
      "Epoch [8/10], batch: [33/49, loss: 0.0135]\n",
      "Epoch [8/10], batch: [34/49, loss: 0.0140]\n",
      "Epoch [8/10], batch: [35/49, loss: 0.0362]\n",
      "Epoch [8/10], batch: [36/49, loss: 0.0158]\n",
      "Epoch [8/10], batch: [37/49, loss: 0.1043]\n",
      "Epoch [8/10], batch: [38/49, loss: 0.0225]\n",
      "Epoch [8/10], batch: [39/49, loss: 0.0194]\n",
      "Epoch [8/10], batch: [40/49, loss: 0.0236]\n",
      "Epoch [8/10], batch: [41/49, loss: 0.0256]\n",
      "Epoch [8/10], batch: [42/49, loss: 0.1785]\n",
      "Epoch [8/10], batch: [43/49, loss: 0.2195]\n",
      "Epoch [8/10], batch: [44/49, loss: 0.0042]\n",
      "Epoch [8/10], batch: [45/49, loss: 0.0249]\n",
      "Epoch [8/10], batch: [46/49, loss: 0.0326]\n",
      "Epoch [8/10], batch: [47/49, loss: 0.0558]\n",
      "Epoch [8/10], batch: [48/49, loss: 0.2095]\n",
      "Epoch [9/10], batch: [0/49, loss: 0.0127]\n",
      "Epoch [9/10], batch: [1/49, loss: 0.0045]\n",
      "Epoch [9/10], batch: [2/49, loss: 0.0139]\n",
      "Epoch [9/10], batch: [3/49, loss: 0.1345]\n",
      "Epoch [9/10], batch: [4/49, loss: 0.0197]\n",
      "Epoch [9/10], batch: [5/49, loss: 0.0632]\n",
      "Epoch [9/10], batch: [6/49, loss: 0.0078]\n",
      "Epoch [9/10], batch: [7/49, loss: 0.0595]\n",
      "Epoch [9/10], batch: [8/49, loss: 0.0084]\n",
      "Epoch [9/10], batch: [9/49, loss: 0.0031]\n",
      "Epoch [9/10], batch: [10/49, loss: 0.0507]\n",
      "Epoch [9/10], batch: [11/49, loss: 0.0650]\n",
      "Epoch [9/10], batch: [12/49, loss: 0.0424]\n",
      "Epoch [9/10], batch: [13/49, loss: 0.0599]\n",
      "Epoch [9/10], batch: [14/49, loss: 0.0093]\n",
      "Epoch [9/10], batch: [15/49, loss: 0.0017]\n",
      "Epoch [9/10], batch: [16/49, loss: 0.0927]\n",
      "Epoch [9/10], batch: [17/49, loss: 0.0134]\n",
      "Epoch [9/10], batch: [18/49, loss: 0.0117]\n",
      "Epoch [9/10], batch: [19/49, loss: 0.0773]\n",
      "Epoch [9/10], batch: [20/49, loss: 0.0065]\n",
      "Epoch [9/10], batch: [21/49, loss: 0.0023]\n",
      "Epoch [9/10], batch: [22/49, loss: 0.0023]\n",
      "Epoch [9/10], batch: [23/49, loss: 0.0027]\n",
      "Epoch [9/10], batch: [24/49, loss: 0.0092]\n",
      "Epoch [9/10], batch: [25/49, loss: 0.0183]\n",
      "Epoch [9/10], batch: [26/49, loss: 0.0898]\n",
      "Epoch [9/10], batch: [27/49, loss: 0.0471]\n",
      "Epoch [9/10], batch: [28/49, loss: 0.0066]\n",
      "Epoch [9/10], batch: [29/49, loss: 0.0044]\n",
      "Epoch [9/10], batch: [30/49, loss: 0.1778]\n",
      "Epoch [9/10], batch: [31/49, loss: 0.0030]\n",
      "Epoch [9/10], batch: [32/49, loss: 0.0241]\n",
      "Epoch [9/10], batch: [33/49, loss: 0.0037]\n",
      "Epoch [9/10], batch: [34/49, loss: 0.0219]\n",
      "Epoch [9/10], batch: [35/49, loss: 0.0075]\n",
      "Epoch [9/10], batch: [36/49, loss: 0.0032]\n",
      "Epoch [9/10], batch: [37/49, loss: 0.0019]\n",
      "Epoch [9/10], batch: [38/49, loss: 0.0065]\n",
      "Epoch [9/10], batch: [39/49, loss: 0.0044]\n",
      "Epoch [9/10], batch: [40/49, loss: 0.0023]\n",
      "Epoch [9/10], batch: [41/49, loss: 0.0039]\n",
      "Epoch [9/10], batch: [42/49, loss: 0.0025]\n",
      "Epoch [9/10], batch: [43/49, loss: 0.0050]\n",
      "Epoch [9/10], batch: [44/49, loss: 0.0331]\n",
      "Epoch [9/10], batch: [45/49, loss: 0.0506]\n",
      "Epoch [9/10], batch: [46/49, loss: 0.0160]\n",
      "Epoch [9/10], batch: [47/49, loss: 0.0035]\n",
      "Epoch [9/10], batch: [48/49, loss: 0.0066]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_batches = len(dataloader_train_V2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, d in enumerate(dataloader_train_V2):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_v2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46cf733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.109\n",
      "Acccuracy  : 0.516\n",
      "F1 Score: 0.400\n"
     ]
    }
   ],
   "source": [
    "calc_loss_acc(model_v2, loss_func, dataloader_val_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82b953",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "#### Now that we managed to get the models running, we try to improve the models using the train and validation sets. One thing we will do is to remove some layers, as I have a feeling that we have to many convolutional layers for a problem that doesn't need such a deep structure. As mentioned in the task description, we only need to do so with one of the two datasets. In my case this is the V1 dataset (anger and joy).\n",
    "#### The first model in the race is the same as above. We will train with 15 epochs though and report the validation error at end of every epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae6560",
   "metadata": {},
   "source": [
    "### Training and evaluating Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544d28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_1(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=5, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(64, 16, kernel_size=3, padding=\"same\")\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.linear = nn.Linear(16, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # pool the 16 dimension to 1\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2659a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_1 = my_model_1(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d024c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.665\n",
      "Acccuracy  : 0.619\n",
      "F1 Score: 0.020\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.620\n",
      "Acccuracy  : 0.665\n",
      "F1 Score: 0.025\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.651\n",
      "Acccuracy  : 0.634\n",
      "F1 Score: 0.309\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.606\n",
      "Acccuracy  : 0.657\n",
      "F1 Score: 0.290\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.686\n",
      "Acccuracy  : 0.553\n",
      "F1 Score: 0.515\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.568\n",
      "Acccuracy  : 0.667\n",
      "F1 Score: 0.592\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.800\n",
      "Acccuracy  : 0.615\n",
      "F1 Score: 0.057\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.407\n",
      "Acccuracy  : 0.781\n",
      "F1 Score: 0.517\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.863\n",
      "Acccuracy  : 0.591\n",
      "F1 Score: 0.255\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.231\n",
      "Acccuracy  : 0.907\n",
      "F1 Score: 0.844\n",
      "################################\n",
      "--------EPOCH: 6 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.924\n",
      "Acccuracy  : 0.599\n",
      "F1 Score: 0.558\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.187\n",
      "Acccuracy  : 0.917\n",
      "F1 Score: 0.890\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_1(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_1, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_1, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0e603",
   "metadata": {},
   "source": [
    "### Training and evalating Model 2\n",
    "#### Let's make some changes to the model and the parameters and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d90f560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_2(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 256, kernel_size=10, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(256, 128, kernel_size=8, padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(128, 64, kernel_size=6, padding=\"same\")\n",
    "        self.conv4 = nn.Conv1d(64, 32, kernel_size=3, padding=\"same\")\n",
    "\n",
    "\n",
    "        self.linear2 = nn.Linear(32, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x, _ = x.max(dim=-1)\n",
    "        #rint(x.shape)\n",
    "        \n",
    "        \n",
    "       # print(x.shape)\n",
    "\n",
    "\n",
    "        y_out = self.linear2(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1769b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model_2 = my_model_2(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b07ee381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damja\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:297: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ..\\aten\\src\\ATen\\native\\Convolution.cpp:647.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.658\n",
      "Acccuracy  : 0.623\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.600\n",
      "Acccuracy  : 0.664\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.657\n",
      "Acccuracy  : 0.623\n",
      "F1 Score: 0.000\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.525\n",
      "Acccuracy  : 0.664\n",
      "F1 Score: 0.000\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.823\n",
      "Acccuracy  : 0.607\n",
      "F1 Score: 0.056\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.295\n",
      "Acccuracy  : 0.852\n",
      "F1 Score: 0.718\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.267\n",
      "Acccuracy  : 0.514\n",
      "F1 Score: 0.486\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.221\n",
      "Acccuracy  : 0.884\n",
      "F1 Score: 0.852\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.915\n",
      "Acccuracy  : 0.591\n",
      "F1 Score: 0.245\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.071\n",
      "Acccuracy  : 0.973\n",
      "F1 Score: 0.959\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_2, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_2, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68223f",
   "metadata": {},
   "source": [
    "### Training and evalating Model 3\n",
    "#### Let's see what additional changes we can make to make the model perform better. It is really difficult as I do not have a intuition what makes the model perform well and what not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08948473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_3(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, batch_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 64, kernel_size=10, padding=\"same\")\n",
    "\n",
    "\n",
    "        \n",
    "        self.linear2 = nn.Linear(64, 2) # only 2 classes as output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        #30 = max num of tokens\n",
    "        x = x.reshape(len(x), self.embedding_size, 30) ## Embedding Length needs to be treated as channel dimension\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        y_out = self.linear2(x)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eb8e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model_3 = my_model_3(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_3.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0af6a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.737\n",
      "Acccuracy  : 0.611\n",
      "F1 Score: 0.180\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.436\n",
      "Acccuracy  : 0.769\n",
      "F1 Score: 0.526\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.746\n",
      "Acccuracy  : 0.630\n",
      "F1 Score: 0.286\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.236\n",
      "Acccuracy  : 0.939\n",
      "F1 Score: 0.902\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.967\n",
      "Acccuracy  : 0.638\n",
      "F1 Score: 0.114\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.202\n",
      "Acccuracy  : 0.907\n",
      "F1 Score: 0.839\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.052\n",
      "Acccuracy  : 0.634\n",
      "F1 Score: 0.130\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.161\n",
      "Acccuracy  : 0.921\n",
      "F1 Score: 0.866\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.838\n",
      "Acccuracy  : 0.626\n",
      "F1 Score: 0.351\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.048\n",
      "Acccuracy  : 0.993\n",
      "F1 Score: 0.990\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_3(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_3, loss_func, dataloader_val_V1)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_3, loss_func, dataloader_train_V1)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f9752",
   "metadata": {},
   "source": [
    "## Testing all 3 models on the validation sets, to determine which model is the best.\n",
    "### Now, I will use all the trained models and evaluate them on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e86e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### EVALUATING MODEL 1 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.936\n",
      "Acccuracy  : 0.584\n",
      "F1 Score: 0.541\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 2 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.915\n",
      "Acccuracy  : 0.591\n",
      "F1 Score: 0.245\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 3 ####\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.838\n",
      "Acccuracy  : 0.626\n",
      "F1 Score: 0.351\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"#### EVALUATING MODEL 1 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_1, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(\"#### EVALUATING MODEL 2 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_2, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(\"#### EVALUATING MODEL 3 ####\")\n",
    "print('Metrics on Validation Set:')\n",
    "calc_loss_acc(model_3, loss_func, dataloader_val_V1)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cc9ca",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "#### Let's look at the performance of our best model, which seems to be model 3 on our other dataset. For this, we of course need to train the model again on the other dataset and evaluate it. Note that we should evaluate it on the test set at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b5998",
   "metadata": {},
   "source": [
    "First, we will initialize the model and then train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62cbb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model_3_v2 = my_model_3(len(word_to_ix), EMBED_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(model_3_v2.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ed9fb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------EPOCH: 1 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.028\n",
      "Acccuracy  : 0.532\n",
      "F1 Score: 0.664\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.659\n",
      "Acccuracy  : 0.662\n",
      "F1 Score: 0.723\n",
      "################################\n",
      "--------EPOCH: 2 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.823\n",
      "Acccuracy  : 0.532\n",
      "F1 Score: 0.479\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.190\n",
      "Acccuracy  : 0.949\n",
      "F1 Score: 0.943\n",
      "################################\n",
      "--------EPOCH: 3 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.840\n",
      "Acccuracy  : 0.548\n",
      "F1 Score: 0.512\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.099\n",
      "Acccuracy  : 0.988\n",
      "F1 Score: 0.987\n",
      "################################\n",
      "--------EPOCH: 4 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 0.841\n",
      "Acccuracy  : 0.543\n",
      "F1 Score: 0.573\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.090\n",
      "Acccuracy  : 0.974\n",
      "F1 Score: 0.973\n",
      "################################\n",
      "--------EPOCH: 5 --------\n",
      "Metrics on Validation Set:\n",
      "Loss : 1.010\n",
      "Acccuracy  : 0.505\n",
      "F1 Score: 0.387\n",
      "       -----      \n",
      "Metrics on Training Set:\n",
      "Loss : 0.048\n",
      "Acccuracy  : 0.994\n",
      "F1 Score: 0.993\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "num_batches = len(dataloader_train_V2)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i, d in enumerate(dataloader_train_V2):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = d\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model_3_v2(x)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_batch = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 == 0: \n",
    "            #print(f'Epoch [{epoch}/{num_epochs}], batch: [{i}/{num_batches}, loss: {loss_batch:.4f}]')\n",
    "            pass\n",
    "    \n",
    "    # at end of epoch calculate the validation loss\n",
    "    print(f'--------EPOCH: {epoch} --------')\n",
    "    print('Metrics on Validation Set:')\n",
    "    calc_loss_acc(model_3_v2, loss_func, dataloader_val_V2)\n",
    "    print(\"       -----      \")\n",
    "    print('Metrics on Training Set:')\n",
    "    calc_loss_acc(model_3_v2, loss_func, dataloader_train_V2)\n",
    "    print('################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d095b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### EVALUATING MODEL 3 on TEST SET of dataset 1 ####\n",
      "Metrics on Test Set:\n",
      "Loss : 0.804\n",
      "Acccuracy  : 0.660\n",
      "F1 Score: 0.444\n",
      "----------------------------------\n",
      "#### EVALUATING MODEL 3 on TEST SET of dataset 2 ####\n",
      "Metrics on Test Set:\n",
      "Loss : 0.918\n",
      "Acccuracy  : 0.562\n",
      "F1 Score: 0.455\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"#### EVALUATING MODEL 3 on TEST SET of dataset 1 ####\")\n",
    "print('Metrics on Test Set:')\n",
    "calc_loss_acc(model_3, loss_func, dataloader_test_V1)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"#### EVALUATING MODEL 3 on TEST SET of dataset 2 ####\")\n",
    "print('Metrics on Test Set:')\n",
    "calc_loss_acc(model_3_v2, loss_func, dataloader_test_V2)\n",
    "print(\"----------------------------------\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
